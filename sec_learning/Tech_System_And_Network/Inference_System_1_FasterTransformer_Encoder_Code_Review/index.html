<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Exo 2:300,300italic,400,400italic,700,700italic|Caveat:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zobinhuang.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":180,"display":"post","padding":10,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:type" content="website">
<meta property="og:title" content="Code Review Record of FasterTransformer BERT">
<meta property="og:url" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/Inference_System_1_FasterTransformer_Encoder_Code_Review/index.html">
<meta property="og:site_name" content="Zobin">
<meta property="og:description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/Inference_System_1_FasterTransformer_Encoder_Code_Review/pic/xxx.png">
<meta property="article:published_time" content="2023-09-06T09:49:36.847Z">
<meta property="article:modified_time" content="2023-09-06T09:49:36.847Z">
<meta property="article:author" content="Zhuobin Huang">
<meta property="article:tag" content="Zobin">
<meta property="article:tag" content="黄卓彬">
<meta property="article:tag" content="zobinHuang">
<meta property="article:tag" content="网络工程">
<meta property="article:tag" content="Networking Engineering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/Inference_System_1_FasterTransformer_Encoder_Code_Review/pic/xxx.png">

<link rel="canonical" href="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/Inference_System_1_FasterTransformer_Encoder_Code_Review/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Code Review Record of FasterTransformer BERT | Zobin
</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Zobin" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zobin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Lovin' Tech with Tea</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about-me">

    <a href="/sec_about/" rel="section"><i class="fa fa-user fa-fw"></i>About Me</a>

  </li>
        <li class="menu-item menu-item-library">

    <a href="/sec_learning/" rel="section"><i class="fa fa-duotone fa-book fa-fw"></i>Library</a>

  </li>
        <li class="menu-item menu-item-production">

    <a href="/sec_music/" rel="section"><i class="fa fa-music fa-fw"></i>Production</a>

  </li>
        <li class="menu-item menu-item-thoughts">

    <a href="/sec_thoughts/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Thoughts</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="en">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">Code Review Record of FasterTransformer BERT
</h1>

<div class="post-meta">
  
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Tech_System_And_Network/">TECH_SYSTEM_AND_NETWORK</a></li>
          <li>INFERENCE_SYSTEM_1_FASTERTRANSFORMER_ENCODER_CODE_REVIEW</li>
        
  </ul>

</div>

</header>

      
      
      
      <div class="post-body">
          <head>
<!--导入样式表-->
<link rel="stylesheet" type="text/css" href="style/index.css">

<!--导入网页脚本-->
<script src="script/index.js"></script>

<!--支持伪代码显示-->
<script>
    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3.0.0/es5/tex-chtml.js"
        integrity="sha256-3Fdoa5wQb+JYfEmTpQHx9sc/GuwpfC/0R9EpBki+mf8=" crossorigin>
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js">
</script>

<!--支持网页公式显示-->    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>

<!--支持矩阵显示-->
<script type="text/javascript">
  run_maths = function() {
    if (document.querySelector('[class*="cmath"]') !== null) {
      if (typeof (mjax_path)=='undefined') { mjax_path='https://cdn.jsdelivr.net/npm/mathjax@2'; }
      if (typeof (mjax_config)=='undefined') { mjax_config='AM_CHTML'; }
      smjax = document.createElement ('script');
      smjax.setAttribute('src',`${mjax_path}/MathJax.js?config=${mjax_config}`);
      smjax.setAttribute('async',true);
      document.getElementsByTagName('head')[0].appendChild(smjax);
    }
  };
  if (document.readyState === 'loading') {  
    window.addEventListener('DOMContentLoaded', run_maths); 
  } else { 
    run_maths(); 
  }
</script>
</head>

<body onload="load_page()">

<!-- 导入 mermaid -->
<script src="script/mermaid.min.js"></script>
<script>mermaid.initialize({startOnLoad:true});</script>

<!-- 导入 chart.js -->
<script src="script/chart.min.js"></script>

<!-- 本文的 Metadata -->
<div id="metadata"></div>

<!-- Start your post here -->
<h2 class="title">Torch Op</h2>
<div class="div_learning_post">
  <h3 class="title">Op Definition</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Firstly we take a look at how cpp-written code expose to python-side. According to the official article「EXTENDING TORCHSCRIPT WITH CUSTOM C++ CLASSES」<cite>torch_custom_op</cite>，we can easily find the Torch Op definition of BERT inside FasterTransformer, from <code>fastertransformer/th_op/bert/BertOp.h</code> <cite>ft_th_op_bert_bertop_h</cite>，listed below:

  <div class="code_segment" label="define_op">

  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FasterTransformerBert</span>:</span> <span class="keyword">public</span> th::jit::CustomClassHolder &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    FasterTransformerBert(th::Tensor q_kernel,</span><br><span class="line">                          th::Tensor q_bias,</span><br><span class="line">                          th::Tensor k_kernel,</span><br><span class="line">                          th::Tensor k_bias,</span><br><span class="line">                          th::Tensor v_kernel,</span><br><span class="line">                          th::Tensor v_bias,</span><br><span class="line">                          th::Tensor attr_output_kernel,</span><br><span class="line">                          th::Tensor attr_output_bias,</span><br><span class="line">                          th::Tensor attr_output_layernorm_gamma,</span><br><span class="line">                          th::Tensor attr_output_layernorm_beta,</span><br><span class="line">                          th::Tensor inter_kernel,</span><br><span class="line">                          th::Tensor inter_bias,</span><br><span class="line">                          th::Tensor output_kernel,</span><br><span class="line">                          th::Tensor output_bias,</span><br><span class="line">                          th::Tensor output_layernorm_gamma,</span><br><span class="line">                          th::Tensor output_layernorm_beta,</span><br><span class="line">                          <span class="keyword">int64_t</span>    head_num,</span><br><span class="line">                          <span class="keyword">int64_t</span>    head_size,</span><br><span class="line">                          <span class="keyword">int64_t</span>    inter_size,</span><br><span class="line">                          <span class="keyword">bool</span>       remove_padding,</span><br><span class="line">                          <span class="keyword">int64_t</span>    layer_num,</span><br><span class="line">                          <span class="keyword">bool</span>       sparse,</span><br><span class="line">                          <span class="keyword">double</span>     q_scaling,</span><br><span class="line">                          <span class="keyword">int64_t</span>    tensor_para_size,</span><br><span class="line">                          <span class="keyword">int64_t</span>    pipeline_para_size);</span><br><span class="line"></span><br><span class="line">    ~FasterTransformerBert();</span><br><span class="line"></span><br><span class="line">    <span class="function">th::Tensor <span class="title">forward</span><span class="params">(th::Tensor input, th::Tensor sequence_lengths)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;th::Tensor&gt; <span class="title">get_pickle_info</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">const</span> at::ScalarType    _st;</span><br><span class="line">    <span class="keyword">bool</span>                    _remove_padding;</span><br><span class="line">    IFBert*                 ftbert;</span><br><span class="line">    th::Tensor              head_info;</span><br><span class="line">    th::Tensor              scaling_info;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;th::Tensor&gt; weights;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;The class <code>FasterTransformerBert</code> is inherited from <code>th::jit::CustomClassHolder</code> ，it defined <code>forward</code> and <code>get_pickle_info</code> interfaces, and exposed these two intferfaces to TorchScript via the declaration inside <code>fastertransformer/th_op/bert/BertOp.cc</code> <cite>ft_th_op_bert_bertop_cc</cite>, listed below:

  <div class="code_segment" label="expose_op">

  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">auto</span> fasterTransformerBertTHS =</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> LEGACY_THS</span></span><br><span class="line">    torch::jit::class_&lt;torch_ext::FasterTransformerBert&gt;(<span class="string">&quot;FasterTransformerBert&quot;</span>)</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    torch::jit::class_&lt;torch_ext::FasterTransformerBert&gt;(<span class="string">&quot;FasterTransformer&quot;</span>, <span class="string">&quot;Bert&quot;</span>)</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">        .def(torch::jit::init&lt;th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              th::Tensor,</span><br><span class="line">                              <span class="keyword">int64_t</span>,</span><br><span class="line">                              <span class="keyword">int64_t</span>,</span><br><span class="line">                              <span class="keyword">int64_t</span>,</span><br><span class="line">                              <span class="keyword">bool</span>,</span><br><span class="line">                              <span class="keyword">int64_t</span>,</span><br><span class="line">                              <span class="keyword">bool</span>,</span><br><span class="line">                              <span class="keyword">double</span>,</span><br><span class="line">                              <span class="keyword">int64_t</span>,</span><br><span class="line">                              <span class="keyword">int64_t</span>&gt;())</span><br><span class="line">        .def(<span class="string">&quot;forward&quot;</span>, &amp;torch_ext::FasterTransformerBert::forward)</span><br><span class="line">        .def_pickle(</span><br><span class="line">            [](<span class="keyword">const</span> c10::intrusive_ptr&lt;torch_ext::FasterTransformerBert&gt;&amp; self) -&gt; <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;th::Tensor&gt; &#123;</span><br><span class="line">                <span class="keyword">return</span> self-&gt;get_pickle_info();</span><br><span class="line">            &#125;,</span><br><span class="line">            [](<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;th::Tensor&gt; state) -&gt; c10::intrusive_ptr&lt;torch_ext::FasterTransformerBert&gt; &#123;</span><br><span class="line">                <span class="keyword">int64_t</span> head_num           = state[<span class="number">16</span>][<span class="number">0</span>].item().to&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">                <span class="keyword">int64_t</span> head_size          = state[<span class="number">16</span>][<span class="number">1</span>].item().to&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">                <span class="keyword">bool</span>    remove_padding     = (<span class="keyword">bool</span>)(state[<span class="number">16</span>][<span class="number">2</span>].item().to&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">                <span class="keyword">int64_t</span> layer_num          = state[<span class="number">16</span>][<span class="number">3</span>].item().to&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">                <span class="keyword">bool</span>    sparse             = (<span class="keyword">bool</span>)(state[<span class="number">16</span>][<span class="number">4</span>].item().to&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">                <span class="keyword">int64_t</span> inter_size         = state[<span class="number">16</span>][<span class="number">5</span>].item().to&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">                <span class="keyword">int64_t</span> tensor_para_size   = state[<span class="number">16</span>][<span class="number">6</span>].item().to&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">                <span class="keyword">int64_t</span> pipeline_para_size = state[<span class="number">16</span>][<span class="number">7</span>].item().to&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">                <span class="keyword">double</span>  q_scaling          = state[<span class="number">17</span>][<span class="number">0</span>].item().to&lt;<span class="keyword">double</span>&gt;();</span><br><span class="line">                <span class="keyword">return</span> c10::make_intrusive&lt;torch_ext::FasterTransformerBert&gt;(state[<span class="number">0</span>],</span><br><span class="line">                                                                            state[<span class="number">1</span>],</span><br><span class="line">                                                                            state[<span class="number">2</span>],</span><br><span class="line">                                                                            state[<span class="number">3</span>],</span><br><span class="line">                                                                            state[<span class="number">4</span>],</span><br><span class="line">                                                                            state[<span class="number">5</span>],</span><br><span class="line">                                                                            state[<span class="number">6</span>],</span><br><span class="line">                                                                            state[<span class="number">7</span>],</span><br><span class="line">                                                                            state[<span class="number">8</span>],</span><br><span class="line">                                                                            state[<span class="number">9</span>],</span><br><span class="line">                                                                            state[<span class="number">10</span>],</span><br><span class="line">                                                                            state[<span class="number">11</span>],</span><br><span class="line">                                                                            state[<span class="number">12</span>],</span><br><span class="line">                                                                            state[<span class="number">13</span>],</span><br><span class="line">                                                                            state[<span class="number">14</span>],</span><br><span class="line">                                                                            state[<span class="number">15</span>],</span><br><span class="line">                                                                            head_num,</span><br><span class="line">                                                                            head_size,</span><br><span class="line">                                                                            inter_size,</span><br><span class="line">                                                                            remove_padding,</span><br><span class="line">                                                                            layer_num,</span><br><span class="line">                                                                            sparse,</span><br><span class="line">                                                                            q_scaling,</span><br><span class="line">                                                                            tensor_para_size,</span><br><span class="line">                                                                            pipeline_para_size);</span><br><span class="line">            &#125;);</span><br></pre></td></tr></table></figure>
  </div>

  <h3 class="title">Op Usage</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Oncde finished compiling the above cpp code, one can take this op to process torch tensor now. According to <code>fastertransformer/examples/pytorch/bert/utils/encoder.py</code> <cite>ft_example_pytorch_bert_utils_encoder_py</cite>, firstly one need to explicitly load the compiled shared library, listed below:

  <div class="code_segment" label="load_library">

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.classes.load_library(path)</span><br></pre></td></tr></table></figure>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Where the <code>path</code> indicates the path to the compiled shared library, and then one can use the op in a very easy way, full codes are listed below:

  <div class="code_segment" label="use_op">

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fastertransformer/examples/pytorch/bert/utils/encoder.py</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomEncoder</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer_num, head_num, head_size, weights,</span></span></span><br><span class="line"><span class="function"><span class="params">                int8_mode=<span class="number">0</span>, remove_padding=<span class="literal">False</span>, sparse=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                path=<span class="string">&#x27;./lib/libth_transformer.so&#x27;</span>, tensor_para_size=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                pipeline_para_size=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layer_num = layer_num</span><br><span class="line">        self.remove_padding = remove_padding</span><br><span class="line">        self.int8_mode = int8_mode</span><br><span class="line">        torch.classes.load_library(path)</span><br><span class="line"></span><br><span class="line">        weights_ = weights.listed_weights()</span><br><span class="line"></span><br><span class="line">        self.use_mpi = dist.is_mpi_available()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.use_mpi:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                dist.init_process_group(backend=<span class="string">&#x27;mpi&#x27;</span>)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                print(<span class="string">&quot;[INFO] WARNING: Exception occurred in dist.init_process_group(backend=&#x27;mpi&#x27;). Maybe the process group has been initialized somewhere else.&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">&quot;[INFO] MPI is not available in this PyTorch build.&quot;</span>)</span><br><span class="line">            <span class="keyword">assert</span> tensor_para_size == <span class="number">1</span>, <span class="string">&quot;[FATAL] MPI is required for tensor_para_size &gt; 1.&quot;</span></span><br><span class="line">            <span class="keyword">assert</span> pipeline_para_size == <span class="number">1</span>, <span class="string">&quot;[FATAL] MPI is required for pipeline_para_size &gt; 1.&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> int8_mode == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(weights_) == <span class="number">16</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                self.encoders = torch.classes.FasterTransformer.Bert(</span><br><span class="line">                    *weights_,</span><br><span class="line">                    head_num, head_size, <span class="number">4</span> * head_num * head_size, remove_padding, layer_num, sparse, <span class="number">1.0</span>,</span><br><span class="line">                    tensor_para_size, pipeline_para_size)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="comment"># legacy ths for 20.03 image</span></span><br><span class="line">                self.encoders = torch.classes.FasterTransformerBert(</span><br><span class="line">                    *weights_,</span><br><span class="line">                    head_num, head_size, <span class="number">4</span> * head_num * head_size, remove_padding, layer_num, sparse, <span class="number">1.0</span>,</span><br><span class="line">                    tensor_para_size, pipeline_para_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(weights_) == <span class="number">18</span></span><br><span class="line">            <span class="keyword">assert</span> tensor_para_size == <span class="number">1</span>, <span class="string">&quot;INT8 BERT still only support tensor_para_size = 1&quot;</span></span><br><span class="line">            <span class="keyword">assert</span> pipeline_para_size == <span class="number">1</span>, <span class="string">&quot;INT8 BERT still only support pipeline_para_size = 1&quot;</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                self.encoders = torch.classes.FasterTransformer.INT8Bert(</span><br><span class="line">                    *weights_,</span><br><span class="line">                    head_num, head_size, remove_padding, layer_num, int8_mode, sparse, <span class="number">1.0</span>)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="comment"># legacy ths for 20.03 image</span></span><br><span class="line">                self.encoders = torch.classes.FasterTransformerINT8Bert(</span><br><span class="line">                    *weights_,</span><br><span class="line">                    head_num, head_size, remove_padding, layer_num, int8_mode, sparse, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, attention_mask, sequence_lengths</span>):</span></span><br><span class="line">        hidden_states = self.encoders.forward(hidden_states, sequence_lengths)</span><br><span class="line">        <span class="keyword">return</span> (hidden_states,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fastertransformer/examples/pytorch/bert/bert_example.py</span></span><br><span class="line">output = custom_encoder(ft_inp, ft_mask, ft_mem_seq_lens) </span><br></pre></td></tr></table></figure>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;
  </div>
</div>

<h2 class="title">Forward Process</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;In this section we take a look at how the forward process is conducted behind the Op.

  <div class="code_segment" label="forward">

  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">th::Tensor <span class="title">FasterTransformerBert::forward</span><span class="params">(th::Tensor input, th::Tensor sequence_lengths)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    CHECK_INPUT(input, _st);</span><br><span class="line">    CHECK_TH_CUDA(sequence_lengths);</span><br><span class="line">    CHECK_CONTIGUOUS(sequence_lengths);</span><br><span class="line">    TORCH_CHECK(sequence_lengths.dtype() == torch::kInt32, <span class="string">&quot;sequence_lengths dtype should be int32&quot;</span>);</span><br><span class="line">    <span class="keyword">size_t</span> batch_size = (<span class="keyword">size_t</span>)input.size(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">size_t</span> seq_len    = (<span class="keyword">size_t</span>)input.size(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> output = torch::empty_like(input);</span><br><span class="line">    ftbert-&gt;forward(batch_size, seq_len, input, sequence_lengths, output, _remove_padding);</span><br><span class="line">    <span class="keyword">return</span> output;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  </div>

  <h3 class="title">Input & Output Tensors</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Firstly we need to take a look at the input/output tensor of the op. Based on the above <coderef>use_op</coderef>:

  <div class="code_segment" label="input_output">

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomEncoder</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, attention_mask, sequence_lengths</span>):</span></span><br><span class="line">        hidden_states = self.encoders.forward(hidden_states, sequence_lengths)</span><br><span class="line">        <span class="keyword">return</span> (hidden_states,)</span><br></pre></td></tr></table></figure>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;
</div>



<div class="div_ref" id="ref_container"></div>

</body>

<!-- 代码块 -->
<!-- <div class="code_segment" label="forward">

<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* hello*/</span></span><br></pre></td></tr></table></figure>
</div> -->

<!-- 代码块引用 -->
<!-- <coderef>forward</coderef> -->

<!-- Comment -->
<!-- <div class="comblock" title="xxx"> -->

<!-- Chart Support -->
<!-- Check https://www.runoob.com/chartjs/chartjs-tutorial.html -->
<!-- <div class="chartjs" label="naive_impt_performance"></div> -->

<!-- Note Support -->
<!-- Check https://theme-next.js.org/docs/tag-plugins/note.html -->

<!-- 圆圈数字 -->
<!--
⓪ ① ② ③ ④ ⑤ ⑥ ⑦ ⑧ ⑨ ⑩ ⑪ ⑫ ⑬ ⑭ ⑮ ⑯ ⑰ ⑱ ⑲ ⑳ ㉑ ㉒ ㉓ ㉔ ㉕ ㉖ ㉗ ㉘ ㉙ ㉚ ㉛ ㉜ ㉝ ㉞ ㉟ ㊱ ㊲ ㊳ ㊴ ㊵ ㊶ ㊷ ㊸ ㊹ ㊺ ㊻ ㊼ ㊽ ㊾ ㊿
-->

<!-- Flow Chart -->
<!--
Format see: https://mermaid-js.github.io/mermaid/#/flowchart
-->
<!-- <flowchart class="mermaid">
 Mermaid Flow Chart Code
</flowchart> -->

<!-- Sign Block -->
<!--
<div class="noteblock">
A NOTE
</div>

<div class="queblock">
A QUESTION
</div>
-->

<!--图片、引用-->
<!-- 
<div class="img" title="img title" label="img_label" source="url">
  <img src="" height="" />
</div>

<imaging>img_label</imaging>
-->

<!--等式、引用-->
<!-- 
<div class="equation" label="equation_label">
</div>

<equation>equation_label</equation>
-->

<!--定理、引用、证明-->
<!-- 
<div class="theorm" label="theorm_label">
</div>

<theorm>theorm_label</theorm>

<div class="theorm_prove">
</div>
-->

<!--引用其它章节-->
<!-- 
<ref></ref> 
-->

<!--引用文献-->
<!-- 
<cite></cite> 
-->

<!--关键词-->
<!-- 
<def></def> 
-->

<!--醒目注意-->
<!-- 
<note></note> 
-->

<!--段落-->
<!--
<h3 class="paragraph">Paragraph Name</h3>
-->

<!--表格-->
<!--
<div class="table" title="Table Title" label="table_label">
  <table border="1" align="center" bgcolor="#FFFFFF">
    <tr>
      <th>A</th>
      <th>B</th>
      <th>C</th>
    </tr>
    <tr>
      <td>xxx</td>
      <td>xxx</td>
      <td>xxx</td>
    </tr>
  </table>
</div>
-->

<!--矩阵公式-->
<!--
<div class="cmath" align="center">
  `((1, 0),(1, 0))`
</div><br>
-->

<!--伪代码-->
<!--
<pre id="quicksort" style="display:hidden;">
  % This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
  \begin{algorithm}
  \caption{Quicksort}
  \begin{algorithmic}
  \PROCEDURE{Quicksort}{$A, p, r$}
      % Add Here

      % 空行
      % \STATE \texttt{\\}
  \ENDPROCEDURE
  \end{algorithmic}
  \end{algorithm}
</pre>
<script>
    pseudocode.renderElement(document.getElementById("quicksort"));
</script>
-->
<!--
Latex 伪代码格式见: https://github.com/SaswatPadhi/pseudocode.js
-->

<!--图片-->
<!--
<div align="center">
  <img src="./pic/xxx.png" width=80%>
</div>
-->

<!--正文-->
<!--
<p>
&nbsp;&nbsp;&nbsp;&nbsp;公式：<span>`\overline{A}\overline{B}`</span>
</p>
-->
      </div>
      
      
      
    </div>
    
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Tech_System_And_Network/">TECH_SYSTEM_AND_NETWORK</a></li>
          <li>INFERENCE_SYSTEM_1_FASTERTRANSFORMER_ENCODER_CODE_REVIEW</li>
        
  </ul>

    
    
    


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhuobin Huang"
      src="/images/avatar_2.png">
  <p class="site-author-name" itemprop="name">Zhuobin Huang</p>
  <div class="site-description" itemprop="description">System Engineer</div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zobinHuang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zobinHuang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zobin1999@gmail.com" title="E-Mail → mailto:zobin1999@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhuobin Huang</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
