<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Exo 2:300,300italic,400,400italic,700,700italic|Caveat:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zobinhuang.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":180,"display":"post","padding":10,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:type" content="website">
<meta property="og:title" content="PyTorch 数据加载源码分析">
<meta property="og:url" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/index.html">
<meta property="og:site_name" content="Zobin">
<meta property="og:description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/workflow.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/cooperate.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/map_style.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/iter_style.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/dataloader_wf.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/dataloader_wf_1.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/collate_single.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/collate.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/single_process.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/xxx.png">
<meta property="article:published_time" content="2022-09-27T16:52:14.212Z">
<meta property="article:modified_time" content="2022-09-27T16:52:14.212Z">
<meta property="article:author" content="Zhuobin Huang">
<meta property="article:tag" content="Zobin">
<meta property="article:tag" content="黄卓彬">
<meta property="article:tag" content="zobinHuang">
<meta property="article:tag" content="网络工程">
<meta property="article:tag" content="Networking Engineering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/workflow.png">

<link rel="canonical" href="https://zobinhuang.github.io/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>PyTorch 数据加载源码分析 | Zobin
</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Zobin" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zobin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Lovin' Tech with Tea</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about-me-(关于我)">

    <a href="/sec_about/" rel="section"><i class="fa fa-user fa-fw"></i>About Me (关于我)</a>

  </li>
        <li class="menu-item menu-item-library-(知识库)">

    <a href="/sec_learning" rel="section"><i class="fa fa-duotone fa-book fa-fw"></i>Library (知识库)</a>

  </li>
        <li class="menu-item menu-item-music-(独立音乐人)">

    <a href="/sec_music" rel="section"><i class="fa fa-music fa-fw"></i>Music (独立音乐人)</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="en">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">PyTorch 数据加载源码分析
</h1>

<div class="post-meta">
  
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Tech_System_And_Network/">TECH_SYSTEM_AND_NETWORK</a></li>
          <li>PYTORCH_DATALOADER</li>
        
  </ul>

</div>

</header>

      
      
      
      <div class="post-body">
          <head>
<!--导入样式表-->
<link rel="stylesheet" type="text/css" href="style/index.css">

<!--导入网页脚本-->
<script src="script/index.js"></script>

<!--支持伪代码显示-->
<script>
    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3.0.0/es5/tex-chtml.js"
        integrity="sha256-3Fdoa5wQb+JYfEmTpQHx9sc/GuwpfC/0R9EpBki+mf8=" crossorigin>
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js">
</script>

<!--支持网页公式显示-->    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>

<!--支持矩阵显示-->
<script type="text/javascript">
  run_maths = function() {
    if (document.querySelector('[class*="cmath"]') !== null) {
      if (typeof (mjax_path)=='undefined') { mjax_path='https://cdn.jsdelivr.net/npm/mathjax@2'; }
      if (typeof (mjax_config)=='undefined') { mjax_config='AM_CHTML'; }
      smjax = document.createElement ('script');
      smjax.setAttribute('src',`${mjax_path}/MathJax.js?config=${mjax_config}`);
      smjax.setAttribute('async',true);
      document.getElementsByTagName('head')[0].appendChild(smjax);
    }
  };
  if (document.readyState === 'loading') {  
    window.addEventListener('DOMContentLoaded', run_maths); 
  } else { 
    run_maths(); 
  }
</script>
</head>

<body onload="load_page()">

<!-- 导入 mermaid -->
<script src="script/mermaid.min.js"></script>
<script>mermaid.initialize({startOnLoad:true});</script>

<!-- 本文的 Metadata -->
<div id="metadata"></div>

<!-- Start your post here -->
<h2 class="title">基本概念</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在本文中，我们将对 <code>torch.utils.data</code> 模块中的 <b>Dataset</b>，<b>BatchSampler</b> 和 <b>DataLoader</b> 三个用于 PyTorch 框架数据加载的关键实体进行分析。

  <h3 class="title">Python 中的迭代</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;理解 Python 中与迭代相关的概念，是理解 <code>torch.utils.data</code> 模块中的 <b>Dataset</b>，<b>Sampler</b> 和 <b>DataLoader</b> 三个实体的关键，因此如果读者朋友对 Python 中与迭代相关的概念尚不熟悉，建议先对我的另一篇文章 <a href="/sec_learning/Tech_Program/Python_Iteration/index.html">Python 中的迭代</a> 进行阅读。

  <h3 class="title">Python 中的多进程</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;我们在后面介绍 <b>DataLoader</b> 实体的相关内容的时候将会涉及到如何基于 Python 的多进程机制加速数据加载的过程，因此需要读者对 Python 的多进程机制有一定的了解。如果您对此不是特别熟悉，建议先对我的另一篇文章 <a href="/sec_learning/Tech_Program/Python_Multiprocessing/index.html">Python 的多进程</a> 进行阅读。

  <h3 class="title">数据加载</h3>
  <label class="title">dataloading</label>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先，模型的训练基于数据集，因此在训练过程中，有一部分的工作是要关心数据集如何从磁盘中被加载到 Host Memory / GPU Memory，数据集如何进行采样生成 Mini-batch，生成的 Mini-batch 如何被依次送进模型中进行训练，这也就是本文要关心的训练过程中被称为 <def>Dataloading (数据加载)</def> 的环节。本文把 Dataloading 中划分出了若干功能性实体。

  <div class="img" title="训练流程中的 DataSet, DataLoader 和 Sampler" label="img_dataloading">
    <img src="./pic/workflow.png" width="90%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<imgref>img_dataloading</imgref> 展示了 Dataloading 在整个训练过程中所处的位置。首先，我们的数据集被存放在磁盘中，<def><b>Dataset</b></def> 实体用于将 Dataset 从磁盘中读取到 Host Memory 中，并且提供了用于访问 Dataset 中各条 Sample 的 Feature 和 Label 的方法 (i.e. 对于 Map-style Dataset 来说提供了 <code>__getitem__(self)</code> 魔法方法; Iterable-style Dataset 则提供了 <code>__iter__(self)</code> 魔法方法)。在训练过程中，会有多轮 Epoches，每一轮 Epoch 会有多轮 Iterations，每一轮 Iteration 一个 Mini-batch 送入模型进行前向传播、损失值计算、参数梯度计算和参数更新，在每一轮 Epoch 中会完成一次对训练集 (Mini-batches) 的遍历。为了更好的实现对 Mini-batches 的遍历的编程抽象，<def><b>DataLoader</b></def> 实体应该提供一种 <def>Iterable (可迭代)</def> 的方法，使得我们在每轮 Epoch 中可以基于 Python <code>for mini-batch in DataLoader</code> 的范式完成对 Mini-batches 的遍历。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<b>DataLoader</b> 在为某轮 Epoch 生成 Mini-batches 的过程中，是按照什么规则生成 Mini-batches 的呢？可以是按顺序在 <b>Dataset</b> 中进行采样，也可以是按照随机的规则进行采样，生成 Mini-batches 的采样规则就是由 <def><b>BatchSampler</b></def> 实体予以实现的。具体来说，<b>BatchSampler</b> 实体基于一定的采样规则，把采样生成的 Samples 的索引返回给 <b>DataLoader</b>，最后由 <b>DataLoader</b> 中的 <def><b>Fetcher</b></def> 实体完成从 <b>Dataset</b> 中提取序号对应的 Samples 下的特征和标签数据，以最终完成 Mini-batches 的生成。另外，我们把一次迭代生成一条 Sample 的索引的采样器称为 <def><b>Sampler</b></def> 实体，生成多条 Samples 的索引的采样器则称为 <b>BatchSampler</b>，我们后面会看到，PyTorch 中使用 <b>Sampler</b> 实体来设置采样的规则 (e.g. 顺序采样，随机采样，etc.)，然后再基于 <b>Sampler</b> 结合 Batch Size 等设置生成 <b>BatchSampler</b> 实体。

  <div class="background">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;综上，<b>Dataset</b>，<b>BatchSampler</b> 和 <b>DataLoader</b> 可以总结为:

  <ul>
    <li><def><b>Dataset</b></def>: 将原始存储在磁盘中的数据集读取到内存中并封装为 Python 对应的对象，并且暴露提取接口;</li>
    <li><def><b>BatchSampler</b></def>: 在每次迭代时输出当前 Iteration 所使用的 Samples 的索引;</li>
    <li><def><b>DataLoader</b></def>: 基于设置好的 <b>Dataset</b> 和 <b>BatchSampler</b> 实体，在每次迭代时，<b>DataLoader</b> 将首先迭代 <b>BatchSampler</b> 实体以获得当前 Iteration 使用的 Samples 的索引，然后调用其 <b>Fetcher</b> 实体完成对应 Samples 的特征和标签数据的组装，最后进行输出。</li>
  </ul>

  <div class="img" title="DataLoader, Sampler 和 Dataset 三者关系图">
    <img src="./pic/cooperate.png" width="700px" />
  </div>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;下面我们分别对上述的功能性实体进行介绍。
</div> 

<h2 class="title">Dataset 功能性介绍</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<b>Dataset</b> 实体负责对来源于磁盘的 Raw Dataset 进行封装，将其封装成 Python 可识别的数据结构。<code>torch.utils.data</code> 模块提供了多种形式的 <b>Dataset</b> 实体抽象，并且分别提供了对应的接口类完成对这些类型的数据集的抽象，用户需要实现接口类中的相应接口，以完成对自定义数据集的封装。我们下面对这些数据集抽象分别进行分析。

  <div class="multi_img">
    <div class="img" title="Map-style Dataset" label="map_style_ds">
      <img src="./pic/map_style.png" height="300px" />
    </div>
    <div class="img" title="Iterable-style Dataset" label="iter_style_ds">
      <img src="./pic/iter_style.png" height="300px" />
    </div>
  </div>

  <h3 class="title">Map-style Dataset</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>torch.utils.data</code> 模块使用 <code>Dataset</code> 接口类抽象 Map-style 的数据集，Map-style 顾名思义就是数据集中的每一条 Sample 都拥有一个索引，用户可以通过索引的方式来获取 Samples。<code>Dataset</code> 的定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span>(<span class="params">Generic[T_co]</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>) -&gt; T_co:</span></span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__add__</span>(<span class="params">self, other: <span class="string">&#x27;Dataset[T_co]&#x27;</span></span>) -&gt; &#x27;ConcatDataset[T_co]&#x27;:</span></span><br><span class="line">      <span class="keyword">return</span> ConcatDataset([self, other])</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;可以发现，Map-style 的数据集通过定义 <code>__getitem__()</code> 魔法方法，实现了从索引到 Sample 的映射，使得用户可以使用 <code>dataset[idx]</code> 就可以访问 <code>idx</code> 对应的 Sample。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;细心的读者会发现在上面展示的 <code>Dataset</code> 接口类中并没有规定实现 <code>__len()__</code> 接口，原因是 <code>return NotImplemented</code> 或者 <code>raise NotImplementedError()</code> 之类的默认实现都会存在各自的问题，因此 <code>Dataset</code> 接口类把对 <code>__len()__</code> 接口的实现留给了子类。

  <h3 class="title">Iterable-style Dataset</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Iterable-style 的数据集是一种通过实现 <code>__iter__()</code> 来获取数据的 Dataset，这种类型的数据集特别适用于以下情况: ① 像 Map-style 数据集一样基于索引随机读取的代价很大甚至不大可能；或者 ② 每轮 Iteration 所使用的 Batch Size 并不固定，而取决于获取的数据。<code>torch.utils.data</code> 模块使用 <code>IterableDataset</code> 接口类抽象 Iterable-style 的数据集，其定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IterableDataset</span>(<span class="params">Dataset[T_co]</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[T_co]:</span></span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__add__</span>(<span class="params">self, other: Dataset[T_co]</span>):</span></span><br><span class="line">      <span class="keyword">return</span> ChainDataset([self, other])</span><br></pre></td></tr></table></figure>
  <h3 class="title">其它 Dataset</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;上述的 Map-style 和 Iterable-style 的 Dataset 是 PyTorch 中两种最主要的 Dataset，下面我们介绍几种基于它们的其它 Datasets。

  <h4 class="paragraph">Concat Dataset</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>ConcatDataset</code> 被用于级联多个数据集类，使得级联出来的数据集就像是一个统一大的数据集一样，可以基于索引/关键字对 Samples 进行访问，具体定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConcatDataset</span>(<span class="params">Dataset[T_co]</span>):</span></span><br><span class="line">  datasets: List[Dataset[T_co]]</span><br><span class="line">  cumulative_sizes: List[<span class="built_in">int</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">  @staticmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cumsum</span>(<span class="params">sequence</span>):</span></span><br><span class="line">      r, s = [], <span class="number">0</span></span><br><span class="line">      <span class="keyword">for</span> e <span class="keyword">in</span> sequence:</span><br><span class="line">          l = <span class="built_in">len</span>(e)</span><br><span class="line">          r.append(l + s)</span><br><span class="line">          s += l</span><br><span class="line">      <span class="keyword">return</span> r</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, datasets: Iterable[Dataset]</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      <span class="built_in">super</span>(ConcatDataset, self).__init__()</span><br><span class="line">      self.datasets = <span class="built_in">list</span>(datasets)</span><br><span class="line">      <span class="keyword">assert</span> <span class="built_in">len</span>(self.datasets) &gt; <span class="number">0</span>, <span class="string">&#x27;datasets should not be an empty iterable&#x27;</span>  <span class="comment"># type: ignore[arg-type]</span></span><br><span class="line">      <span class="keyword">for</span> d <span class="keyword">in</span> self.datasets:</span><br><span class="line">          <span class="keyword">assert</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(d, IterableDataset), <span class="string">&quot;ConcatDataset does not support IterableDataset&quot;</span></span><br><span class="line">      self.cumulative_sizes = self.cumsum(self.datasets)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">return</span> self.cumulative_sizes[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">      <span class="keyword">if</span> idx &lt; <span class="number">0</span>:</span><br><span class="line">          <span class="keyword">if</span> -idx &gt; <span class="built_in">len</span>(self):</span><br><span class="line">              <span class="keyword">raise</span> ValueError(<span class="string">&quot;absolute value of index should not exceed dataset length&quot;</span>)</span><br><span class="line">          idx = <span class="built_in">len</span>(self) + idx</span><br><span class="line">      dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)</span><br><span class="line">      <span class="keyword">if</span> dataset_idx == <span class="number">0</span>:</span><br><span class="line">          sample_idx = idx</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          sample_idx = idx - self.cumulative_sizes[dataset_idx - <span class="number">1</span>]</span><br><span class="line">      <span class="keyword">return</span> self.datasets[dataset_idx][sample_idx]</span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cummulative_sizes</span>(<span class="params">self</span>):</span></span><br><span class="line">      warnings.warn(<span class="string">&quot;cummulative_sizes attribute is renamed to &quot;</span></span><br><span class="line">                    <span class="string">&quot;cumulative_sizes&quot;</span>, DeprecationWarning, stacklevel=<span class="number">2</span>)</span><br><span class="line">      <span class="keyword">return</span> self.cumulative_sizes</span><br></pre></td></tr></table></figure>
  <h4 class="paragraph">Chain Dataset</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>ChainDataset</code> 被用于包含多个 <code>IterableDataset</code> 数据集，在 <code>IterableDataset</code> 的 <code>__add__()</code> 方法中被调用，具体定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChainDataset</span>(<span class="params">IterableDataset</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, datasets: Iterable[Dataset]</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      <span class="built_in">super</span>(ChainDataset, self).__init__()</span><br><span class="line">      self.datasets = datasets</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">for</span> d <span class="keyword">in</span> self.datasets:</span><br><span class="line">          <span class="keyword">assert</span> <span class="built_in">isinstance</span>(d, IterableDataset), <span class="string">&quot;ChainDataset only supports IterableDataset&quot;</span></span><br><span class="line">          <span class="keyword">for</span> x <span class="keyword">in</span> d:</span><br><span class="line">              <span class="keyword">yield</span> x</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">      total = <span class="number">0</span></span><br><span class="line">      <span class="keyword">for</span> d <span class="keyword">in</span> self.datasets:</span><br><span class="line">          <span class="keyword">assert</span> <span class="built_in">isinstance</span>(d, IterableDataset), <span class="string">&quot;ChainDataset only supports IterableDataset&quot;</span></span><br><span class="line">          total += <span class="built_in">len</span>(d)  <span class="comment"># type: ignore[arg-type]</span></span><br><span class="line">      <span class="keyword">return</span> total</span><br></pre></td></tr></table></figure>
  <h4 class="paragraph">Subset Dataset</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>Subset</code> 被用于将原有数据集指定下标的 Samples 封装为一个新的数据集，具体定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Subset</span>(<span class="params">Dataset[T_co]</span>):</span></span><br><span class="line">  dataset: Dataset[T_co]</span><br><span class="line">  indices: Sequence[<span class="built_in">int</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset: Dataset[T_co], indices: Sequence[<span class="built_in">int</span>]</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      self.dataset = dataset</span><br><span class="line">      self.indices = indices</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">isinstance</span>(idx, <span class="built_in">list</span>):</span><br><span class="line">          <span class="keyword">return</span> self.dataset[[self.indices[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]]</span><br><span class="line">      <span class="keyword">return</span> self.dataset[self.indices[idx]]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">len</span>(self.indices)</span><br></pre></td></tr></table></figure>
  <h4 class="paragraph">Tensor Dataset</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>TensorDataset</code> 用于获取封装成 Tensor 的数据集，每一个样本都通过索引张量来获得，具体代码如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TensorDataset</span>(<span class="params">Dataset[Tuple[Tensor, ...]]</span>):</span></span><br><span class="line">  tensors: Tuple[Tensor, ...]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *tensors: Tensor</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      <span class="keyword">assert</span> <span class="built_in">all</span>(tensors[<span class="number">0</span>].size(<span class="number">0</span>) == tensor.size(<span class="number">0</span>) <span class="keyword">for</span> tensor <span class="keyword">in</span> tensors), <span class="string">&quot;Size mismatch between tensors&quot;</span></span><br><span class="line">      self.tensors = tensors</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">tuple</span>(tensor[index] <span class="keyword">for</span> tensor <span class="keyword">in</span> self.tensors)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">return</span> self.tensors[<span class="number">0</span>].size(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</div>

<h2 class="title">BatchSampler 功能性介绍</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在我们定义好了 <b>Dataset</b> 实体抽象，已经拥有了一些接口可以获取 <b>Dataset</b> 中各条 Samples。在训练的各轮 Epoches 中，我们需要在每轮 Iteration 中从 <b>Dataset</b> 中读取单条 Sample (或多条 Samples 以形成 Mini-batch) 对模型进行训练，那么应该按照什么顺序来读取 <b>Dataset</b> 中的内容以形成这些 Mini-batches 呢？这也就是我们本节要讨论的 <b>BatchSampler</b> 实体抽象的工作。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Map-style Dataset 来说，如 <imgref>map_style_ds</imgref> 所示，每一条 Sample 都会有一个索引，<b>BatchSampler</b> 的功能就是在每轮 Epoch 的每轮 Iteration 中输出参与训练的 Samples 的 索引。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;而对于 Iteration-style Dataset 来说，如 <imgref>iter_style_ds</imgref> 所示，每一条 Sample 并不会有索引。正如我们上面所述，Iteration-style Dataset 需要基于 Iterator 的方法来完成对数据集中的 Samples 的访问，因此对 Iteration-style Dataset 中 Samples 的遍历顺序完全由其 Iterator 定义的 <code>__next__(self)</code> 魔法方法定义的访问顺序决定，因此本节讨论的 <b>BatchSampler</b> 实体并不对 Iteration-style Dataset 有效，因此<note>我们默认本章剩余篇幅讨论的都是 Map-style Dataset</note>。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;回顾我们在 <ref>dataloading</ref> 中描述的，<b>Sampler</b> 实体每次迭代输出一条 Sample 的索引，而 <b>BatchSampler</b> 实体每次迭代则输出多条 Samples 的索引。在 PyTorch 的实现中，通常实现定义出 <b>Sampler</b> 实体，以确定采样规则 (e.g. 随机采样，顺序采样，etc.)，然后再基于定义好的 <b>Sampler</b> 实体结合 Batch Size 等设置生成 <b>BatchSampler</b> 实体。

  <h3 class="title">Sampler 实体</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;我们下面首先来看 <b>Sampler</b> 实体的视线。在 <code>torch.utils.data</code> 模块中，<code>Sampler</code> 类定义了 <b>Sampler</b> 实体所应该拥有的接口，定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sampler</span>(<span class="params">Generic[T_co]</span>):</span></span><br><span class="line">  <span class="string">r&quot;&quot;&quot;Base class for all Samplers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Every Sampler subclass has to provide an :meth:`__iter__` method, providing a</span></span><br><span class="line"><span class="string">  way to iterate over indices of dataset elements, and a :meth:`__len__` method</span></span><br><span class="line"><span class="string">  that returns the length of the returned iterators.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  .. note:: The :meth:`__len__` method isn&#x27;t strictly required by</span></span><br><span class="line"><span class="string">            :class:`~torch.utils.data.DataLoader`, but is expected in any</span></span><br><span class="line"><span class="string">            calculation involving the length of a :class:`~torch.utils.data.DataLoader`.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_source: Optional[Sized]</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[T_co]:</span></span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;从上面的代码和注释中可以看到，<b>Sampler</b> 实体需要定义 <code>__iter__(self)</code> 魔法方法，以可以通过 <code>iter()</code> 的方法来获得 <b>Sampler</b> 的 Iterator，对 Iterator 进行的每次迭代可以得到的采样得到的单条 Sample 的序号。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于 <code>Sampler</code> 接口类，<code>torch.utils.data</code> 模块中提供了多种内置的 <b>Sampler</b> 实现，下面我们展示了其中的 <code>RandomSampler</code> 的相关定义。

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomSampler</span>(<span class="params">Sampler[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">  <span class="string">r&quot;&quot;&quot;Samples elements randomly. If without replacement, then sample from a shuffled dataset.</span></span><br><span class="line"><span class="string">  If with replacement, then user can specify :attr:`num_samples` to draw.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">      data_source (Dataset): dataset to sample from</span></span><br><span class="line"><span class="string">      replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``</span></span><br><span class="line"><span class="string">      num_samples (int): number of samples to draw, default=`len(dataset)`.</span></span><br><span class="line"><span class="string">      generator (Generator): Generator used in sampling.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  data_source: Sized</span><br><span class="line">  replacement: <span class="built_in">bool</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_source: Sized, replacement: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               num_samples: Optional[<span class="built_in">int</span>] = <span class="literal">None</span>, generator=<span class="literal">None</span></span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      self.data_source = data_source</span><br><span class="line">      self.replacement = replacement</span><br><span class="line">      self._num_samples = num_samples</span><br><span class="line">      self.generator = generator</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.replacement, <span class="built_in">bool</span>):</span><br><span class="line">          <span class="keyword">raise</span> TypeError(<span class="string">&quot;replacement should be a boolean value, but got &quot;</span></span><br><span class="line">                          <span class="string">&quot;replacement=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.replacement))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.num_samples, <span class="built_in">int</span>) <span class="keyword">or</span> self.num_samples &lt;= <span class="number">0</span>:</span><br><span class="line">          <span class="keyword">raise</span> ValueError(<span class="string">&quot;num_samples should be a positive integer &quot;</span></span><br><span class="line">                           <span class="string">&quot;value, but got num_samples=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.num_samples))</span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">num_samples</span>(<span class="params">self</span>) -&gt; int:</span></span><br><span class="line">      <span class="comment"># dataset size might change at runtime</span></span><br><span class="line">      <span class="keyword">if</span> self._num_samples <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          <span class="keyword">return</span> <span class="built_in">len</span>(self.data_source)</span><br><span class="line">      <span class="keyword">return</span> self._num_samples</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[int]:</span></span><br><span class="line">      n = <span class="built_in">len</span>(self.data_source)</span><br><span class="line">      <span class="keyword">if</span> self.generator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          seed = <span class="built_in">int</span>(torch.empty((), dtype=torch.int64).random_().item())</span><br><span class="line">          generator = torch.Generator()</span><br><span class="line">          generator.manual_seed(seed)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          generator = self.generator</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> self.replacement:</span><br><span class="line">          <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_samples // <span class="number">32</span>):</span><br><span class="line">              <span class="keyword">yield</span> <span class="keyword">from</span> torch.randint(high=n, size=(<span class="number">32</span>,), dtype=torch.int64, generator=generator).tolist()</span><br><span class="line">          <span class="keyword">yield</span> <span class="keyword">from</span> torch.randint(high=n, size=(self.num_samples % <span class="number">32</span>,), dtype=torch.int64, generator=generator).tolist()</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_samples // n):</span><br><span class="line">              <span class="keyword">yield</span> <span class="keyword">from</span> torch.randperm(n, generator=generator).tolist()</span><br><span class="line">          <span class="keyword">yield</span> <span class="keyword">from</span> torch.randperm(n, generator=generator).tolist()[:self.num_samples % n]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>) -&gt; int:</span></span><br><span class="line">      <span class="keyword">return</span> self.num_samples</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在 <code>RandomSampler</code> 的 <code>__iter__(self)</code> 魔法方法中我们可以看到，其返回的实际上是一个 Generator，采用 on-the-fly 的方式，随机地，可支持有放回地生成 Sample 的采样序号。

  <h3 class="title">BatchSampler 实体</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在 <code>torch.utils.data</code> 模块中，<code>BatchSampler</code> 类实现了 <b>BatchSampler</b> 实体所应该拥有的接口，定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchSampler</span>(<span class="params">Sampler[List[<span class="built_in">int</span>]]</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Wraps another sampler to yield a mini-batch of indices.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sampler (Sampler or Iterable): Base sampler. Can be any iterable object</span></span><br><span class="line"><span class="string">        batch_size (int): Size of mini-batch.</span></span><br><span class="line"><span class="string">        drop_last (bool): If ``True``, the sampler will drop the last batch if</span></span><br><span class="line"><span class="string">            its size would be less than ``batch_size``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))</span></span><br><span class="line"><span class="string">        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))</span></span><br><span class="line"><span class="string">        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, sampler: Union[Sampler[<span class="built_in">int</span>], Iterable[<span class="built_in">int</span>]], batch_size: <span class="built_in">int</span>, drop_last: <span class="built_in">bool</span></span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="comment"># Since collections.abc.Iterable does not check for `__getitem__`, which</span></span><br><span class="line">        <span class="comment"># is one way for an object to be an iterable, we don&#x27;t do an `isinstance`</span></span><br><span class="line">        <span class="comment"># check here.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(batch_size, <span class="built_in">int</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(batch_size, <span class="built_in">bool</span>) <span class="keyword">or</span> \</span><br><span class="line">                batch_size &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;batch_size should be a positive integer value, &quot;</span></span><br><span class="line">                            <span class="string">&quot;but got batch_size=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(batch_size))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(drop_last, <span class="built_in">bool</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;drop_last should be a boolean value, but got &quot;</span></span><br><span class="line">                            <span class="string">&quot;drop_last=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(drop_last))</span><br><span class="line">        self.sampler = sampler</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.drop_last = drop_last</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[List[int]]:</span></span><br><span class="line">        <span class="comment"># Implemented based on the benchmarking in https://github.com/pytorch/pytorch/pull/76951</span></span><br><span class="line">        <span class="keyword">if</span> self.drop_last:</span><br><span class="line">            sampler_iter = <span class="built_in">iter</span>(self.sampler)</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    batch = [<span class="built_in">next</span>(sampler_iter) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.batch_size)]</span><br><span class="line">                    <span class="keyword">yield</span> batch</span><br><span class="line">                <span class="keyword">except</span> StopIteration:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch = [<span class="number">0</span>] * self.batch_size</span><br><span class="line">            idx_in_batch = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> self.sampler:</span><br><span class="line">                batch[idx_in_batch] = idx</span><br><span class="line">                idx_in_batch += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> idx_in_batch == self.batch_size:</span><br><span class="line">                    <span class="keyword">yield</span> batch</span><br><span class="line">                    idx_in_batch = <span class="number">0</span></span><br><span class="line">                    batch = [<span class="number">0</span>] * self.batch_size</span><br><span class="line">            <span class="keyword">if</span> idx_in_batch &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">yield</span> batch[:idx_in_batch]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>) -&gt; int:</span></span><br><span class="line">        <span class="comment"># Can only be called if self.sampler has __len__ implemented</span></span><br><span class="line">        <span class="comment"># We cannot enforce this condition, so we turn off typechecking for the</span></span><br><span class="line">        <span class="comment"># implementation below.</span></span><br><span class="line">        <span class="comment"># Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]</span></span><br><span class="line">        <span class="keyword">if</span> self.drop_last:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(self.sampler) // self.batch_size  <span class="comment"># type: ignore[arg-type]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (<span class="built_in">len</span>(self.sampler) + self.batch_size - <span class="number">1</span>) // self.batch_size  <span class="comment"># type: ignore[arg-type]</span></span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;从上面的代码中可以看到，<code>BatchSampler</code> 定义了三个初始化参数:

  <ul>
    <li><code>sampler</code>: 当前创建的 <code>BatchSampler</code> 基于的 <b>Sampler</b>，用于确定采样的具体规则;</li>
    <li><code>batch_size</code>: 每一次对 <code>BatchSampler</code> 进行迭代 (i.e. 每一轮 Iteration) 时，<code>BatchSampler</code> 要输出的 Samples 的索引的数目;</li>
    <li><code>drop_last</code>: 布尔变量，指示是否丢弃最后一个规模达不到 <code>batch_size</code> 的 Mini-batch;</li>
  </ul>
</div>

<h2 class="title">DataLoader 功能性介绍</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;铺垫完了 <b>Dataset</b> 和 <b>BatchSampler</b> 两个实体后，现在我们来到了数据加载流程的核心 —— <b>DataLoader</b>。<code>torch.utils.data</code> 提供的 <code>DataLoader</code> 是 PyTorch 加载数据的核心，其支持 Map-style 和 Iterable-style 的 <b>Dataset</b> 的数据加载，支持单进程/多进程数据加载，还可以设置 loading order, batch size, pin memory 等加载参数的设置，可以认为是数据加载流程的统一入口。

  <h3 class="title">DataLoader 接口</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>DataLoader</code> 的接口定义如下：

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(</span><br><span class="line">  dataset: Dataset[T_co],</span><br><span class="line">  batch_size: Optional[<span class="built_in">int</span>] = <span class="number">1</span>,</span><br><span class="line">  shuffle: Optional[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span><br><span class="line">  sampler: Union[Sampler, Iterable, <span class="literal">None</span>] = <span class="literal">None</span>,</span><br><span class="line">  batch_sampler: Union[Sampler[Sequence], Iterable[Sequence], <span class="literal">None</span>] = <span class="literal">None</span>,</span><br><span class="line">  num_workers: <span class="built_in">int</span> = <span class="number">0</span>,</span><br><span class="line">  collate_fn: Optional[_collate_fn_t] = <span class="literal">None</span>,</span><br><span class="line">  pin_memory: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">  drop_last: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">  timeout: <span class="built_in">float</span> = <span class="number">0</span>,</span><br><span class="line">  worker_init_fn: Optional[_worker_init_fn_t] = <span class="literal">None</span>,</span><br><span class="line">  multiprocessing_context=<span class="literal">None</span>,</span><br><span class="line">  generator=<span class="literal">None</span>,</span><br><span class="line">  *,</span><br><span class="line">  prefetch_factor: <span class="built_in">int</span> = <span class="number">2</span>,</span><br><span class="line">  persistent_workers: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">  pin_memory_device: <span class="built_in">str</span> = <span class="string">&quot;&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;接口参数整理如下所示:

  <div class="table" title="DataLoader 接口参数说明">
  <table>
    <tr>
      <th align="center">Attribute</th>
      <th align="center">Description</th>
      <th align="center">Default Value</th>
      <th align="center">Type</th>
    </tr>
    <tr>
      <td><code>dataset</code></td>
      <td>要加载的 <b>Dataset</b> 实体</td>
      <td></td>
      <td><code>Dataset</code></td>
    </tr>
    <tr>
      <td><code>batch_size</code></td>
      <td>每轮 Iteration 要加载的 Samples 的数目</td>
      <td>$1$</td>
      <td><code>int</code></td>
    </tr>
    <tr>
      <td><code>shuffle</code></td>
      <td>设置为 <code>True</code> 时，将调用 <code>RandomSampler</code> 进行随机索引</td>
      <td><code>False</code></td>
      <td><code>bool</code></td>
    </tr>
    <tr>
      <td><code>sampler</code></td>
      <td>用户指定的 <b>Sampler</b> 实体，定义从 <note>Map-style</note> Dataset 中提取样本的策略，每一次 <code>yield</code> <note>一条</note> Sample 的索引。如果指定了 <code>sampler</code>, 则上述 <code>shuffle</code> 参数必须为 <code>False</code>，否则会和 <code>RandomSampler</code> 互斥</td>
      <td><code>None</code></td>
      <td><code>Sampler</code>, <code>Iterable</code></td>
    </tr>
    <tr>
      <td><code>batch_sampler</code></td>
      <td>用户指定的 <b>BatchSampler</b> 实体，定义从 <note>Map-style</note> Dataset 中提取样本的策略，每一次 <code>yield</code> <note>多条</note> Samples 的索引</td>
      <td><code>None</code></td>
      <td><code>Sampler</code>, <code>Iterable</code></td>
    </tr>
    <tr>
      <td><code>num_workers</code></td>
      <td>要用于数据加载的子进程数，$0$ 表示仅在主进程中加载数据</td>
      <td>$0$</td>
      <td><code>int</code></td>
    </tr>
    <tr>
      <td><code>collate_fn</code></td>
      <td>在将 Map-style Dataset 取出的数据整合成最终 Mini-batch 时使用</td>
      <td><code>None</code></td>
      <td><code>callable</code></td>
    </tr>
    <tr>
      <td><code>pin_memory</code></td>
      <td>如果为 <code>True</code>，则 <code>DataLoader</code> 在将张量返回之前将其复制到 CUDA 的锁页内存中</td>
      <td><code>False</code></td>
      <td><code>bool</code></td>
    </tr>
    <tr>
      <td><code>drop_last</code></td>
      <td>若设置为 <code>True</code>，则当该数据集大小 (i.e. <code>len(dataset)</code>) 不能被该批次大小 (i.e. <code>batch_size</code>) 整除时，删除最后一个不完整的批次；如果为 <code>False</code> 并且数据集的大小不能被批次大小整除，那么最后一批将较小</td>
      <td><code>False</code></td>
      <td><code>bool</code></td>
    </tr>
    <tr>
      <td><code>timeout</code></td>
      <td>如果为正数，则为从 Worker 构建 Mini-batch 的超时值，应始终为非负数。超过这个时间还没从 Worker 读取到数据的话就会报错</td>
      <td>$0$</td>
      <td><code>numeric</code></td>
    </tr>
    <tr>
      <td><code>worker_init_fn</code></td>
      <td>如果不为 <code>None</code>，它将会被每个 Worker 子进程调用。该函数将以 Worker Index (i.e. [0, <code>num_workers</code> - 1] 内的整形) 作为输入</td>
      <td><code>None</code></td>
      <td><code>callable</code></td>
    </tr>
    <tr>
      <td><code>prefetch_factor</code></td>
      <td>每个 Worker 提前加载的 Sample 数量</td>
      <td>$2$</td>
      <td><code>int</code></td>
    </tr>
    <tr>
      <td><code>persistent_workers</code></td>
      <td>如果为 <code>True</code>，则 <code>Dataloader</code> 将不会终止 Worker 进程，直到对 <b>Dataset</b> 的迭代完成</td>
      <td><code>False</code></td>
      <td><code>bool</code></td>
    </tr>
  </table>
  </div>

  <h3 class="title">自动化批处理 (Automatic Batching)</h3>
  <div class="img" title="自动批处理流程">
    <img src="./pic/dataloader_wf.png" width="90%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>DataLoader</code> 非常方便地为用户提供了 <def>自动化批处理 (Automatic Batching)</def> 的功能，通过指定我们上面看到的相关接口中的参数，用户可以自定义出他们想要的 <b>DataLoader</b> 实体，然后在训练主进程中通过迭代的方式获取每轮 Iteration 指定规模和指定顺序的 Samples。我们可以把自动化批处理的设置氛围两个部分: ① 设置 BatchSampler; 以及 ② 设置基于 Sampled Indices 合成 Mini-batch 的方法，我们下面分别进行介绍。

  <h4 class="paragraph">设置 BatchSampler</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于 SGD 的训练思路，通常在一轮 Iteration 中会使用多个 Samples 组成的 Mini-batch 进行训练，而通常不会只有一条 Sample 参与训练，<code>DataLoader</code> 提供了相关的参数进行 <b>BatchSampler</b> 相关的设置:

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Map-style Dataset 和 Iterable-style Dataset 来说，当 <code>batch_size</code> (默认为 $1$) 不为 <code>None</code> 时，生成的 <b>DataLoader</b> 在每一次被迭代时将 <code>yield</code> 出一批 Samples，而不只是一个单独的 Sample，该参数与 <code>drop_last</code> 和 <code>shuffle</code> 参数配合，将决定每一轮 Iteration 所使用的 Samples 的顺序和数目。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在 <code>DataLoader</code> 的构造函数中，相关代码会基于 <code>batch_size</code> 和 <code>drop_last</code> 参数，结合用户指定的 <b>Sampler</b> 实体构造出对应的 <b>BatchSampler</b> 实体。那么 <b>Sampler</b> 实体是如何被指定的呢？对于 Map-style Dataset 来说，可以通过两种方式被指定: 一种是通过 <code>sampler</code> 参数显式指定 <b>Sampler</b>，一种是通过 <code>shuffle</code> 参数，当它为 <code>True</code> 时使用 <code>RandomSampler</code>; 而对于 Iterable-style Dataset 来说，<b>Sampler</b> 实体并无意义，为了实现代码的兼容性，<code>DataLoader</code> 的构造函数会使用专门针对于 <code>Iterable-style Dataset</code> 提供的 Dummy Infinite Sampler <code>_InfiniteConstantSampler</code> 以充当 <b>Sampler</b> 实体，该 <b>Sampler</b> 可以无限地被迭代，其实质上是调用了 Iterable-style Dataset 的 <code>__iter__(self)</code> 魔法方法。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;单独对于 Map-style Dataset 来说，用户还可以使用 <code>batch_sampler</code> 参数来直接设置 Mini-batch 的 <b>BatchSampler</b>。

  <h4 class="paragraph">设置基于 Sampled Indices 合成 Mini-batch 的方法</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在基于指定的 <b>BatchSampler</b> 实体 <code>yield</code> 出一批某轮 Iteration 使用的 Samples 的 Indices 后，接下来就需要由 <code>collate_fn</code> 参数指定的 <def>Collate Function (整理函数)</def> 基于指定的 Indices 对应的 Samples 数据合成最终的 Mini-batch。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Map-style Dataset 来说，Collate Function 流程可以抽象如下:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> indices <span class="keyword">in</span> batch_sampler:</span><br><span class="line">    <span class="keyword">yield</span> collate_fn([dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices])</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Iteration-style Dataset 来说，Collate Function 流程可以抽象如下:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset_iter = <span class="built_in">iter</span>(dataset)</span><br><span class="line"><span class="keyword">for</span> indices <span class="keyword">in</span> batch_sampler:</span><br><span class="line">    <span class="keyword">yield</span> collate_fn([<span class="built_in">next</span>(dataset_iter) <span class="keyword">for</span> _ <span class="keyword">in</span> indices])</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;具体的 Collate Function 的详细设置细节可见 <ref>collate</ref>。

  <h3 class="title">关闭自动批处理</h3>
  <div class="img" title="关闭自动批处理流程">
    <img src="./pic/dataloader_wf_1.png" width="90%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;当用户想用 <b>Dataset</b> 的代码手动处理 Batch，或每轮 Iteration 仅基于单条 Sample 进行训练时，可将 <code>batch_size</code> 和 <code>batch_sampler</code> 两个参数同时设为 <code>None</code>, 此时 <code>DataLoader</code> 将关闭自动批处理，对 <code>DataLoader</code> 的迭代将使得其通过 <b>Sampler</b> 实体获得单条 Sample 的索引，然后将该索引对应的 Sample 数据交给 <code>collate_fn</code> 处理，以获得最终的 <code>DataLoader</code> 输出。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Map-style Dataset 来说，Collate Function 流程可以抽象如下:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> sampler:</span><br><span class="line">    <span class="keyword">yield</span> collate_fn(dataset[index])</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Iteration-style Dataset 来说，Collate Function 流程可以抽象如下:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> <span class="built_in">iter</span>(dataset):</span><br><span class="line">    <span class="keyword">yield</span> collate_fn(data)</span><br></pre></td></tr></table></figure>
  <h3 class="title">Collate Function</h3>
  <label class="title">collate</label>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;上面说到，Collate Function 的输入是从 <b>Dataset</b> 实体中获取的 Sample(s) 的数据，其输出就是对 <code>DataLoader</code> 迭代所获得的输出，在开启/关闭自动批处理时，它的运行逻辑稍有不同。

  <h4 class="paragraph">关闭自动批处理时的情况</h4>

  <div class="img" title="关闭自动批处理的 Collate 函数 (以各条 Sample 的形式是 dict 为例)" label="img_batch_collate_single" >
    <img src="./pic/collate_single.png" width="90%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;如 <imgref>img_batch_collate_single</imgref> 所示，当关闭自动批处理时，<code>collate_fn</code> 仅作用于单个 Sample，其工作就是简单地将 NumPy <code>arrays</code> 转化为 PyTorch 的 <code>Tensor</code>，在转换过程中保留了 Sample 原有的数据结构，图中展示了当 Sample 的数据结构是 <code>dict</code> 时的情况。

  <h4 class="paragraph">开启自动批处理时的情况</h4>

  <div class="img" title="开启自动批处理的 Collate 函数 (以各条 Sample 的形式是 dict 为例)" label="img_batch_collate" >
    <img src="./pic/collate.png" width="90%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;而当开启自动批处理时，如 <imgref>img_batch_collate</imgref> 所示，<code>collate_fn</code> 作用于多个 Samples，其将输入样本整理为一个 Batch，并将其 <code>yield</code> 回当前轮次的 Itertaion 以供训练。为了将输入样本整理为一个 Batch，<code>collate_fn</code> 的默认值 <code>default_collate()</code> 做了下面 $3$ 件事情:

  <ul>
    <li>追加 (Prepend) 一个新的维度作为 Batch Dimension (长度即为 Batch 的大小);</li>
    <li>将 NumPy <code>arrays</code> 和 Python Numberical Values 转化为 PyTorch 的 <code>Tensor</code>;</li>
    <li>保留输入的 Samples 中各条 Sample 的数据结构，如 <imgref>img_batch_collate</imgref> 所示，比如各条 Sample 是 <code>dict</code> 时，<code>default_collate()</code> 将输出具有相同 Keys，且处理过的 Batched Tensor (或 Batched List，当无法转化为 Tensor 的时候) 作为值的 <code>dict</code>。当各条 Sample 是 <code>list</code>、<code>tuple</code> 和 <code>namedtuple</code> 等时同理;</li>
  </ul>

  <h3 class="title">单进程数据加载</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;当设置 <code>DataLoader</code> 的 <code>num_workers</code> 为 $0$ (默认值) 时，则 <code>DataLoader</code> 的初始化进程和读取数据的进程是一样的，此时数据加载可能会导致主进程阻塞。当用于在进程之间共享数据的资源 (例如共享内存，文件描述符) 有限时，或者当整个数据集很小并且可以完全加载到内存中时，此模式可能是首选。此外，单进程加载通常显示更多可读的错误跟踪，因此对于调试很有用。

  <h3 class="title">多进程数据加载</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在多进程模式下，每轮 Iteration 开始时对 <code>DataLoader</code> 进行迭代时，都会创建 <code>num_workers</code> 个 <def>工作进程 (Worker)</def>，<code>dataset</code>, <code>collate_fn</code>, <code>worker_init_fn</code> 等参数都会被传到各个 Worker 中，各个 Worker 使用这些参数进行初始化和数据的读取和整理。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>torch.utils.data</code> 模块提供了 <code>get_work_info</code> 函数用于在各个 Worker 进程中获取各个进程相关的信息，包括 Worker 的 ID，Dataset 的 Replica，以及 Initial Seed 等，在主进程而不是 Worker 进程中调用 <code>get_work_info</code> 函数将返回 <code>None</code>。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Map-style 的 Dataset 来说，主进程将会利用 <b>Sampler</b> (<b>BatchSampler</b>) 实体生成 Indices，然后将生成的 Indices 发送给各个 Worker 进程，也即「选择 Sample」这件事情是在主进程完成的，而「根据选择结果加载数据」这件事情是在各个 Worker 进程中完成的。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Iterable-style 的 Dataset 来说，主进程中将不会进行生成 Indices 的操作，在各个 Worker 进程中会有一份 Dataset 的 Replica，各个 Worker 进程可以基于 <code>get_work_info</code> 函数获得的信息，对各份 Replica 进行不同的操作。

  <h3 class="title">Pinned Memory (锁页内存)</h3>
  <label class="title">pinned_memory</label>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;我的另一篇文章 <a href="/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_Memory_Management/index.html">CUDA 内存管理</a> 对 CUDA Pinned Memory 相关内容进行了介绍，如果您相关内容不熟悉，可以先移步阅读。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Host Memory 中的 Memory Page 有两种存在方式，一是 <def>锁页 (Pinned)</def>，二是 <def>不锁页 (Unpinned)</def>，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换，而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。主机在向 GPU 拷贝数据时，CUDA 要求数据必须放在 Pinned Host Memory 中，因此如果数据不在 Pinned Memory 中，会在 Host Memory 中经历从 Unpinned Memory 到 Pinned Memory 的拷贝过程。

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pin_memory</span>(<span class="params">data, device=<span class="literal">None</span></span>):</span></span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, torch.Tensor):</span><br><span class="line">      <span class="keyword">return</span> data.pin_memory(device)</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, string_classes):</span><br><span class="line">      <span class="keyword">return</span> data</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, collections.abc.Mapping):</span><br><span class="line">      <span class="keyword">try</span>:</span><br><span class="line">          <span class="keyword">return</span> <span class="built_in">type</span>(data)(&#123;k: pin_memory(sample, device) <span class="keyword">for</span> k, sample <span class="keyword">in</span> data.items()&#125;)  <span class="comment"># type: ignore[call-arg]</span></span><br><span class="line">      <span class="keyword">except</span> TypeError:</span><br><span class="line">          <span class="comment"># The mapping type may not support `__init__(iterable)`.</span></span><br><span class="line">          <span class="keyword">return</span> &#123;k: pin_memory(sample, device) <span class="keyword">for</span> k, sample <span class="keyword">in</span> data.items()&#125;</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="built_in">hasattr</span>(data, <span class="string">&#x27;_fields&#x27;</span>):  <span class="comment"># namedtuple</span></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">type</span>(data)(*(pin_memory(sample, device) <span class="keyword">for</span> sample <span class="keyword">in</span> data))</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, <span class="built_in">tuple</span>):</span><br><span class="line">      <span class="keyword">return</span> [pin_memory(sample, device) <span class="keyword">for</span> sample <span class="keyword">in</span> data]  <span class="comment"># Backwards compatibility.</span></span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, collections.abc.Sequence):</span><br><span class="line">      <span class="keyword">try</span>:</span><br><span class="line">          <span class="keyword">return</span> <span class="built_in">type</span>(data)([pin_memory(sample, device) <span class="keyword">for</span> sample <span class="keyword">in</span> data])  <span class="comment"># type: ignore[call-arg]</span></span><br><span class="line">      <span class="keyword">except</span> TypeError:</span><br><span class="line">          <span class="comment"># The sequence type may not support `__init__(iterable)` (e.g., `range`).</span></span><br><span class="line">          <span class="keyword">return</span> [pin_memory(sample, device) <span class="keyword">for</span> sample <span class="keyword">in</span> data]</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">hasattr</span>(data, <span class="string">&quot;pin_memory&quot;</span>):</span><br><span class="line">      <span class="keyword">return</span> data.pin_memory()</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;PyTorch 为存储在 Host Memory 中的 Tensor 提供了 <code>pin_memory()</code> 方法，其定义如上所示，该方法返回操作的 Tensor 的副本，并将数据放在 Pinned Memory 中。对于放在 Pinned Memory 中的 Tensor，用户可以使用 <def>异步 GPU 拷贝 (Asynchronous GPU copies</def> 以将 Tensor 拷贝至 GPU 内存中，做法是在 Tensor 的 <code>to()</code> 方法中加上参数 <code>non_blocking=True</code>，以实现数据传输和 Host 计算两者的 Overlapping <cite>torch_pinned_memory</cite>。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 <code>DataLoader</code> 来说，我们可以设置传入参数 <code>pin_memory=True</code>，以设置 <code>DataLoader</code> 将每次迭代返回的 Tensor 都放置到 Pinned Memory 中，以缩减数据在 Host Memory 和 GPU Memory 之间的拷贝时间。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;另外，从上面关于 <code>pin_memory()</code> 方法的代码定义中可以看到，如果传入该函数的是一个自定义的数据类型，则该函数会直接返回该数据，而不做任何 Pinning 相关的处理。对于 <code>DataLoader</code> 来说，当我们使用 <code>collate_fn</code> 指定了自定义的整理函数并且该整理函数返回了自定义类型的 Batch 数据，则当我们指定 <code>DataLoader</code> 的 <code>pin_memory=True</code> 时，则会导致 Memory Pinning 的操作并不会生效的情况。为了解决这种情况，我们需要手动地为传入 <code>collate_fn</code> 的数据添加与 Memory Pinning 相关的代码，具体示例代码如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleCustomBatch</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data</span>):</span></span><br><span class="line">        transposed_data = <span class="built_in">list</span>(<span class="built_in">zip</span>(*data))</span><br><span class="line">        self.inp = torch.stack(transposed_data[<span class="number">0</span>], <span class="number">0</span>)</span><br><span class="line">        self.tgt = torch.stack(transposed_data[<span class="number">1</span>], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># custom memory pinning method on custom type</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pin_memory</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.inp = self.inp.pin_memory()</span><br><span class="line">        self.tgt = self.tgt.pin_memory()</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_wrapper</span>(<span class="params">batch</span>):</span></span><br><span class="line">    <span class="keyword">return</span> SimpleCustomBatch(batch)</span><br><span class="line"></span><br><span class="line">inps = torch.arange(<span class="number">10</span> * <span class="number">5</span>, dtype=torch.float32).view(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">tgts = torch.arange(<span class="number">10</span> * <span class="number">5</span>, dtype=torch.float32).view(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">dataset = TensorDataset(inps, tgts)</span><br><span class="line"></span><br><span class="line">loader = DataLoader(dataset, batch_size=<span class="number">2</span>, collate_fn=collate_wrapper,</span><br><span class="line">                    pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_ndx, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">    print(sample.inp.is_pinned())</span><br><span class="line">    print(sample.tgt.is_pinned())</span><br></pre></td></tr></table></figure>
</div>

<h2 class="title">DataLoader 源码解析</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于上一节对 PyTorch 提供的 <code>DataLoader</code> 有了功能性的认识后，本节我们将对其源码按顺序进行分析。

  <h3 class="title">对 <code>DataLoader</code> 进行迭代</h3>

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先在主进程中，我们会使用如上所示的代码对 <code>DataLoader</code> 进行遍历，此时会调用它的 <code>__iter__(self)</code> 魔法方法，该方法定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params">Generic[T_co]</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>) -&gt; &#x27;_BaseDataLoaderIter&#x27;:</span></span><br><span class="line">      <span class="keyword">if</span> self.persistent_workers <span class="keyword">and</span> self.num_workers &gt; <span class="number">0</span>:</span><br><span class="line">          <span class="comment"># 对于多进程数据加载的情况</span></span><br><span class="line">          <span class="keyword">if</span> self._iterator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">              <span class="comment"># 第一次发起遍历，则创建 iterator</span></span><br><span class="line">              self._iterator = self._get_iterator()</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">              <span class="comment"># 不是第一次发起遍历，则重置 iterator</span></span><br><span class="line">              self._iterator._reset(self)</span><br><span class="line">          <span class="keyword">return</span> self._iterator</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># 对于多进程数据加载的情况</span></span><br><span class="line">          <span class="keyword">return</span> self._get_iterator()</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在上面的代码中可以看见其调用了 <code>DataLoader</code> 类下的 <code>_get_iterator</code> 获取 Iterator，具体代码如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params">Generic[T_co]</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_iterator</span>(<span class="params">self</span>) -&gt; &#x27;_BaseDataLoaderIter&#x27;:</span></span><br><span class="line">      <span class="keyword">if</span> self.num_workers == <span class="number">0</span>:</span><br><span class="line">          <span class="keyword">return</span> _SingleProcessDataLoaderIter(self)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          self.check_worker_number_rationality()</span><br><span class="line">          <span class="keyword">return</span> _MultiProcessingDataLoaderIter(self)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;可以看到其根据单/多进程数据读取的不同情况，返回了不同的 Iterator，我们下面分情况进行讨论。

  <h3 class="title"><code>DataLoader</code> 的 Iterator</h3>

  <h4 class="title">Iterator 基类: <code>_BaseDataLoaderIter</code> </h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先，不论是单进程数据加载所使用的迭代器 <code>_SingleProcessDataLoaderIter</code>，还是多进程数据加载所使用的迭代器 <code>_MultiProcessingDataLoaderIter</code>，他们都继承自 <code>_BaseDataLoaderIter</code> 基类，其源码如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_BaseDataLoaderIter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, loader: DataLoader</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      self._dataset = loader.dataset</span><br><span class="line">      self._shared_seed = loader._get_shared_seed()</span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">isinstance</span>(self._dataset, IterDataPipe):</span><br><span class="line">          shared_rng = torch.Generator()</span><br><span class="line">          shared_rng.manual_seed(self._shared_seed)</span><br><span class="line">          self._dataset = torch.utils.data.graph_settings.apply_shuffle_seed(self._dataset, shared_rng)</span><br><span class="line">      self._dataset_kind = loader._dataset_kind</span><br><span class="line">      self._IterableDataset_len_called = loader._IterableDataset_len_called</span><br><span class="line">      self._auto_collation = loader._auto_collation</span><br><span class="line">      self._drop_last = loader.drop_last</span><br><span class="line">      self._index_sampler = loader._index_sampler</span><br><span class="line">      self._num_workers = loader.num_workers</span><br><span class="line">      self._prefetch_factor = loader.prefetch_factor</span><br><span class="line">      <span class="comment"># for other backends, pin_memory_device need to set. if not set</span></span><br><span class="line">      <span class="comment"># default behaviour is CUDA device. if pin_memory_device is selected</span></span><br><span class="line">      <span class="comment"># and pin_memory is not set, the default behaviour false.</span></span><br><span class="line">      <span class="keyword">if</span> (<span class="built_in">len</span>(loader.pin_memory_device) == <span class="number">0</span>):</span><br><span class="line">          self._pin_memory = loader.pin_memory <span class="keyword">and</span> torch.cuda.is_available()</span><br><span class="line">          self._pin_memory_device = <span class="literal">None</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">if</span> <span class="keyword">not</span> loader.pin_memory:</span><br><span class="line">              warn_msg = (<span class="string">&quot;pin memory device is set and pin_memory flag is not used then device pinned memory won&#x27;t be used&quot;</span></span><br><span class="line">                          <span class="string">&quot;please set pin_memory to true, if you need to use the device pin memory&quot;</span>)</span><br><span class="line">              warnings.warn(warn_msg)</span><br><span class="line"></span><br><span class="line">          self._pin_memory = loader.pin_memory</span><br><span class="line">          self._pin_memory_device = loader.pin_memory_device</span><br><span class="line">      self._timeout = loader.timeout</span><br><span class="line">      self._collate_fn = loader.collate_fn</span><br><span class="line">      self._sampler_iter = <span class="built_in">iter</span>(self._index_sampler)</span><br><span class="line">      self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()</span><br><span class="line">      self._persistent_workers = loader.persistent_workers</span><br><span class="line">      self._num_yielded = <span class="number">0</span></span><br><span class="line">      self._profile_name = <span class="string">&quot;enumerate(DataLoader)#&#123;&#125;.__next__&quot;</span>.<span class="built_in">format</span>(self.__class__.__name__)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>) -&gt; &#x27;_BaseDataLoaderIter&#x27;:</span></span><br><span class="line">      <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_reset</span>(<span class="params">self, loader, first_iter=<span class="literal">False</span></span>):</span></span><br><span class="line">      self._sampler_iter = <span class="built_in">iter</span>(self._index_sampler)</span><br><span class="line">      self._num_yielded = <span class="number">0</span></span><br><span class="line">      self._IterableDataset_len_called = loader._IterableDataset_len_called</span><br><span class="line">      self._shared_seed = loader._get_shared_seed()</span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">isinstance</span>(self._dataset, IterDataPipe):</span><br><span class="line">          shared_rng = torch.Generator()</span><br><span class="line">          shared_rng.manual_seed(self._shared_seed)</span><br><span class="line">          self._dataset = torch.utils.data.graph_settings.apply_shuffle_seed(self._dataset, shared_rng)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_next_index</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">next</span>(self._sampler_iter)  <span class="comment"># may raise StopIteration</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_next_data</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__next__</span>(<span class="params">self</span>) -&gt; Any:</span></span><br><span class="line">      <span class="keyword">with</span> torch.autograd.profiler.record_function(self._profile_name):</span><br><span class="line">          <span class="keyword">if</span> self._sampler_iter <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">              <span class="comment"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span></span><br><span class="line">              self._reset()  <span class="comment"># type: ignore[call-arg]</span></span><br><span class="line">          data = self._next_data()</span><br><span class="line">          self._num_yielded += <span class="number">1</span></span><br><span class="line">          <span class="keyword">if</span> self._dataset_kind == _DatasetKind.Iterable <span class="keyword">and</span> \</span><br><span class="line">                  self._IterableDataset_len_called <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> \</span><br><span class="line">                  self._num_yielded &gt; self._IterableDataset_len_called:</span><br><span class="line">              warn_msg = (<span class="string">&quot;Length of IterableDataset &#123;&#125; was reported to be &#123;&#125; (when accessing len(dataloader)), but &#123;&#125; &quot;</span></span><br><span class="line">                          <span class="string">&quot;samples have been fetched. &quot;</span>).<span class="built_in">format</span>(self._dataset, self._IterableDataset_len_called,</span><br><span class="line">                                                                self._num_yielded)</span><br><span class="line">              <span class="keyword">if</span> self._num_workers &gt; <span class="number">0</span>:</span><br><span class="line">                  warn_msg += (<span class="string">&quot;For multiprocessing data-loading, this could be caused by not properly configuring the &quot;</span></span><br><span class="line">                               <span class="string">&quot;IterableDataset replica at each worker. Please see &quot;</span></span><br><span class="line">                               <span class="string">&quot;https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.&quot;</span>)</span><br><span class="line">              warnings.warn(warn_msg)</span><br><span class="line">          <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">  <span class="built_in">next</span> = __next__  <span class="comment"># Python 2 compatibility</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>) -&gt; int:</span></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">len</span>(self._index_sampler)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getstate__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span> add limited pickling support for sharing an iterator</span></span><br><span class="line">      <span class="comment"># across multiple threads for HOGWILD.</span></span><br><span class="line">      <span class="comment"># Probably the best way to do this is by moving the sample pushing</span></span><br><span class="line">      <span class="comment"># to a separate thread and then just sharing the data queue</span></span><br><span class="line">      <span class="comment"># but signalling the end is tricky without a non-blocking API</span></span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;&#123;&#125; cannot be pickled&quot;</span>, self.__class__.__name__)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在 <code>_BaseDataLoaderIter</code> 中定义了 <code>__next__(self)</code> 函数，我们在主进程中使用 <code>for</code> 循环迭代 <code>DataLoader</code> 时，首先其会调用 <code>DataLoader</code> 的 <code>__iter__(self)</code> 魔法方法以获得 Iterator，而 <code>DataLoader</code> 的 Iterators 实现都继承自 <code>_BaseDataLoaderIter</code>，因此在获取完 Iterator 后 <code>for</code> 循环实际上就是通过不断调用 <code>_BaseDataLoaderIter</code> 的 <code>__next__(self)</code> 以获得下一批用于训练的 Batched Tensor。从上面的代码中可以看到，<code>__next__(self)</code> 则是调用 <code>_next_data()</code> 以获取相关数据，而后者留给继承自 <code>_BaseDataLoaderIter</code> 的子类予以实现。

  <h4 class="title">单进程加载 Iterator: <code>_BaseDataLoaderIter</code> 类</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在单进程数据加载的设定下，代表 Iterator 的类是  <code>_SingleProcessDataLoaderIter</code>，其定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_SingleProcessDataLoaderIter</span>(<span class="params">_BaseDataLoaderIter</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, loader</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(_SingleProcessDataLoaderIter, self).__init__(loader)</span><br><span class="line">        <span class="keyword">assert</span> self._timeout == <span class="number">0</span></span><br><span class="line">        <span class="keyword">assert</span> self._num_workers == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self._dataset_fetcher = _DatasetKind.create_fetcher(</span><br><span class="line">            self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_next_data</span>(<span class="params">self</span>):</span></span><br><span class="line">        index = self._next_index()  <span class="comment"># may raise StopIteration</span></span><br><span class="line">        data = self._dataset_fetcher.fetch(index)  <span class="comment"># may raise StopIteration</span></span><br><span class="line">        <span class="keyword">if</span> self._pin_memory:</span><br><span class="line">            data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先从 <code>_SingleProcessDataLoaderIter</code> 的初始化参数可以看到，其在父类 <code>_BaseDataLoaderIter</code> 的基础上定义了 <code>_dataset_fetcher</code>, 并传入 <code>_dataset</code>, <code>_auto_collation</code>, <code>_collate_fn</code> 等参数，该类用于根据指定的 Indices 来 Fetch 对应的 Samples，事实上就是我们前文提到过的 <b>Fetcher</b> 实体，我们在后面会对其源码进行分析。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;其次可以看见 <code>_SingleProcessDataLoaderIter</code> 实现了具体的 <code>_next_data()</code> 方法，其需要 <code>next_index()</code> 来获取要 Fetch 的 Samples 的 Indices，并将 Indices 传入 <code>_dataset_fetcher</code> 中以获取对应样本。

  <h4 class="title">多进程加载 Iterator: <code>_MultiProcessingDataLoaderIter</code> 类</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;当用户创建 <code>dataloader</code> 时传入的 <code>num_workers</code> 大于 $1$ 的时候，对 <code>dataloader</code> 的迭代操作就会基于 <code>_MultiProcessingDataLoaderIter</code> 类进行，我们下面首先对 <code>_MultiProcessingDataLoaderIter</code> 类的构造函数进行分析。

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_MultiProcessingDataLoaderIter</span>(<span class="params">_BaseDataLoaderIter</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, loader</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(_MultiProcessingDataLoaderIter, self).__init__(loader)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> self._num_workers &gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> self._prefetch_factor &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> loader.multiprocessing_context <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        multiprocessing_context = multiprocessing</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        multiprocessing_context = loader.multiprocessing_context</span><br><span class="line"></span><br><span class="line">    self._worker_init_fn = loader.worker_init_fn</span><br><span class="line">    <span class="comment"># No certainty which module multiprocessing_context is</span></span><br><span class="line">    self._worker_result_queue = multiprocessing_context.Queue()  <span class="comment"># type: ignore[var-annotated]</span></span><br><span class="line">    self._worker_pids_set = <span class="literal">False</span></span><br><span class="line">    self._shutdown = <span class="literal">False</span></span><br><span class="line">    self._workers_done_event = multiprocessing_context.Event()</span><br><span class="line"></span><br><span class="line">    self._index_queues = []</span><br><span class="line">    self._workers = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self._num_workers):</span><br><span class="line">        <span class="comment"># No certainty which module multiprocessing_context is</span></span><br><span class="line">        index_queue = multiprocessing_context.Queue()  <span class="comment"># type: ignore[var-annotated]</span></span><br><span class="line">        <span class="comment"># Need to `cancel_join_thread` here!</span></span><br><span class="line">        <span class="comment"># See sections (2) and (3b) above.</span></span><br><span class="line">        index_queue.cancel_join_thread()</span><br><span class="line">        w = multiprocessing_context.Process(</span><br><span class="line">            target=_utils.worker._worker_loop,</span><br><span class="line">            args=(self._dataset_kind, self._dataset, index_queue,</span><br><span class="line">                  self._worker_result_queue, self._workers_done_event,</span><br><span class="line">                  self._auto_collation, self._collate_fn, self._drop_last,</span><br><span class="line">                  self._base_seed, self._worker_init_fn, i, self._num_workers,</span><br><span class="line">                  self._persistent_workers, self._shared_seed))</span><br><span class="line">        w.daemon = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># NB: Process.start() actually take some time as it needs to</span></span><br><span class="line">        <span class="comment">#     start a process and pass the arguments over via a pipe.</span></span><br><span class="line">        <span class="comment">#     Therefore, we only add a worker to self._workers list after</span></span><br><span class="line">        <span class="comment">#     it started, so that we do not call .join() if program dies</span></span><br><span class="line">        <span class="comment">#     before it starts, and __del__ tries to join but will get:</span></span><br><span class="line">        <span class="comment">#     AssertionError: can only join a started process.</span></span><br><span class="line">        w.start()</span><br><span class="line">        self._index_queues.append(index_queue)</span><br><span class="line">        self._workers.append(w)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self._pin_memory:</span><br><span class="line">        self._pin_memory_thread_done_event = threading.Event()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Queue is not type-annotated</span></span><br><span class="line">        self._data_queue = queue.Queue()  <span class="comment"># type: ignore[var-annotated]</span></span><br><span class="line">        pin_memory_thread = threading.Thread(</span><br><span class="line">            target=_utils.pin_memory._pin_memory_loop,</span><br><span class="line">            args=(self._worker_result_queue, self._data_queue,</span><br><span class="line">                  torch.cuda.current_device(),</span><br><span class="line">                  self._pin_memory_thread_done_event, self._pin_memory_device))</span><br><span class="line">        pin_memory_thread.daemon = <span class="literal">True</span></span><br><span class="line">        pin_memory_thread.start()</span><br><span class="line">        <span class="comment"># Similar to workers (see comment above), we only register</span></span><br><span class="line">        <span class="comment"># pin_memory_thread once it is started.</span></span><br><span class="line">        self._pin_memory_thread = pin_memory_thread</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self._data_queue = self._worker_result_queue</span><br><span class="line"></span><br><span class="line">    <span class="comment"># In some rare cases, persistent workers (daemonic processes)</span></span><br><span class="line">    <span class="comment"># would be terminated before `__del__` of iterator is invoked</span></span><br><span class="line">    <span class="comment"># when main process exits</span></span><br><span class="line">    <span class="comment"># It would cause failure when pin_memory_thread tries to read</span></span><br><span class="line">    <span class="comment"># corrupted data from worker_result_queue</span></span><br><span class="line">    <span class="comment"># atexit is used to shutdown thread and child processes in the</span></span><br><span class="line">    <span class="comment"># right sequence before main process exits</span></span><br><span class="line">    <span class="keyword">if</span> self._persistent_workers <span class="keyword">and</span> self._pin_memory:</span><br><span class="line">        <span class="keyword">import</span> atexit</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self._workers:</span><br><span class="line">            atexit.register(_MultiProcessingDataLoaderIter._clean_up_worker, w)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># .pid can be None only before process is spawned (not the case, so ignore)</span></span><br><span class="line">    _utils.signal_handling._set_worker_pids(<span class="built_in">id</span>(self), <span class="built_in">tuple</span>(w.pid <span class="keyword">for</span> w <span class="keyword">in</span> self._workers))  <span class="comment"># type: ignore[misc]</span></span><br><span class="line">    _utils.signal_handling._set_SIGCHLD_handler()</span><br><span class="line">    self._worker_pids_set = <span class="literal">True</span></span><br><span class="line">    self._reset(loader, first_iter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在上面的构造函数中，最值得关注的内容是多条不同用途的用于进程间通信的 <code>Queue</code> 的创建:

  <ul>
    <li>Line 15 创建了 <code>_worker_result_queue</code>，该队列作为各个 Worker 进程的输出队列，用于存储各个 Worker 进程从 <b>Dataset</b> 中读取数据后经由 <code>collate_fn</code> 处理后的结果;</li>
    <li>Line 24 为每一个 Worker 进程创建了 <code>index_queue</code>，该队列作为各个 Worker 进程分别的输入队列，存储着主进程的 <b>Sampler</b> 为各个 Worker 进程生成的 Sampled Indices;</li>
    <li>Line 50 创建了 <code>_data_queue</code>，如果 <code>dataloader</code> 的 <code>pin_memory</code> 被使能，则该队列会被创建，用于作为主进程的 <code>pin_memory_thread</code> Thread 的输出队列，该 Thread 用于将各个 Worker 进程的输出 (i.e. 存储在 <code>_worker_result_queue</code> 中) 转移至 Pinned Memory;</li>
  </ul>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在 Line 22~44 中，我们可以看到 <code>_MultiProcessingDataLoaderIter</code> 在构造函数中就创建了 <code>num_workers</code> 个 Worker 进程，这些 Worker 进程运行的函数是 <code>_utils.worker._worker_loop</code>，首先来看该函数的接口定义:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_worker_loop</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">  dataset_kind,       <span class="comment"># Dataset 类型</span></span></span></span><br><span class="line"><span class="function"><span class="params">  dataset,            <span class="comment"># Dataset</span></span></span></span><br><span class="line"><span class="function"><span class="params">  index_queue,        <span class="comment"># 输入队列，存放的是主进程 Sampler 为当前 Worker 进程生成的 Sampled Indices</span></span></span></span><br><span class="line"><span class="function"><span class="params">  data_queue,         <span class="comment">#</span></span></span></span><br><span class="line"><span class="function"><span class="params">  done_event,         <span class="comment">#</span></span></span></span><br><span class="line"><span class="function"><span class="params">  auto_collation,     <span class="comment">#</span></span></span></span><br><span class="line"><span class="function"><span class="params">  collate_fn,         <span class="comment"># 整理函数</span></span></span></span><br><span class="line"><span class="function"><span class="params">  drop_last,          <span class="comment"># 指示是否丢弃最后一个不完整的 Mini-batch</span></span></span></span><br><span class="line"><span class="function"><span class="params">  base_seed,          <span class="comment">#</span></span></span></span><br><span class="line"><span class="function"><span class="params">  init_fn,            <span class="comment">#</span></span></span></span><br><span class="line"><span class="function"><span class="params">  worker_id,          <span class="comment"># </span></span></span></span><br><span class="line"><span class="function"><span class="params">  num_workers,        <span class="comment"># 主进程创建的 Worker 进程的数量</span></span></span></span><br><span class="line"><span class="function"><span class="params">  persistent_workers, <span class="comment">#</span></span></span></span><br><span class="line"><span class="function"><span class="params">  shared_see          <span class="comment">#</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span>):</span></span><br><span class="line">    <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
  <h3 class="title">单进程数据加载</h3>
  <div class="img" title="单进程数据加载调用链" label="img_single_process">
    <img src="./pic/single_process.png" width="90%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在我们来看用于单进程数据加载的基本流程，其基本调用关系如 <imgref>img_single_process</imgref> 所示。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;当我们在主程序的 <code>for</code> 循环中对 <code>DataLoader</code> 进行迭代时，其会 ① 首先调用 <code>DataLoader</code> 的 <code>__iter__(self)</code> 魔法方法以获得 Iterator，从上面展示过的程序中我们可以知道，在单进程数据加载的设定下，代表 Iterator 的类是  <code>_SingleProcessDataLoaderIter</code>；然后 ② 调用 <code>_SingleProcessDataLoaderIter</code> 的 <code>__next__</code> 以获取当前 Iteration 使用的 Sample Batch，从上一小节的分析我们知道:

  <ol>
    <li><code>_SingleProcessDataLoaderIter</code> 的 <code>__next__</code> 方法继承自父类 <code>_BaseDataLoaderIter</code>;</li>
    <li>父类 <code>_BaseDataLoaderIter</code> 的 <code>__next__</code> 方法实际上调用了 <code>_next_data</code> 方法来实现迭代逻辑，而后者留给子类实现;</li>
    <li>
      <code>_SingleProcessDataLoaderIter</code> 的 <code>_next_data</code> 方法过程可以分为三步:
      <div class="background">
      <ol>
        <li>调用 <code>_SingleProcessDataLoaderIter</code> 定义的 <code>next_index()</code> 从 <b>BatchSampler</b> (<b>Sampler</b>) 实体中获取 Sampled Indices (Index);</li>
        <li>基于 Sampled Indices (Index)，调用 <b>Fetcher</b> 实体从 <b>Dataset</b> 中获取并处理对应 Samples 的数据；</li>
        <li>将处理后的 Sample(s) 数据转移至 Pinned Memory (如果 <code>pin_memory==True</code>);</li>
      </ol>
      </div>
    </li>
  </ol>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;下面我们就 <code>_SingleProcessDataLoaderIter</code> 类定义的 <code>_next_data</code> 方法所实现的三步逻辑进行分析:

  <h4 class="paragraph">获取 Indices</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;上面看到的 <code>next_index()</code> 方法是在 <code>_SingleProcessDataLoaderIter</code> 的父类 <code>_BaseDataLoaderIter</code> 中定义的，我们把相关代码整理如下:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_BaseDataLoaderIter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, loader: DataLoader</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        self._index_sampler = loader._index_sampler</span><br><span class="line">        self._sampler_iter = <span class="built_in">iter</span>(self._index_sampler)</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_next_index</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">next</span>(self._sampler_iter)  <span class="comment"># may raise StopIteration</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params">Generic[T_co]</span>):</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_auto_collation</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.batch_sampler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_index_sampler</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self._auto_collation:</span><br><span class="line">            <span class="keyword">return</span> self.batch_sampler</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.sampler</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;从上面的代码可以看出，根据 <code>DataLoader</code> 中是否启用了了 <code>batch_sampler</code>，<code>next_index()</code> 将对应地从 <code>DataLoader</code> 的 <code>batch_sampler</code> 或者 <code>sampler</code> 中迭代出 Indices。

  <h4 class="paragraph">Samples 加载</h3>

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DatasetKind</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  Map = <span class="number">0</span></span><br><span class="line">  Iterable = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @staticmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">create_fetcher</span>(<span class="params">kind, dataset, auto_collation, collate_fn, drop_last</span>):</span></span><br><span class="line">      <span class="keyword">if</span> kind == _DatasetKind.Map:</span><br><span class="line">          <span class="keyword">return</span> _utils.fetch._MapDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">return</span> _utils.fetch._IterableDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在来看 Samples 加载的部分，也即 <b>Fetcher</b> 实体。在 <code>_SingleProcessDataLoaderIter</code> 的构造函数中可以看到其调用了 <code>_DatasetKind.create_fetcher</code> 创建了 <b>Fetcher</b> 实体，相关代码如上所示。根据数据集类型的不同，该函数会创建出 <code>_MapDatasetFetcher</code> 类型或者 <code>_IterableDatasetFetcher</code> 类型的 <b>Fetcher</b> 实体，分别对应 Map-style 或者 Iterable-style 的数据集。

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_MapDatasetFetcher</span>(<span class="params">_BaseDatasetFetcher</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset, auto_collation, collate_fn, drop_last</span>):</span></span><br><span class="line">      <span class="built_in">super</span>(_MapDatasetFetcher, self).__init__(dataset, auto_collation, collate_fn, drop_last)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fetch</span>(<span class="params">self, possibly_batched_index</span>):</span></span><br><span class="line">      <span class="keyword">if</span> self.auto_collation:</span><br><span class="line">          data = [self.dataset[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> possibly_batched_index]</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          data = self.dataset[possibly_batched_index]</span><br><span class="line">      <span class="keyword">return</span> self.collate_fn(data)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 <code>_MapDatasetFetcher</code> 类型 <b>Fetcher</b> 来说，如上所示，其定义的 <code>fetch()</code> 函数直接输入 Indices，作为 Map 的 Key，获得对应的样本，然后将获取的样本交给 <code>collate_fn</code> 指定的 Collate Function 进行输出前的整理。

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_IterableDatasetFetcher</span>(<span class="params">_BaseDatasetFetcher</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset, auto_collation, collate_fn, drop_last</span>):</span></span><br><span class="line">      <span class="built_in">super</span>(_IterableDatasetFetcher, self).__init__(dataset, auto_collation, collate_fn, drop_last)</span><br><span class="line">      self.dataset_iter = <span class="built_in">iter</span>(dataset)</span><br><span class="line">      self.ended = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fetch</span>(<span class="params">self, possibly_batched_index</span>):</span></span><br><span class="line">      <span class="keyword">if</span> self.ended:</span><br><span class="line">          <span class="keyword">raise</span> StopIteration</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> self.auto_collation:</span><br><span class="line">          data = []</span><br><span class="line">          <span class="keyword">for</span> _ <span class="keyword">in</span> possibly_batched_index:</span><br><span class="line">              <span class="keyword">try</span>:</span><br><span class="line">                  data.append(<span class="built_in">next</span>(self.dataset_iter))</span><br><span class="line">              <span class="keyword">except</span> StopIteration:</span><br><span class="line">                  self.ended = <span class="literal">True</span></span><br><span class="line">                  <span class="keyword">break</span></span><br><span class="line">          <span class="keyword">if</span> <span class="built_in">len</span>(data) == <span class="number">0</span> <span class="keyword">or</span> (self.drop_last <span class="keyword">and</span> <span class="built_in">len</span>(data) &lt; <span class="built_in">len</span>(possibly_batched_index)):</span><br><span class="line">              <span class="keyword">raise</span> StopIteration</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          data = <span class="built_in">next</span>(self.dataset_iter)</span><br><span class="line">      <span class="keyword">return</span> self.collate_fn(data)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 <code>_IterableDatasetFetcher</code> 类型 <b>Fetcher</b> 来说，如上所示，其在构造函数内设置了对应 Dataset 初始的迭代器，在 <code>fetch()</code> 方法内利用该迭代器获取元素，输入 <code>fetch()</code> 的 Indices 其实已经没有多大作用了。

  <h4 class="paragraph">转移至 Pinned Memory</h3>

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_SingleProcessDataLoaderIter</span>(<span class="params">_BaseDataLoaderIter</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, loader</span>):</span></span><br><span class="line">      <span class="built_in">super</span>(_SingleProcessDataLoaderIter, self).__init__(loader)</span><br><span class="line">      <span class="keyword">assert</span> self._timeout == <span class="number">0</span></span><br><span class="line">      <span class="keyword">assert</span> self._num_workers == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">      self._dataset_fetcher = _DatasetKind.create_fetcher(</span><br><span class="line">          self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_next_data</span>(<span class="params">self</span>):</span></span><br><span class="line">      index = self._next_index()  <span class="comment"># may raise StopIteration</span></span><br><span class="line">      data = self._dataset_fetcher.fetch(index)  <span class="comment"># may raise StopIteration</span></span><br><span class="line">      <span class="keyword">if</span> self._pin_memory:</span><br><span class="line">          data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)</span><br><span class="line">      <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;从上面 <code>_SingleProcessDataLoaderIter</code> 的 <code>_next_data</code> 方法中可以看见，在 <b>Fetcher</b> 实体完成 Sample(s) 的加载和处理后，最后一步就是根据用户是否指定将 Tensor 数据转移至 Pinned Memory，调用 PyTorch 官方提供的 <code>_utils.pin_memory.pin_memory</code> 方法进行 Pinned Memory 的转移，这个函数我们在 <ref>pinned_memory</ref> 中进行了说明。

  <h3 class="title">多进程数据加载</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;TODO
</div>


<div class="div_ref" id="ref_container"></div>

</body>

<!-- 圆圈数字 -->
<!--
⓪ ① ② ③ ④ ⑤ ⑥ ⑦ ⑧ ⑨ ⑩ ⑪ ⑫ ⑬ ⑭ ⑮ ⑯ ⑰ ⑱ ⑲ ⑳ ㉑ ㉒ ㉓ ㉔ ㉕ ㉖ ㉗ ㉘ ㉙ ㉚ ㉛ ㉜ ㉝ ㉞ ㉟ ㊱ ㊲ ㊳ ㊴ ㊵ ㊶ ㊷ ㊸ ㊹ ㊺ ㊻ ㊼ ㊽ ㊾ ㊿
-->

<!-- Flow Chart -->
<!--
Format see: https://mermaid-js.github.io/mermaid/#/flowchart
-->
<!-- <flowchart class="mermaid">
 Mermaid Flow Chart Code
</flowchart> -->

<!-- Sign Block -->
<!--
<noteblock>
A NOTE
</noteblock>

<queblock>
A QUESTION
</queblock>
-->

<!--图片、引用-->
<!-- 
<div class="img" title="img title" label="img_label" source="url">
  <img src="" height="" />
</div>

<imaging>img_label</imaging>
-->

<!--等式、引用-->
<!-- 
<div class="equation" label="equation_label">
</div>

<equation>equation_label</equation>
-->

<!--定理、引用、证明-->
<!-- 
<div class="theorm" label="theorm_label">
</div>

<theorm>theorm_label</theorm>

<div class="theorm_prove">
</div>
-->

<!--引用其它章节-->
<!-- 
<ref></ref> 
-->

<!--引用文献-->
<!-- 
<cite></cite> 
-->

<!--关键词-->
<!-- 
<def></def> 
-->

<!--醒目注意-->
<!-- 
<note></note> 
-->

<!--段落-->
<!--
<h3 class="paragraph">Paragraph Name</h3>
-->

<!--表格-->
<!--
<div class="table" title="Table Title" label="table_label">
  <table border="1" align="center" bgcolor="#FFFFFF">
    <tr>
      <th>A</th>
      <th>B</th>
      <th>C</th>
    </tr>
    <tr>
      <td>xxx</td>
      <td>xxx</td>
      <td>xxx</td>
    </tr>
  </table>
</div>
-->

<!--矩阵公式-->
<!--
<div class="cmath" align="center">
  `((1, 0),(1, 0))`
</div><br>
-->

<!--伪代码-->
<!--
<pre id="quicksort" style="display:hidden;">
  % This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
  \begin{algorithm}
  \caption{Quicksort}
  \begin{algorithmic}
  \PROCEDURE{Quicksort}{$A, p, r$}
      % Add Here

      % 空行
      % \STATE \texttt{\\}
  \ENDPROCEDURE
  \end{algorithmic}
  \end{algorithm}
</pre>
<script>
    pseudocode.renderElement(document.getElementById("quicksort"));
</script>
-->
<!--
Latex 伪代码格式见: https://github.com/SaswatPadhi/pseudocode.js
-->

<!--图片-->
<!--
<div align="center">
  <img src="./pic/xxx.png" width=80%>
</div>
-->

<!--正文-->
<!--
<p>
&nbsp;&nbsp;&nbsp;&nbsp;公式：<span>`\overline{A}\overline{B}`</span>
</p>
-->
      </div>
      
      
      
    </div>
    
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Tech_System_And_Network/">TECH_SYSTEM_AND_NETWORK</a></li>
          <li>PYTORCH_DATALOADER</li>
        
  </ul>

    
    
    


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhuobin Huang"
      src="/images/avatar_2.png">
  <p class="site-author-name" itemprop="name">Zhuobin Huang</p>
  <div class="site-description" itemprop="description">System Engineer</div>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhuobin Huang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
