<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Exo 2:300,300italic,400,400italic,700,700italic|Caveat:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zobinhuang.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":180,"display":"post","padding":10,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:type" content="website">
<meta property="og:title" content="线性回归和梯度下降">
<meta property="og:url" content="https://zobinhuang.github.io/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/index.html">
<meta property="og:site_name" content="Zobin">
<meta property="og:description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/sl.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/gd.gif">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/gradient.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/gradient_2.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/lr_exp.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/lwg.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/xxx.png">
<meta property="article:published_time" content="2022-09-15T08:16:11.338Z">
<meta property="article:modified_time" content="2022-09-15T08:16:11.338Z">
<meta property="article:author" content="Zhuobin Huang">
<meta property="article:tag" content="Zobin">
<meta property="article:tag" content="黄卓彬">
<meta property="article:tag" content="zobinHuang">
<meta property="article:tag" content="网络工程">
<meta property="article:tag" content="Networking Engineering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png">

<link rel="canonical" href="https://zobinhuang.github.io/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>线性回归和梯度下降 | Zobin
</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Zobin" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zobin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Lovin' Tech with Tea</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about-me-(关于我)">

    <a href="/sec_about/" rel="section"><i class="fa fa-user fa-fw"></i>About Me (关于我)</a>

  </li>
        <li class="menu-item menu-item-library-(知识库)">

    <a href="/sec_learning" rel="section"><i class="fa fa-duotone fa-book fa-fw"></i>Library (知识库)</a>

  </li>
        <li class="menu-item menu-item-music-(独立音乐人)">

    <a href="/sec_music" rel="section"><i class="fa fa-music fa-fw"></i>Music (独立音乐人)</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="en">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">线性回归和梯度下降
</h1>

<div class="post-meta">
  
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Algorithm/">ALGORITHM</a></li>
          <li>ML_2_LINEAR_REGRESSION_AND_GRADIENT_DESCENT</li>
        
  </ul>

</div>

</header>

      
      
      
      <div class="post-body">
          <head>
<!--导入样式表-->
<link rel="stylesheet" type="text/css" href="style/index.css">

<!--导入网页脚本-->
<script src="script/index.js"></script>

<!--支持伪代码显示-->
<script>
    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3.0.0/es5/tex-chtml.js"
        integrity="sha256-3Fdoa5wQb+JYfEmTpQHx9sc/GuwpfC/0R9EpBki+mf8=" crossorigin>
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js">
</script>

<!--支持网页公式显示-->    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>

<!--支持矩阵显示-->
<script type="text/javascript">
  run_maths = function() {
    if (document.querySelector('[class*="cmath"]') !== null) {
      if (typeof (mjax_path)=='undefined') { mjax_path='https://cdn.jsdelivr.net/npm/mathjax@2'; }
      if (typeof (mjax_config)=='undefined') { mjax_config='AM_CHTML'; }
      smjax = document.createElement ('script');
      smjax.setAttribute('src',`${mjax_path}/MathJax.js?config=${mjax_config}`);
      smjax.setAttribute('async',true);
      document.getElementsByTagName('head')[0].appendChild(smjax);
    }
  };
  if (document.readyState === 'loading') {  
    window.addEventListener('DOMContentLoaded', run_maths); 
  } else { 
    run_maths(); 
  }
</script>
</head>

<body onload="load_page()">

<div align="center" class="div_indicate_source">
  <h4>⚠ 转载请注明出处：<font color="red"><i>作者：ZobinHuang，更新日期：Sept.15 2022</i></font></h4>
</div>
<div class="div_licence">
  <br>
  <div align="center">
      <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="知识共享许可协议" style="border-width:0; margin-left: 20px; margin-right: 20px;" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a>
  </div>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;本<span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" rel="dct:type">作品</span>由 <span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName"><b>ZobinHuang</b></span> 采用 <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><font color="red">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</font></a> 进行许可，在进行使用或分享前请查看权限要求。若发现侵权行为，会采取法律手段维护作者正当合法权益，谢谢配合。
  </p>
</div>
<br>
<div class="div_catalogue">
  <div align="center">
    <h1> 目录 </h1>
    <p>
    <font size="3px">有特定需要的内容直接跳转到相关章节查看即可。</font>
  </div>
  <div class="div_load_catalogue_alert" id="load_catalogue_alert">正在加载目录...</div>
  <div class="div_catalogue_container" id="catalogue_container">
  </div>
</div><br>

<!-- Start your post here -->
<h2 class="title">ML 范式: Supervised Learning</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;本文要重点介绍的 <def>线性回归 (Linear Regression)</def> 是属于 Supervised Learning 的一种学习算法，而学习算法只是整个 Machine Learning 流程中的一环。本节首先我们回顾一下 Supervised Learning 的基本流程:

  <div class="img" title="监督学习" label="supervised_learning">
    <img src="./pic/sl.png" width=80%>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;如 <imgref>supervised_learning</imgref> 所示，首先我们来看 <def>训练 (Training)</def> 过程。我们首先给出一个 <def>训练集 (Training Set)</def>，里面包含了 $m$ 条 <def>样本 (Samples)</def> $(X,Y)=[(X^{(0)},Y^{(0)}),...,(X^{(m-1)},Y^{(m-1)})]$，每一条 Sample $(X^{(i)},Y^{(i)})$  中都有对应的 <def>特征 (Features)</def> $X^{(i)}$ 以及其对应的 <def>标签 (Label)</def> $Y^{(i)}$。注意到 $X^{(i)}$ 是 $n$ 维的，即 

  <div class="equation">
  $X^{(i)}=[x_{0}^{(i)},x_{1}^{(i)},...,x_{n-1}^{(i)}]$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;$y^{(i)}$ 是 $l$ 维的，即

  <div class="equation">
  $Y^{(i)}=[y_{0}^{(i)},y_{1}^{(i)},...,y_{l-1}^{(i)}]$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;这些 Samples 将作为 <def>学习算法 (Learning Algorithm)</def> 的输入。后者首先定义了一个带参数 $\Theta$ 的 <def>假设 (Hypothesis)</def> $h(X_{\text{h}};\Theta)$，Hypothesis 的本质是一个计算过程，用于根据输入的特征 $X_{\text{h}}$ 输出预测值。由于 Hypothesis 携带参数，并且做 Supervised Learning 的 Training 的目标就是基于 Training Set 学习这些参数，因此我们还需要为 Hypothesis 定义一个 <def>损失函数 (Loss Function)</def>，Loss Function 的目标是将 Hypothesis 输出的预测值和 Training Set 中给出的 Label 进行比较，并且量化地给出两者的差异，以用于后续参数优化，我们把这种差异称为 <def>损失值 (Loss)</def>。Loss Function 和 Hypothesis 就组成了我们常说的机器学习 <def>模型 (Model)</def>。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;有了 Model 和 Training Set，Supervised Learning  的 Training 过程就是把 Training Set 中的 Samples 输入到 Model 中，首先计算出 Loss，接着我们就需要一个 <def>参数求解器 (Parameter Solver)</def>，后者基于最优化算法的思想，基于 Loss 计算出 Hypothesis 中各个参数的更新值。在完成参数的更新后，我们就得到了一个 <def>训练好的模型 (Trained Model)</def>，此时 Traning 流程就结束了。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在 <def>推理 (Inference)</def> 流程中，Trained Model 就可以用于对新的输入特征 $X^{(n ew)}$ 输出一个预测值 $Y^{(n ew)}$。
</div>

<h2 class="title">学习算法: 线性回归 (Linear Regression)</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在完成对 Supervised Learning 流程的理解后，本节我们将进入我们第一个 Learining Algorithm —— <def>线性回归 (Linear Regression)</def> 的学习。

  <h3 class="title">Hypothesis 的形式</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于上述流程，我们首先关心的问题是 Linear Regression 的 Hypothesis $h(X_{\text{h}};\Theta)$ 的形式。Linear Regression 的 Hypothesis 定义如下:

  <div class="equation">
    $\hat{Y}^{(i)} = h(X^{(i)};\Theta) = \theta_{b} + \sum_{j=0}^{n-1}\theta_jx_j^{(i)}$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;其中 $n$ 表示了输入向量 $X$ 的维度，对应的也是参数 $\Theta$ 的维度; $\theta_{b}$ 是一个偏置值，我们可以给 $\Theta$ 和 $X$ 多加一个维度来合并这个偏置值，这样一来 $x_n$ 就恒等为 $1$。化简如下：

  <div class="equation" label="equ_lr_model">
    $\hat{Y}^{(i)} = h(X^{(i)};\Theta) = \sum_{j=0}^{n}\theta_jx_j^{(i)}$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;再次强调，$X$, $Y$ 和 $\Theta$ 都是向量，可以表示如下:

  <div class="equation" label="equ_x_y_theta">
  <div class="cmath" align="center">
    `\Theta = ((\theta_0),(\theta_1),(...),(\theta_{n}))`，`X = ((x_0),(x_1),(...),(x_{n}))`，`Y = ((y_0),(y_1),(...),(y_{l-1}))` 
  </div>
  </div>

  <h3 class="title">Loss Function 的形式</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在有了 $h(X_{\text{h}};\Theta)$ 的形式之后，我们接下来关心的问题就是 Loss Function 该如何定义。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;直觉告诉我们，$h(X_{\text{h}};\Theta)$ 所应用的参数 $\Theta$，应该使得 $\forall X^{(i)}\in X$，$h(X^{(i)};\Theta) \approx Y^{(i)}$，也即 $h(X_{\text{h}};\Theta)$ 输出的结果应该尽可能得和 Training Set 中各个 Sample 的 Label 接近。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于这个直觉，我们定义一个简单的 Loss Function:

  <div class="equation">
  $\mathcal{J}(\Theta) = \sum_{i=0}^{m-1}(h(X^{(i)};\Theta)-Y^{(i)})^2$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;为了后面计算方便，我们在前面加上系数:

  <div class="equation" label="equ_loss_define">
  $\mathcal{J}(\Theta) = \frac{1}{2}\sum_{i=0}^{m-1}(h(X^{(i)};\Theta)-Y^{(i)})^2$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;有了 Loss Function 之后，接下来 Parameter Solver 的工作就是：找到使得 $\mathcal{J}(\Theta)$ 最小的 $\Theta$ 值，也即:

  <div class="equation" label="equ_goal">
  $\Theta^{\text{*}}=$ argmin$[\mathcal{J}(\Theta)]$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;稍后我们在 <ref>gd</ref> 和 <ref>ne</ref> 中将 Parameter Solver 是如何实现上面这个目标的。

  <h3 class="title">从最大似然的角度来解释 Loss Function 的定义</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在上文中，我们定义了 <equation>equ_loss_define</equation> 的基于平方差的 Loss Function，在本节中我们从概率的角度将说明为什么将 Lost Function 定义为平方差是合理的，并且我们将从最大似然的角度出发，分析我们上文定义 Linear Regression 这个 Loss Function 的过程的本质 (内在思路)。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先，我们先把 Hypothesis 的输入特征 $X^{(i)}_{\text{h}}$ 和这份输入特征对应的标签 $Y^{(i)}_{\text{h}}$ 视为随机变量，这点很重要，有了随机变量的假设，我们就可以对这两者进行概率分布的假设。为什么要进行概率分布的假设呢？因为我们本节将从最大似然的角度来解释 Loss Function 的定义，Loss Function 基于 Hypothesis 而定义，因此我们需要首先对 Hypothesis 的概率特性进行表示。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在让我们开始，首先我们可以认为 $X^{(i)}_{\text{h}}$ 和 $Y^{(i)}_{\text{h}}$ 存在如下关系：

  <div class="equation">
  $Y^{(i)}_{\text{h}}=\Theta^TX^{(i)}_{\text{h}}+\epsilon^{(i)}$
  </div> 

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;其中 $\epsilon^{(i)}$ 是误差项，是一个随机变量，描述的是对于一份输入特征 $X^{(i)}_{\text{h}}$ 的 Hypothesis 输出结果 $\Theta^TX^{(i)}_{\text{h}}$ 与对应的标签 $Y^{(i)}_{\text{h}}$ 之间的误差，其本质是无法被模型描述的特征与标签之间的关系 (Unmodel Effects)。我们进一步假设 $\epsilon^{(i)}$ 是 <def>Independently and Identically Distributed</def> (i.e. IID, 独立同分布) 的，并且我们假设并且服从的是均值为 $0$，方差为 $\sigma^2$ 的正态分布，$\epsilon^{(i)} \sim N(0,\sigma^2)$，也即:

  <div class="equation">
  $p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma} \cdot \exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;这样一来，我们可以得到，在基于给定参数 $\Theta$ 和已知输入特征 $X^{(i)}_{\text{h}}$ 的条件下，输入特征 $X^{(i)}_{\text{h}}$ 对应的标签 $Y^{(i)}$ 是一个服从正态分布的随机变量，其以 $\sigma^2$ 为方差在 $\Theta^TX^{(i)}_{\text{h}}$ 附近波动，也即满足如下关系:

  <div class="equation", label="eq_y_x_distribution">
  $p(Y_{\text{h}}^{(i)}|X_{\text{h}}^{(i)};\Theta) = \frac{1}{\sqrt{2\pi}\sigma} \cdot \exp(-\frac{(Y_{\text{h}}^{(i)}-\Theta^TX_{\text{h}}^{(i)})^2}{2\sigma^2})$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;写作概率表达式，即:

  <div class="equation">
  $(Y_{\text{h}}^{(i)}|X_{\text{h}}^{(i)};\Theta) \sim N(\Theta^TX_{\text{h}}^{(i)},\sigma^2)$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;$(Y_{\text{h}}^{(i)}|X_{\text{h}}^{(i)};\Theta)$ 读作 “基于参数 $\Theta$，在给定随机变量 $X_{\text{h}}^{(i)}$ (i.e. 特征) 的条件下，随机变量 $Y_{\text{h}}^{(i)}$ (i.e. 标签) 的分布”。值得注意的是，这里我们使用的是分号 '$;$' 来分隔参数项 $\Theta$，而不是使用逗号 '$,$' 写作 $p(Y^{(i)}|X^{(i)},\Theta)$，因为后者会造成参数 $\Theta$ 是随机变量的歧义。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;好了，现在让我们重新思考我们做 Training 的初衷: 给定一个 Training Set $(X,Y)$，我们的目的是 <def>估计</def> 一组参数 $\Theta$，使得模型基于 Training Set 中各条 Sample 的 Features 的预测结果 $\Theta^TX^{(i)}$ 更加接近 Training Set 提供的 Label，也即 $Y^{(i)}$。也就是说， $\Theta$ 是一个我们希望调整的量，而来自数据集的 $X^{(i)}$ 和 $Y^{(i)}$ 对于我们来说是定量。实际上，$X^{(i)}$ 和 $Y^{(i)}$ 可以视为针对随机变量 $X_{\text{h}}^{(i)}$ 和 $Y_{\text{h}}^{(i)}$ 所做的实验结果，而随机变量 $X_{\text{h}}^{(i)}$ 和 $Y_{\text{h}}^{(i)}$ 的分布我们是已经知晓是 <equation>eq_y_x_distribution</equation> 了。按照 <def>最大似然估计 (Maximum Likelyhood Estimation)</def> 的思路，对参数 $\Theta$ 的估计，就是找出一种参数取值 $\Theta^{\text{*}}$，使得基于 <equation>eq_y_x_distribution</equation> 的随机变量 $X_{\text{h}}^{(i)}$ 和 $Y_{\text{h}}^{(i)}$ 的分布，出现实验结果 $X^{(i)}$ 和 $Y^{(i)}$ 的可能性最大。如果考虑多个 Samples，那么对参数 $\Theta$ 的估计，就是找出一种参数取值 $\Theta^{\text{*}}$，使得基于 <equation>eq_y_x_distribution</equation> 的随机变量 $X_{\text{h}}$ 和 $Y_{\text{h}}$ 的分布，出现实验结果 $X$ 和 $Y$ 的可能性最大，也即:

  <div class="equation" label="eq_parameter">
  $\Theta^{\text{*}} = \text{argmax}[p(Y_{\text{h}}=Y|X_{\text{h}}=X;\Theta)]$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;此时，我们的讨论对象已经从 <equation>eq_y_x_distribution</equation> 的随机变量 $X_{\text{h}}^{(i)}$ 和 $Y_{\text{h}}^{(i)}$，变为了 <equation>eq_parameter</equation> 的参数 $\Theta$，此时在各次实验结果中的随机变量 $X_{\text{h}}^{(i)}$ 和 $Y_{\text{h}}^{(i)}$ 已经取为定值，而 $\Theta$ 是一个变量值。在讨论随机变量时，我们使用的是概率分布的概念；而在讨论参数变量时，在数学上的相关概念是 <def>Likelyhood (似然函数)</def>，在我们的问题中，我们可以把参数 $\Theta$ 的似然函数 $\mathcal{L}(\Theta)$ 定义如下:

  <div class="equation" label="likelyhood">
  $\mathcal{L}(\Theta) = p(Y_{\text{h}}=Y|X_{\text{h}}=X;\Theta)$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<equation>eq_parameter</equation> 所代表的优化问题，使用似然函数进行解释，就是找到一种参数取值 $\Theta^{\text{*}}$，使得似然函数 $\mathcal{L}(\Theta)$ 取值最大，也即:

  <div class="equation" label="eq_likelyhood_opt">
  $\Theta^{\text{*}} = \text{argmax}[\mathcal{L}(\Theta)]$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;为了求解 <equation>eq_likelyhood_opt</equation>，我们需要得到 $\mathcal{L}(\Theta)$ 的表达式，我们把 <equation>likelyhood</equation> 进一步展开可得:

  <div class="equation" label="likelyhood_expend">
  \begin{aligned}
  \mathcal{L}(\Theta) &= p(Y_{\text{h}}=Y|X_{\text{h}}=X;\Theta) \\ \\
  & (由于我们对 \epsilon 有 \text{i.i.d} 的假设，因此原式可以展开为如下形式) \\ \\
  &= \prod_{i=0}^{m-1} p(Y^{(i)}_{\text{h}}=Y^{(i)}|X^{(i)}_{\text{h}}=X^{(i)};\Theta) \\ \\
  &= \prod_{i=0}^{m-1} \frac{1}{\sqrt{2\pi}\sigma} \cdot \exp(-\frac{(Y^{(i)}-\Theta^TX^{(i)})^2}{2\sigma^2})
  \end{aligned}
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在，基于 <equation>likelyhood_expend</equation>，我们的目标是完成对 <equation>eq_likelyhood_opt</equation> 的求解。我们能想到的最直接的办法就是用 <equation>likelyhood_expend</equation> 对参数 $\Theta$ 求导，然而由于 <equation>likelyhood_expend</equation> 中出现了累乘，求导并不是特别方便，所以我们使用对数将其转化为累加的形式，也即 <def>对数似然值 (Log Likelyhood)</def>，用符号表示为 $\mathcal{l}(\Theta)$，具体如下所示:

  <div class="equation" label="log_likelyhood">
  \begin{aligned}
  \mathcal{l}(\Theta) &= \log \mathcal{L}(\Theta) \\
  &= \log \prod_{i=0}^{m-1} \frac{1}{\sqrt{2\pi}\sigma} \cdot \exp(-\frac{(Y^{(i)}-\Theta^TX^{(i)})^2}{2\sigma^2}) \\
  &= \sum_{i=0}^{m-1}\left\{ \log \frac{1}{\sqrt{2\pi}\sigma} + \log \exp(-\frac{(Y^{(i)}-\Theta^TX^{(i)})^2}{2\sigma^2}) \right\} \\
  &= m \cdot \log \frac{1}{\sqrt{2\pi}\sigma} - \frac{\sum_{i=0}^{m-1}(Y^{(i)}-\Theta^TX^{(i)})^2}{2\sigma^2} 
  \end{aligned}
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;化简到这里，我们发现，要使 $\mathcal{l}(\Theta)$ 最大，实际上就是使得 $\sum_{i=0}^{m-1}(Y^{(i)}-\Theta^TX^{(i)})^2$ 最小，而这正是我们在 <equation>equ_loss_define</equation> 中看到的 Linear Regression 问题的 Loss Function 定义，因此我们仅需要把 <equation>equ_loss_define</equation> 作为 Parameter Solver 的优化目标即可。
</div>

<h2 class="title">求解器: Batch/Stochastic Gradient Descent</h2>
<div class="div_learning_post">
  <label class="title">gd</label>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;本节我们将关心我们如何通过 <def>梯度下降 (Gradient Descent)</def> 来求解 <equation>equ_goal</equation> 代表的 Loss Function 优化目标。

  <h3 class="title">Batch Gradient Descent</h3>

  <h4 class="title">初步思路</h4>

  <div class="img" title="梯度下降">
    <img src="./pic/gd.gif" width=500px>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;一个初步的思路是，我们首先随机选定一组参数 $\Theta$，比如 $\Theta={\theta_0=0,\theta_1=0,...,\theta_{n-1}=0}$，然后我们不断改变 $\Theta$ 来降低 $\mathcal{J}(\theta)$ 的值。这个思路在输入向量是二维的时候，可以采用如上所示的图来表示。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;既然我们采用了逐步下降的办法，我们就需要使用到数学工具 —— 梯度。下面我们对相关数学知识进行回顾。

  <h4 class="title">求导的本质</h4>

  <div class="img" title="导数值的本质: 斜率">
    <img src="./pic/gradient.png" width=300px>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于单变量连续函数 $f(x)$ 来说，如果我们对其自变量 $x$ 进行求导:

  <div class="equation">
  $f'(x) = \frac{df(x)}{dx}$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;我们最终实际上会得到一个向量 $(1,f'(x))$，这个向量指定了当自变量取值为 $x^{(i)}$ 时，函数的增长方向向量 $(1,f'(x^{(i)}))$，如上图所示。这个增长方向向量也可以使用我们平时常说的 "斜率" 来表示，也即 $\tan\beta=\frac{f'(x^{(i)})}{1}=f'(x^{(i)})$，我们称这个 "斜率" 为 $f(x)$ 的 <def>梯度 (Gradient)</def>，记为 $\nabla(x)=f'(x)$。梯度 $\nabla(x)$ 反映了自变量 $x$ 发生的微小变化给 $f(x)$ 带来的影响。

  <div class="img" title="合成增长方向向量">
    <img src="./pic/gradient_2.png" height=400px>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;同样地，对于多维连续函数 $f(X)$ 来说，如果我们对其自变量 $X=[x_1,x_2,...,x_n]$ 求偏导数，我们会得到在各个方向上的 $f(X)$ 的增长方向向量 $(1,f'(x_1))$, $(1,f'(x_2))$, ..., $(1,f'(x_n))$。如上图所示，以二维的情况为例，若自变量在各个增长方向上的速率相同，则我们可以对各个增长方向向量进行合成，并求出总的梯度为

  <div class="equation">
  $\frac{\nabla(x_1)+\nabla(x_2)}{\sqrt{2}}=\frac{\frac{\partial f(x)}{\partial x_0}+\frac{\partial f(x)}{\partial x_1}}{\sqrt{2}}$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;如果自变量在各个增长方向上的速率不同，则就没有以上形式的结论。我们可以把总的梯度表示为由各个方向上的梯度组成的向量，也即:

  <div class="equation">
  $f'(X)$ $=$ $\nabla(X)$ $=$ $[\nabla(x_1),\nabla(x_2)]^{T}$ $=$ $[\frac{\partial(x)}{\partial x_1},\frac{\partial(x)}{\partial x_2}]^{T}$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;这样一来，我们如果知道了 $X$ 在各个分量 $x_p$ 上的变化率 $\alpha_p$，我们就能求出来 $f'(X)$ 的真实梯度:

  <div class="equation">
  $[\alpha_1, \alpha_2] \cdot \nabla(X)$ $=$ $[\alpha_1, \alpha_2] \cdot [\nabla(x_1),\nabla(x_2)]^{T}$ $=$ $\alpha_1\nabla(x_1) + \alpha_2\nabla(x_2)$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;可见，单变量的连续函数的梯度向量实际上是一种特殊情况，其向量只有一个分量，因此是一个标量。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;拓展到更多维的情况下，我们得到:

  <div class="equation">
  $\nabla(X)$$=$$[\nabla(x_1),\nabla(x_2),...,\nabla(x_n)]^{T}$$=$$[\frac{\partial(x)}{\partial x_1},\frac{\partial(x)}{\partial x_2},...,\frac{\partial(x)}{\partial x_n}]^{T}$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;梯度中的每一个分量 $\nabla(x_i)$ 代表着: 自变量 $X$ 的分量 $x_i$ 发生的微小变化给 $f(x)$ 带来的影响。

  <h4 class="title">Batch Gradient Descent</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;回到我们的问题: 采用逐渐下降的办法来逼近 $\mathcal{J}(\Theta)$ 的最小值。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;上面我们看到了，求解出 $f'(X^{(i)})$，我们就可以得到在 $X^{(i)}=(x_0^{(i)}, x_1^{(i)}, ..., x_{n-1}^{(i)})$ 这个点的增长方向向量，体现在各个分量 $x_p$ 的梯度值 $\nabla(x_p^{(i)}) = \frac{\partial(X)}{\partial x_p}|_{x_p=x_p^{(i)}}$。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;同样的道理，求解出 $\mathcal{J}'(\Theta^{(i)})$，我们就可以得到在 $\Theta^{(i)}=(\theta_0^{(i)}, \theta_1^{(i)}, ..., \theta_{n-1}^{(i)})$ 这个点的增长方向向量，体现在各个分量 $\theta_j$ 的梯度值 $\nabla\mathcal{J}(\theta_j^{(i)}) = \frac{\partial\mathcal{J}(\Theta)}{\partial\theta_j}|_{\theta_j=\theta_j^{(i)}}$。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;但是由于我们想要的是让 $\mathcal{J}(\Theta)$ 减小的效果，所以真正有用的是与「增长方向向量」方向相反的向量，因此我们实际上要的是:

  <div class="equation">
  $-\nabla\mathcal{J}(\theta_j) = -\frac{\partial\mathcal{J}(\Theta)}{\partial\theta_j}$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;因此，对于每一个参数，我们应用:

  <div class="equation" label="equ_theta_update">
  $\theta_j := \theta_j - \alpha \frac{\partial \mathcal{J}(\Theta)}{\partial \theta_j}$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;其中 $\alpha$ 称为 <def>Learning Rate</def>，可以理解为每次向着梯度的反方向步进的长度。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于梯度项 $\frac{\partial \mathcal{J}(\Theta)}{\partial \theta_j}$，让我们结合 <equation>equ_lr_model</equation> 和 <equation>equ_loss_define</equation>，代入上述等式求解。

  <div class="equation" label="equ_theta_partical">
  \begin{aligned}
  \frac{\partial \mathcal{J}(\Theta)}{\partial \theta_j} &= \frac{\partial}{\partial \theta_j}{\frac{1}{2}\sum_{i=0}^{m-1}(h(X^{(i)};\Theta)-Y^{(i)})^2} \\ \\
  &= [h(X^{(0)};\Theta)-Y^{(0)}] \cdot \frac{\partial}{\partial \theta_j}[h(X^{(0)};\Theta)-Y^{(0)}] + ... \\ \\
  & (注意到 \text{Linear Regression} 问题中，Y^{(i)} 实际上是一个标量，即 Y^{(i)}=y^{(i)}，因此) \\ \\
  &= [\theta_0x_0^{(0)}+...\theta_nx_n^{(0)}-y^{(0)}] \cdot \frac{\partial}{\partial \theta_j}[\theta_0x_0^{(0)}+...\theta_nx_n^{(0)}-y^{(0)}]+... \\ \\
  &= [\theta_0x_0^{(0)}+...\theta_nx_n^{(0)}-y^{(0)}] \cdot x_j^{(0)} + ... \\ \\
  &= \sum_{i=0}^{m-1}[\theta_0x_0^{(i)}+...\theta_nx_n^{(i)}-y^{(i)}] \cdot x_j^{(i)}
  \end{aligned}
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于 <equation>equ_theta_update</equation> 和 <equation>equ_theta_partical</equation>，我们可以得到更新每一个 $\theta_j$ 的公式:

  <div class="equation" label="equ_theta_final_update">
  $\theta_j := \theta_j - \alpha \cdot {\sum_{i=0}^{m-1}[\theta_0x_0^{(i)}+...\theta_nx_n^{(i)}-y^{(i)}] \cdot x_j^{(i)}}$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于 <equation>equ_theta_final_update</equation>，我们可以得到使用梯度下降迭代得到最终 $\Theta$ 的算法:

  <pre id="gd" style="display:hidden;">
    \begin{algorithm}
    \caption{Batch Gradient Descent}
    \begin{algorithmic}
    \INPUT Training Set $(X,Y)$
    \STATE $\Theta = \overrightarrow{0}$
    \WHILE{ Not Coverage }
      \FOR{$\theta_j$ in $\Theta$}
      \STATE EQUATION\_17($\theta_j$)
      \ENDFOR
    \ENDWHILE
    \end{algorithmic}
    \end{algorithm}
  </pre>
  <script>
      pseudocode.renderElement(document.getElementById("gd"));
  </script>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在我们已经完成了第一个学习算法，观察 <equation>equ_theta_final_update</equation>，可以发现我们在每个参数 $\theta_j$ 的每步更新时，我们需要结合所有 $m$ 条 Samples 来帮助我们计算梯度，因此我们把上述算法称为 <def>Batch Gradient Descent</def> (BGD, 批梯度下降)。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;思考一下，假设现在一共有一亿条 Samples，$\Theta$ 的维度是五万个参数，可以想像，使用 BGD 迭代来更新参数的方法将会十分缓慢。因此，我们下面介绍一种改进的方法。

  <h3 class="title">Stochastic Gradient Descent</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;上面我们发现导致 BGD 缓慢的原因是因为，每轮迭代中算每个参数 $\theta_j$ 的梯度需要使用到整个数据集，因此我们对 BGD 进行修改: 每轮迭代中算每个参数 $\theta_j$ 的梯度时，只随机选取一条 Sample 来进行计算 ，也即:

  <div class="equation">
  $\theta_j := \theta_j - \alpha \cdot [\theta_0x_0^{(i)}+...\theta_nx_n^{(i)}-y^{(i)}] \cdot x_j^{(i)}$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;这样一来，虽然我们不能保证每轮迭代中，对 $\theta_j$ 的更新都是朝着最优的方向前进，但是从平均的角度来看 (i.e. 我们最终还是把整个数据集喂进了学习算法)，我们最终会朝着最优的方向前进。修改过的算法如下:

  <pre id="gd" style="display:hidden;">
    \begin{algorithm}
    \caption{Stochastic Gradient Descent}
    \begin{algorithmic}
    \INPUT Training Set $(X,Y)$
    \STATE $\Theta = \overrightarrow{0}$
    \WHILE{ Not Coverage }
      \FOR{$\theta_j$ in $\Theta$}
      \STATE EQUATION\_18($\theta_j$)
      \ENDFOR
    \ENDWHILE
    \end{algorithmic}
    \end{algorithm}
  </pre>
  <script>
      pseudocode.renderElement(document.getElementById("gd"));
  </script>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;我们把修改过的算法称为 <def>Stochastic Gradient Descent</def> (SGD, 随机梯度下降)。
</div>


<h2 class="title">求解器: Normal Equation</h2>
<div class="div_learning_post">
<label class="title">ne</label>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Linear Regression 问题，除了采用常见的梯度下降算法以外，我们还可以使用矩阵方法来实现一步完成对 <equation>equ_goal</equation> 的求解。

  <h3 class="title">Notation</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;我们说一个 <def>矩阵函数</def> 把一个矩阵 $A$ 映射称为一个实数。比如如下函数：

  <div class="equation">
  <div class="cmath" align="center">
    $f{((A_{11},A_{12}),(A_{21},A_{22}))} = A_{11} + A_{12}^2$
  </div>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对一个上述矩阵函数求导，我们规定形式为:

  <div class="equation">
  <div class="cmath" align="center">
    $\nabla_{A}f(A)=((\frac{\partial f}{\partial A_{11}},\frac{\partial f}{\partial A_{12}}),(\frac{\partial f}{\partial A_{21}},\frac{\partial f}{\partial A_{22}}))$
    $=((1,2A_{12}),(0,0))$
  </div>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;另外，对于 $n \times n$ 的 Square Matrix (方块矩阵) $A$，<def>Trace</def> (迹) 的定义如下:

  <div class="equation">
  $trA=\sum_{i=1}^{n}A_{ii}$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Trace 具有如下的属性:

  <div class="equation">
  对于 $n \times n$ 的矩阵 $A$, $B$, $C$，我们有:
  <ol>
    <li>$trA = trA^T$</li>
    <li>$tr(A+B) = trA + trB$</li>
    <li>$tr(a \cdot A) = a \cdot trA$</li>
    <li>$trAB = trBA$</li>
    <li>$trABC = trCAB = trBCA$</li>
    <li>$......$</li>
  </ol>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;和梯度结合起来，不加证明地，我们给出 Trace 如下的属性:

  <div class="equation">
  对于 $n \times n$ 的矩阵 $A$, $B$ 我们有:
  <ol>
    <li>$\nabla_AtrAB = B^T$</li>
    <li>$\nabla_{A^T}f(A) = (\nabla_{A}f(A))^T$</li>
    <li>$\nabla_AtrABA^TC = CAB + C^TAB^T$</li>
    <li>$\nabla_A|A| = |A|(A^{-1})^T$ [p.s. $|A|$ 指的是 $A$ 的 Determinant (行列式) ]</li>
  </ol>
  </div>

  <h3 class="title">Normal Equation</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在让我们来关心我们如何基于对矩阵求解梯度的方式来一步解得最优参数。回顾 <equation>equ_x_y_theta</equation>，我们可以把各个式子表示为：

  <div class="equation">
  <div class="cmath" align="center">
  `X=((—(X^{(0)})^T—),(—(X^{(1)})^T—),(...),(—(X^{(m)})^T—))`，`\Theta=((\theta_0),(\theta_1),(...),(\theta_m))`，`Y=((Y^{(0)}),(Y^{(1)}),(...),(Y^{(m)}))`
  </div>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;因此有:

  <div class="equation">
  <div class="cmath" align="center">
  `X\Theta - Y``=``(((X^{(0)})^T\Theta),((X^{(1)})^T\Theta),(...),((X^{(m)})^T\Theta))-((Y^{(0)}),(Y^{(1)}),(...),(Y^{(m)}))``=``((h(X^{(0)};\Theta)-Y^{(0)}),(h(X^{(1)};\Theta)-Y^{(1)}),(...),(h(X^{(m)};\Theta)-Y^{(m)}))`
  </div>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;回顾 <equation>equ_loss_define</equation>，我们可以把式子表示为：

  <div class="equation">
  $\mathcal{J}(\Theta)=\frac{1}{2}(X\Theta-Y)^T(X\Theta-Y)$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在我们把 Lost Function $\mathcal{J}(\Theta)$ 表达为了矩阵的形式，结合上面我们讨论过的矩阵的一些性质，我们可以得到:

  <div class="equation">
  \begin{aligned}
  \nabla_\Theta\mathcal{J}(\Theta) &= \nabla_\Theta{\frac{1}{2}(X\Theta-Y)^T(X\Theta-Y)} \\
  &= \frac{1}{2}\nabla_\Theta{(\Theta^TX^T-Y^T)(X\Theta-Y)} \\
  &= \frac{1}{2} \nabla_\Theta (\Theta^TX^TX\Theta - \Theta^TX^TY - Y^TX\Theta+Y^TY) \\
  &= \frac{1}{2} [X^TX\Theta+X^TX\Theta-X^TY-X^TY] \\
  &= X^TX\Theta-X^TY 
  \end{aligned}
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;为了得到全局最优解，我们让上面得到的最终结果等于零向量，即:

  <div class="equation" label="equ_normal_equation">
  $X^TX\Theta = X^TY$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<equation>equ_normal_equation</equation> 即 <def>Normal Equation</def>，进一步我们得到:

  <div class="equation">
  $\Theta=(X^TX)^{-1}X^TY$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于这个式子，我们就可以直接得出最优的参数 $\Theta$，而不用使用迭代的方法来逐渐逼近最优解。
</div>

<h2 class="title">学习算法: Locally Weighted Regression</h2>
<div class="div_learning_post">

  <div class="img" title="线性回归">
    <img src="./pic/lr_exp.png" width=400px>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;上文我们介绍了 Linear Regression 算法，我们基于 Linear Regression 可以在 Training Set 上拟合出来一条直线，用于对新数据进行预测，如上图所示。但是倘若我们的样本空间中的数据并不能很好地使用一条直线来进行拟合的话，我们使用 Linear Regression 算法进行预测的结果将是十分糟糕的。本小节介绍一种针对 Linear Regression 算法进行优化的方法，Namely <def>Locally Weighted Regression</def> (LWR, 局部加权回归)。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;LWR 的基本思路是: 对新数据 $X^{(\text{new})}$ 进行预测时，首先在 Training Set 中找到与 $X^{(\text{new})}$ 更加接近的 Samples，基于这些 Samples 使用 Linear Regression 算出来一组参数 $\Theta^{(s)}$，然后使用 $\Theta^{(s)}$ 完成对 $X^{(\text{new})}$ 的预测。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于上述思路，我们修改我们的 Lost Function 如下所示:

  <div class="equation">
  $\mathcal{J}(\Theta) = \frac{1}{2} \cdot \sum\limits_{i=0}^{m-1}$ <note><b>$w^{(i)}$</b></note> $(h(X^{(i)};\Theta)-Y^{(i)})^2$
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;其中的 $w^{(i)}$ 定义如下:

  <div class="equation" label="equ_omega_define">
  `w^{(i)} = exp(-\frac{[X^{(i)}-X^{(\text{new})}]^2}{2\tau^2})`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;$w^{(i)}$ 的含义是：对于 Training Set 中的各条 Samples $(X^{(i)},Y^{(i)})$，距离 $X^{(\text{new})}$ 越近的 Sample 的权值 $w^{(i)}$ 将更接近于 1，距离 $X^{(\text{new})}$ 越远的 Sample 的权值 $w^{(i)}$ 将更接近于 $0$。这样一来，Training Set 中的各条 Samples 对最终训练出来的参数的影响就有区别了。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;另外，<equation>equ_omega_define</equation> 中的 $\tau$ 是一个 <def>Hyperparameter</def>，用于控制权值函数的 "胖瘦" 情况，如下图所示:

  <div class="img" title="超参用于控制影响范围">
    <img src="./pic/lwg.png" width=400px>
  </div>
</div>

<div class="div_ref" id="ref_container"></div>

</body>


<!--定理、引用、证明-->
<!-- 
<div class="theorm" id="theorm_id">
</div>

<theorm>theorm_id</theorm>

<div class="theorm_prove">
</div>
-->

<!--引用其它章节-->
<!-- 
<ref></ref> 
-->

<!--引用文献-->
<!-- 
<cite></cite> 
-->

<!--关键词-->
<!-- 
<def></def> 
-->

<!--醒目注意-->
<!-- 
<note></note> 
-->

<!--表格-->
<!--
<table border="1" align="center" bgcolor="#FFFFFF">
  <caption>表格</caption>
  <tr>
    <th>A</th>
    <th>B</th>
    <th>C</th>
  </tr>
  <tr>
    <td>xxx</td>
    <td>xxx</td>
    <td>xxx</td>
  </tr>
</table>
-->

<!--矩阵公式-->
<!--
<div class="cmath" align="center">
  $((1, 0),(1, 0))$
</div><br>
-->

<!--伪代码-->
<!--
<pre id="quicksort" style="display:hidden;">
  % This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
  \begin{algorithm}
  \caption{Quicksort}
  \begin{algorithmic}
  \PROCEDURE{Quicksort}{$A, p, r$}
      % Add Here

      % 空行
      % \STATE \texttt{\\}
  \ENDPROCEDURE
  \end{algorithmic}
  \end{algorithm}
</pre>
<script>
    pseudocode.renderElement(document.getElementById("quicksort"));
</script>
-->
<!--
Latex 伪代码格式见: https://github.com/SaswatPadhi/pseudocode.js
-->

<!--图片-->
<!--
<div align="center">
  <img src="./pic/xxx.png" width=80%>
</div>
-->

<!--正文-->
<!--
<p>
&nbsp;&nbsp;&nbsp;&nbsp;公式：<span>$\overline{A}\overline{B}$</span>
</p>
-->
      </div>
      
      
      
    </div>
    
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Algorithm/">ALGORITHM</a></li>
          <li>ML_2_LINEAR_REGRESSION_AND_GRADIENT_DESCENT</li>
        
  </ul>

    
    
    


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhuobin Huang"
      src="/images/avatar_2.png">
  <p class="site-author-name" itemprop="name">Zhuobin Huang</p>
  <div class="site-description" itemprop="description">System Engineer</div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zobinHuang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zobinHuang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zobin1999@gmail.com" title="E-Mail → mailto:zobin1999@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/2861056530" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;2861056530" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/HwangZobin" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;HwangZobin" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhuobin Huang</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
