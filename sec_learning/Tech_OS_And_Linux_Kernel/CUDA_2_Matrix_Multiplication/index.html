<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Exo 2:300,300italic,400,400italic,700,700italic|Caveat:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zobinhuang.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":180,"display":"post","padding":10,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:type" content="website">
<meta property="og:title" content="Matrix Multiplication">
<meta property="og:url" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_2_Matrix_Multiplication/index.html">
<meta property="og:site_name" content="Zobin">
<meta property="og:description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_2_Matrix_Multiplication/pic/matrix_representation.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_2_Matrix_Multiplication/pic/basic.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_2_Matrix_Multiplication/pic/matrix_a_accessing.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_2_Matrix_Multiplication/pic/matrix_b_accessing.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_2_Matrix_Multiplication/pic/tile.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_2_Matrix_Multiplication/pic/xxx.png">
<meta property="article:published_time" content="2022-07-30T07:36:03.398Z">
<meta property="article:modified_time" content="2022-07-30T07:36:03.398Z">
<meta property="article:author" content="Zhuobin Huang">
<meta property="article:tag" content="Zobin">
<meta property="article:tag" content="黄卓彬">
<meta property="article:tag" content="zobinHuang">
<meta property="article:tag" content="网络工程">
<meta property="article:tag" content="Networking Engineering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png">

<link rel="canonical" href="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_2_Matrix_Multiplication/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Matrix Multiplication | Zobin
</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Zobin" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zobin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Lovin' Tech with Tea</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about-me">

    <a href="/sec_about/" rel="section"><i class="fa fa-user fa-fw"></i>About Me</a>

  </li>
        <li class="menu-item menu-item-library">

    <a href="/sec_learning/" rel="section"><i class="fa fa-duotone fa-book fa-fw"></i>Library</a>

  </li>
        <li class="menu-item menu-item-production">

    <a href="/sec_music/" rel="section"><i class="fa fa-music fa-fw"></i>Production</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="en">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">Matrix Multiplication
</h1>

<div class="post-meta">
  
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Tech_OS_And_Linux_Kernel/">TECH_OS_AND_LINUX_KERNEL</a></li>
          <li>CUDA_2_MATRIX_MULTIPLICATION</li>
        
  </ul>

</div>

</header>

      
      
      
      <div class="post-body">
          <head>
<!--导入样式表-->
<link rel="stylesheet" type="text/css" href="style/index.css">

<!--导入网页脚本-->
<script src="script/index.js"></script>

<!--支持伪代码显示-->
<script>
    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3.0.0/es5/tex-chtml.js"
        integrity="sha256-3Fdoa5wQb+JYfEmTpQHx9sc/GuwpfC/0R9EpBki+mf8=" crossorigin>
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js">
</script>

<!--支持网页公式显示-->    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>

<!--支持矩阵显示-->
<script type="text/javascript">
  run_maths = function() {
    if (document.querySelector('[class*="cmath"]') !== null) {
      if (typeof (mjax_path)=='undefined') { mjax_path='https://cdn.jsdelivr.net/npm/mathjax@2'; }
      if (typeof (mjax_config)=='undefined') { mjax_config='AM_CHTML'; }
      smjax = document.createElement ('script');
      smjax.setAttribute('src',`${mjax_path}/MathJax.js?config=${mjax_config}`);
      smjax.setAttribute('async',true);
      document.getElementsByTagName('head')[0].appendChild(smjax);
    }
  };
  if (document.readyState === 'loading') {  
    window.addEventListener('DOMContentLoaded', run_maths); 
  } else { 
    run_maths(); 
  }
</script>
</head>

<body onload="load_page()">

<div align="center" class="div_indicate_source">
  <h4>⚠ 转载请注明出处：<font color="red"><i>作者：ZobinHuang，更新日期：July 28 2022</i></font></h4>
</div>
<div class="div_licence">
  <br>
  <div align="center">
      <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="知识共享许可协议" style="border-width:0; margin-left: 20px; margin-right: 20px;" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a>
  </div>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;本<span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" rel="dct:type">作品</span>由 <span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName"><b>ZobinHuang</b></span> 采用 <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><font color="red">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</font></a> 进行许可，在进行使用或分享前请查看权限要求。若发现侵权行为，会采取法律手段维护作者正当合法权益，谢谢配合。
  </p>
</div>
<br>
<div class="div_catalogue">
  <div align="center">
    <h1> 目录 </h1>
    <p>
    <font size="3px">有特定需要的内容直接跳转到相关章节查看即可。</font>
  </div>
  <div class="div_load_catalogue_alert" id="load_catalogue_alert">正在加载目录...</div>
  <div class="div_catalogue_container" id="catalogue_container">
  </div>
</div><br>

<!-- Start your post here -->
<h2 class="title">矩阵在内存中的存储形式</h2>
<div class="div_learning_post">
  <div class="img" title="矩阵在内存中的数组存储形式" label="matrix_repre">
    <img src="./pic/matrix_representation.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先我们明确矩阵在内存中的存储格式。如 <imgref>matrix_repre</imgref> 所示，矩阵在内存中将被展开成数组进行存储，图中展示的是按行进行展开的存储方法。
</div>

<h2 class="title">基础矩阵乘法</h2>
<div class="div_learning_post">
  <div class="img" title="基础版本矩阵乘法" label="basic_mul">
    <img src="./pic/basic.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;本文我们将讨论 Square Matrix 乘法的 CUDA 实现。对于 Square Matrix 乘法来说，目标矩阵 $\text{Matrix C}$ 的第 $(x,y)$ 个元素，是由源矩阵 $\text{Matrix A}$ 的第 $x$ 行元素，和源矩阵 $\text{Matrix B}$ 的第 $y$ 列对应元素相乘后相加所得到的。在我们基础实现的版本中，我们为目标矩阵 $\text{Matrix C}$ 中的每一个元素分配一条线程进行处理，CUDA Kernel 如下所示:

  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief [CUDA Kernel] Conduct square matrix multiplication (A*B=C)</span></span><br><span class="line"><span class="comment"> * \param matrix_A  source matrix</span></span><br><span class="line"><span class="comment"> * \param matrix_B  source matrix</span></span><br><span class="line"><span class="comment"> * \param matrix_C  destination matrix</span></span><br><span class="line"><span class="comment"> * \param d         dimension of square matrix</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">squareMatrixMul</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> *matrix_A,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> *matrix_B,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *matrix_C,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> d)</span></span>&#123;</span><br><span class="line">  <span class="comment">// check kernel shape</span></span><br><span class="line">  assert(blockDim.x == blockDim.y);</span><br><span class="line">  assert(gridDim.x == gridDim.y);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// obtain corresponding row and column for current thread</span></span><br><span class="line">  <span class="keyword">int</span> row_index = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">  <span class="keyword">int</span> col_index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// initialize destination element</span></span><br><span class="line">  <span class="keyword">int</span> dest_index = row_index*d+col_index;</span><br><span class="line">  matrix_C[dest_index] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// sum of product</span></span><br><span class="line">  <span class="keyword">if</span>(dest_index &lt; d*d)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;d; i++)&#123;</span><br><span class="line">      matrix_C[dest_index] += matrix_A[row_index*d+i] * matrix_B[i*d+col_index];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在 Line 18~19 中，结合 <imgref>basic_mul</imgref> 图示，我们可以根据 Thread Block 和 Thread ID 对当前 Thread 处理的目标矩阵 $\text{Matrix C}$ 元素的行序号和列序号进行计算。然后在 Line 27~29 的 <code>for</code> 循环中我们就可以根据源矩阵 $\text{Matrix A}$ 的目标行序号和 $\text{Matrix B}$ 的目标列序号展开计算，并将最后结果写入到 $\text{Matrix C}$ 中。
</div>

<h2 class="title">基于 Memory Access Coalescing 的矩阵乘法</h2>
<div class="div_learning_post">

  <div class="img" title="矩阵 A 的 Memory Access Pattern" label="matrix_a_access">
    <img src="./pic/matrix_a_accessing.png" width="80%" />
  </div>

  <div class="img" title="矩阵 B 的 Memory Access Pattern" label="matrix_b_access">
    <img src="./pic/matrix_b_accessing.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在上面矩阵乘法的操作中，针对 $\text{Matrix A}$ 是按行访问，针对 $\text{Matrix A}$ 是按列进行访问。二者的 Memory Access Pattern 如 <imgref>matrix_a_access</imgref> 和 <imgref>matrix_b_access</imgref> 所示。可以看到 $\text{Matrix A}$ 事实上访问的是一段连续的内存空间，而 $\text{Matrix B}$ 访问的是离散的内存空间。对于 GPU 来说，如果指令访问的是多段连续的内存空间，那么多次对小内存的访问可以被合并为一次多大内存的访问，因此 $\text{Matrix A}$ 的访问事实上会比 $\text{Matrix B}$ 更快。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;为了使得 $\text{Matrix B}$ 的访问有和 $\text{Matrix A}$ 一样的性能，我们可以采用的优化方法是将 $\text{Matrix B}$ 转置后进行存储，并且使用和 $\text{Matrix A}$ 一样的访存计算方式进行访问，这样一来 $\text{Matrix B}$ 的 Memory Access Pattern 就有和 $\text{Matrix A}$ 一样的性能了。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;优化后的 CUDA Kernel 如下所示:

  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief [CUDA Kernel] Conduct square matrix multiplication (A*B=C)</span></span><br><span class="line"><span class="comment"> *        with aligned memory access pattern</span></span><br><span class="line"><span class="comment"> * \param matrix_A  source matrix</span></span><br><span class="line"><span class="comment"> * \param matrix_B  source matrix (after transposed)</span></span><br><span class="line"><span class="comment"> * \param matrix_C  destination matrix</span></span><br><span class="line"><span class="comment"> * \param d         dimension of square matrix</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">alignedSquareMatrixMul</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> *matrix_A,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> *matrix_B,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *matrix_C,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> d)</span></span>&#123;</span><br><span class="line">  <span class="comment">// check kernel shape</span></span><br><span class="line">  assert(blockDim.x == blockDim.y);</span><br><span class="line">  assert(gridDim.x == gridDim.y);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// obtain corresponding row and column for current thread</span></span><br><span class="line">  <span class="keyword">int</span> row_index = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">  <span class="keyword">int</span> col_index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// initialize destination element</span></span><br><span class="line">  <span class="keyword">int</span> dest_index = row_index*d+col_index;</span><br><span class="line">  matrix_C[dest_index] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// sum of product</span></span><br><span class="line">  <span class="keyword">if</span>(dest_index &lt; d*d)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;d; i++)&#123;</span><br><span class="line">      matrix_C[dest_index] += matrix_A[row_index*d+i] * matrix_B[col_index*d+i];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief Transpose square matrix</span></span><br><span class="line"><span class="comment"> * \param matrix    source matrix</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">transposeSquareMatrix</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;matrix)</span></span>&#123;</span><br><span class="line">    <span class="comment">// obtained matrix size</span></span><br><span class="line">    <span class="keyword">int</span> size = matrix.size();</span><br><span class="line">    <span class="keyword">double</span> _dimension = <span class="built_in">sqrt</span>(size);</span><br><span class="line">    <span class="keyword">int</span> dimension = (<span class="keyword">int</span>)_dimension;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// assertion to check matrix shape</span></span><br><span class="line">    assert(_dimension-dimension == <span class="number">0.0f</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// temp intermediate matrix</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tmp_matrix;</span><br><span class="line">    tmp_matrix.reserve(size);</span><br><span class="line">    tmp_matrix = matrix;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// for all rows</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;dimension; i++)&#123;</span><br><span class="line">        <span class="comment">// for all column</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;dimension; j++)&#123;</span><br><span class="line">            matrix[i*dimension+j] = tmp_matrix[j*dimension+i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</div>

<h2 class="title">基于 Cache Tiling 的矩阵乘法</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;上面展示的基础版本的矩阵乘法中，我们在进行 sum of product 时，在 Line 28 中引入了大量的内存访问。与 Shared Memory 和其它 Cache 相比，访存的开销是较大的。访存将导致运行 CUDA Kernel 的 SM 陷入 Stall 的状态。本节中我们将把矩阵加载到 Shared Memory 中，以加速 CUDA Kernel 的执行过程。

  <div class="img" title="基于 Cache Tiling 矩阵乘法" label="tiled_mul">
    <img src="./pic/tile.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;如 <imgref>tiled_mul</imgref> 所示，以 $\text{Matrix C}$ 中标有颜色的 Thread Block 为例。要计算出该方块中的元素的值，则必须加载 $\text{Matrix A}$ 和 $\text{Matrix B}$ 中带有颜色的方块中的数据。由于 Shared Memory 通常容量有限，因此在这里我们采取 Matrix Tiling 的策略。我们首先加载 $\text{Matrix A}$ 和 $\text{Matrix B}$ 中蓝色部分的数据，并进行 sum of product 的计算，然后将中间结果记录下来；然后加载红色部分的数据，然后进行 sum of product 的计算，最后和中间结果进行相加，就能得到 $\text{Matrix C}$ 的最终目标值。每一个 Thread Block 都进行上述步骤，最终我们就能得到整个 $\text{Matrix C}$ 的计算结果。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于 Cache Tiling 的 CUDA Kernel 源代码如下所示:

  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief [CUDA Kernel] Conduct square matrix multiplication (A*B=C)</span></span><br><span class="line"><span class="comment"> *        based on cache tiling, make sure tile size is equal to </span></span><br><span class="line"><span class="comment"> *        block size</span></span><br><span class="line"><span class="comment"> * \param matrix_A  source matrix</span></span><br><span class="line"><span class="comment"> * \param matrix_B  source matrix</span></span><br><span class="line"><span class="comment"> * \param matrix_C  destination matrix</span></span><br><span class="line"><span class="comment"> * \param tile_size size of each tile</span></span><br><span class="line"><span class="comment"> * \param d         dimension of square matrix</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">tiledSquareMatrixMul</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> *matrix_A,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> *matrix_B,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *matrix_C,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> tile_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> d)</span></span>&#123;</span><br><span class="line">  <span class="comment">// check kernel shape</span></span><br><span class="line">  assert(blockDim.x == blockDim.y);</span><br><span class="line">  assert(gridDim.x == gridDim.y);</span><br><span class="line">  assert(tile_size == blockDim.x);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// declare share memory are for submatrix</span></span><br><span class="line">  <span class="keyword">extern</span> __shared__ <span class="keyword">int</span> tile[];</span><br><span class="line">  <span class="keyword">int</span>* tile_A = tile;</span><br><span class="line">  <span class="keyword">int</span>* tile_B = tile+tile_size*tile_size;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// obtain global row and column index for current thread</span></span><br><span class="line">  <span class="keyword">int</span> row_index = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">  <span class="keyword">int</span> col_index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// obtain shared memory index for current thread</span></span><br><span class="line">  <span class="keyword">int</span> shared_tile_index = threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> tmp = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// traverse all tiles</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;d; i += tile_size)&#123;</span><br><span class="line">    <span class="comment">// load element into shared memory</span></span><br><span class="line">    tile_A[shared_tile_index] = matrix_A[row_index*d+threadIdx.x+i];</span><br><span class="line">    tile_B[shared_tile_index] = matrix_B[threadIdx.y*d+col_index+i*d];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// wait for both tiles to be loaded by threads in current CLB</span></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// computation</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;tile_size; j++)&#123;</span><br><span class="line">      tmp += tile_A[threadIdx.y*blockDim.x+j]*tile_B[j*blockDim.x+threadIdx.x];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// wait for all threads finish computation for current tiles</span></span><br><span class="line">    <span class="comment">// before loading in new one</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// write result to global memory</span></span><br><span class="line">  matrix_C[row_index*d+col_index] = tmp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在上面的代码中，Line 23~25 我们首先基于 <code>__shared__</code> 关键字声明了一块位于 Shared Memory 中的内存空间，值得注意的是我们是通过 <code>extern</code> 关键字以 <note>动态分配</note> 的方式声明这段空间的。接着我们在 Line 37~53 这个 <code>for</code> loop 中，分 Tiles 地进行数据的加载和计算，其中在 Line 39~40 我们将当前 Thread Block 在当前 Tile 所需要的数据拉取到 Shared Memory 中，然后在 Line 46~48 的 <code>for</code> loop 中完成相应的计算。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;另外值得注意的是，我们在将数据加载到 Shared Memory 后，在 Line 43 我们调用了 <code>__syncthreads()</code> API，这个 API 会阻塞等待当前 Thread Block 中所有的 Threads 运行到当前这一步，因为只有在当前 Thread Block 中所有的 Threads 都完成数据向 Shared Memory 中的拷贝后，我们才能进行后续的计算操作。在当前 Thread Block 中所有的 Threads 完成计算过程后，在 Line 52 我们同样调用了该 API，因为只有当所有的 Threads 完成运算之后，我们才能继续将目标数据拷贝到 Shared Memory 中，这个操作将会覆盖 Shared Memory 中原有的数据。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;另外，由于该 Kernel 采用了动态分配 Shared Memory 的方式，因此我们在 CPU 侧代码 launch 这个 kernel 的时候，需要在 <code><<<>>></code> 中加入第三个参数，用于指定动态分配的 Shared Memory 空间的大小，如下所示:

  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// initialize kernel configuration</span></span><br><span class="line"><span class="comment">// number of kernels per block (one dimension)</span></span><br><span class="line"><span class="keyword">int</span> NUM_THREADS_PER_BLOCK = <span class="number">1</span> &lt;&lt; <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// obtain shared memory size for each thread block</span></span><br><span class="line"><span class="keyword">int</span> shared_memory_size = <span class="number">2</span>*NUM_THREADS_PER_BLOCK*NUM_THREADS_PER_BLOCK*<span class="keyword">sizeof</span>(<span class="keyword">int</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// launch kernel</span></span><br><span class="line">tiledSquareMatrixMul&lt;&lt;&lt;blocks, threads, shared_memory_size&gt;&gt;&gt;(</span><br><span class="line">              d_matrix_A, d_matrix_B, d_matrix_C, NUM_THREADS_PER_BLOCK, N);</span><br></pre></td></tr></table></figure>
</div>


<div class="div_ref" id="ref_container"></div>

</body>

<!--图片、引用-->
<!-- 
<div class="img" title="img title" label="img_label" source="url">
  <img src="" height="" />
</div>

<imaging>img_label</imaging>
-->

<!--等式、引用-->
<!-- 
<div class="equation" label="equation_label">
</div>

<equation>equation_label</equation>
-->

<!--定理、引用、证明-->
<!-- 
<div class="theorm" label="theorm_label">
</div>

<theorm>theorm_label</theorm>

<div class="theorm_prove">
</div>
-->

<!--引用其它章节-->
<!-- 
<ref></ref> 
-->

<!--引用文献-->
<!-- 
<cite></cite> 
-->

<!--关键词-->
<!-- 
<def></def> 
-->

<!--醒目注意-->
<!-- 
<note></note> 
-->

<!--表格-->
<!--
<div class="table" title="Table Title" label="table_label">
  <table border="1" align="center" bgcolor="#FFFFFF">
    <tr>
      <th>A</th>
      <th>B</th>
      <th>C</th>
    </tr>
    <tr>
      <td>xxx</td>
      <td>xxx</td>
      <td>xxx</td>
    </tr>
  </table>
</div>
-->

<!--矩阵公式-->
<!--
<div class="cmath" align="center">
  `((1, 0),(1, 0))`
</div><br>
-->

<!--伪代码-->
<!--
<pre id="quicksort" style="display:hidden;">
  % This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
  \begin{algorithm}
  \caption{Quicksort}
  \begin{algorithmic}
  \PROCEDURE{Quicksort}{$A, p, r$}
      % Add Here

      % 空行
      % \STATE \texttt{\\}
  \ENDPROCEDURE
  \end{algorithmic}
  \end{algorithm}
</pre>
<script>
    pseudocode.renderElement(document.getElementById("quicksort"));
</script>
-->
<!--
Latex 伪代码格式见: https://github.com/SaswatPadhi/pseudocode.js
-->

<!--图片-->
<!--
<div align="center">
  <img src="./pic/xxx.png" width=80%>
</div>
-->

<!--正文-->
<!--
<p>
&nbsp;&nbsp;&nbsp;&nbsp;公式：<span>`\overline{A}\overline{B}`</span>
</p>
-->
      </div>
      
      
      
    </div>
    
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Tech_OS_And_Linux_Kernel/">TECH_OS_AND_LINUX_KERNEL</a></li>
          <li>CUDA_2_MATRIX_MULTIPLICATION</li>
        
  </ul>

    
    
    


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhuobin Huang"
      src="/images/smile_me.jpeg">
  <p class="site-author-name" itemprop="name">Zhuobin Huang</p>
  <div class="site-description" itemprop="description">System Engineer</div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zobinHuang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zobinHuang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zobin1999@gmail.com" title="E-Mail → mailto:zobin1999@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhuobin Huang</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
