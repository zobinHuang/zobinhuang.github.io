<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Exo 2:300,300italic,400,400italic,700,700italic|Caveat:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zobinhuang.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":180,"display":"post","padding":10,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:type" content="website">
<meta property="og:title" content="Vector Addition">
<meta property="og:url" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/index.html">
<meta property="og:site_name" content="Zobin">
<meta property="og:description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/single_pointer.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/deep_copy_nightmare.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/on_demand_1.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/on_demand_2.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/on_demand_3.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/over_sub_1.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/over_sub_2.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/over_sub_3.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/over_sub_4.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/system_atom_1.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/system_atom_2.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/system_atom_3.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/prefetching.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/hint_1.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/hint_2.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/hint_3.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/hint_4.png">
<meta property="og:image" content="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/pic/xxx.png">
<meta property="article:published_time" content="2022-07-28T08:46:06.674Z">
<meta property="article:modified_time" content="2022-07-28T08:46:06.674Z">
<meta property="article:author" content="Zhuobin Huang">
<meta property="article:tag" content="Zobin">
<meta property="article:tag" content="黄卓彬">
<meta property="article:tag" content="zobinHuang">
<meta property="article:tag" content="网络工程">
<meta property="article:tag" content="Networking Engineering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png">

<link rel="canonical" href="https://zobinhuang.github.io/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_1_Vector_Addition/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Vector Addition | Zobin
</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Zobin" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zobin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Lovin' Tech with Tea</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about-me-(关于我)">

    <a href="/sec_about/" rel="section"><i class="fa fa-user fa-fw"></i>About Me (关于我)</a>

  </li>
        <li class="menu-item menu-item-library-(知识库)">

    <a href="/sec_learning/" rel="section"><i class="fa fa-duotone fa-book fa-fw"></i>Library (知识库)</a>

  </li>
        <li class="menu-item menu-item-music-(独立音乐人)">

    <a href="/sec_music/" rel="section"><i class="fa fa-music fa-fw"></i>Music (独立音乐人)</a>

  </li>
        <li class="menu-item menu-item-thoughts-(想法)">

    <a href="/sec_thoughts/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Thoughts (想法)</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="en">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">Vector Addition
</h1>

<div class="post-meta">
  
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Tech_OS_And_Linux_Kernel/">TECH_OS_AND_LINUX_KERNEL</a></li>
          <li>CUDA_1_VECTOR_ADDITION</li>
        
  </ul>

</div>

</header>

      
      
      
      <div class="post-body">
          <head>
<!--导入样式表-->
<link rel="stylesheet" type="text/css" href="style/index.css">

<!--导入网页脚本-->
<script src="script/index.js"></script>

<!--支持伪代码显示-->
<script>
    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3.0.0/es5/tex-chtml.js"
        integrity="sha256-3Fdoa5wQb+JYfEmTpQHx9sc/GuwpfC/0R9EpBki+mf8=" crossorigin>
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js">
</script>

<!--支持网页公式显示-->    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>

<!--支持矩阵显示-->
<script type="text/javascript">
  run_maths = function() {
    if (document.querySelector('[class*="cmath"]') !== null) {
      if (typeof (mjax_path)=='undefined') { mjax_path='https://cdn.jsdelivr.net/npm/mathjax@2'; }
      if (typeof (mjax_config)=='undefined') { mjax_config='AM_CHTML'; }
      smjax = document.createElement ('script');
      smjax.setAttribute('src',`${mjax_path}/MathJax.js?config=${mjax_config}`);
      smjax.setAttribute('async',true);
      document.getElementsByTagName('head')[0].appendChild(smjax);
    }
  };
  if (document.readyState === 'loading') {  
    window.addEventListener('DOMContentLoaded', run_maths); 
  } else { 
    run_maths(); 
  }
</script>
</head>

<body onload="load_page()">

<div align="center" class="div_indicate_source">
  <h4>⚠ 转载请注明出处：<font color="red"><i>作者：ZobinHuang，更新日期：July 25 2022</i></font></h4>
</div>
<div class="div_licence">
  <br>
  <div align="center">
      <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="知识共享许可协议" style="border-width:0; margin-left: 20px; margin-right: 20px;" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a>
  </div>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;本<span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" rel="dct:type">作品</span>由 <span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName"><b>ZobinHuang</b></span> 采用 <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><font color="red">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</font></a> 进行许可，在进行使用或分享前请查看权限要求。若发现侵权行为，会采取法律手段维护作者正当合法权益，谢谢配合。
  </p>
</div>
<br>
<div class="div_catalogue">
  <div align="center">
    <h1> 目录 </h1>
    <p>
    <font size="3px">有特定需要的内容直接跳转到相关章节查看即可。</font>
  </div>
  <div class="div_load_catalogue_alert" id="load_catalogue_alert">正在加载目录...</div>
  <div class="div_catalogue_container" id="catalogue_container">
  </div>
</div><br>

<h2 class="title">Manually-managed Memory</h2>
<div class="div_learning_post">
  <h3 class="title">示例程序</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先，下面是最简单的实现向量加法的 CUDA 程序:

  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cassert&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief [CUDA Kernel] Conduct Vector Adding (a+b=c)</span></span><br><span class="line"><span class="comment"> * \param vector_a source vector</span></span><br><span class="line"><span class="comment"> * \param vector_b source vector</span></span><br><span class="line"><span class="comment"> * \param vector_c destination vector</span></span><br><span class="line"><span class="comment"> * \param d dimension of vectors</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">vectorAdd</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> *__restrict vector_a, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> *__restrict vector_b, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *__restrict vector_c, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> d)</span></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (tid &lt; d)&#123;</span><br><span class="line">    vector_c[tid] = vector_a[tid] + vector_b[tid];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief verify the vector addition result from GPU</span></span><br><span class="line"><span class="comment"> * \param vector_a source vector</span></span><br><span class="line"><span class="comment"> * \param vector_b source vector</span></span><br><span class="line"><span class="comment"> * \param vector_c destination vector</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">verifyResult</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;vector_a, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;vector_b, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;vector_c)</span></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;vector_a.size(); i++)</span><br><span class="line">    assert(vector_c[i] == vector_a[i]+vector_b[i]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="comment">// initialize constants</span></span><br><span class="line">  <span class="keyword">constexpr</span> <span class="keyword">int</span> N = <span class="number">1</span> &lt;&lt; <span class="number">16</span>;</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="keyword">int</span> unit_size = <span class="keyword">sizeof</span>(<span class="keyword">int</span>);</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="keyword">int</span> vector_size = N*unit_size;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// create vector in host memory</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vector_a;</span><br><span class="line">  vector_a.reserve(N);</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vector_b;</span><br><span class="line">  vector_b.reserve(N);</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vector_c;</span><br><span class="line">  vector_c.reserve(N);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// initialize random value for each source vector</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++)&#123;</span><br><span class="line">    vector_a.push_back(rand()%<span class="number">100</span>);</span><br><span class="line">    vector_b.push_back(rand()%<span class="number">100</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// allocate memory space on device</span></span><br><span class="line">  <span class="keyword">int</span> *d_vector_a, *d_vector_b, *d_vector_c;</span><br><span class="line">  cudaMalloc(&amp;d_vector_a, vector_size);</span><br><span class="line">  cudaMalloc(&amp;d_vector_b, vector_size);</span><br><span class="line">  cudaMalloc(&amp;d_vector_c, vector_size);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// copy data from host memory to device memory</span></span><br><span class="line">  cudaMemcpy(d_vector_a, vector_a.data(), vector_size, cudaMemcpyHostToDevice);</span><br><span class="line">  cudaMemcpy(d_vector_b, vector_b.data(), vector_size, cudaMemcpyHostToDevice);</span><br><span class="line">  cudaMemcpy(d_vector_c, vector_c.data(), vector_size, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// initialize kernel configuration</span></span><br><span class="line">  <span class="comment">// number of kernels per block</span></span><br><span class="line">  <span class="keyword">int</span> NUM_THREADS_PER_BLOCK = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// number of blocks per grid</span></span><br><span class="line">  <span class="keyword">int</span> NUM_BLOCKS_PER_GRID = vector_size % NUM_THREADS_PER_BLOCK == <span class="number">0</span> ?</span><br><span class="line">                            vector_size / NUM_THREADS_PER_BLOCK :</span><br><span class="line">                            vector_size / NUM_THREADS_PER_BLOCK + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// launch kernel</span></span><br><span class="line">  vectorAdd&lt;&lt;&lt;NUM_BLOCKS_PER_GRID, NUM_THREADS_PER_BLOCK&gt;&gt;&gt;(d_vector_a, d_vector_b, d_vector_c, vector_size);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// copy result back to host memory</span></span><br><span class="line">  cudaMemcpy(vector_c.data(), d_vector_c, vector_size, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// verify result</span></span><br><span class="line">  verifyResult(vector_a, vector_b, vector_c);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// free device memory</span></span><br><span class="line">  cudaFree(d_vector_a);</span><br><span class="line">  cudaFree(d_vector_b);</span><br><span class="line">  cudaFree(d_vector_c);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Get correct vector addition redsult!&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在这段程序中:

  <ul>
    <li>Line 19~28 定义了一个 CUDA kernel，<code>__global__</code> 关键字指明了这个函数是从 CPU 调用，在 GPU 上被执行的函数。在定义输入的指针参数的时候，代码中 <code>__restrict</code> 关键字，表明当前指针在存活期间是独占内存的，不会有其他指针指向这个地址，该声明可以用于编译器优化 <cite>restrict_keyword</cite>;</li>
    <li>Line 85 的 Kernel Launch 是异步的，在 CPU 调用之后会立即返回;</li>
    <li>Line 88 的 <code>cudaMemcpy</code> 是一个同步调用，会等待 Kernel 把结果算好之后，放在 <code>d_c</code> 中之后再进行 Copy 的操作，因此 <code>cudaMemcpy</code> 也会充当 synchronization barrier 的作用;</li>
  </ul>

  <h3 class="title">关键 API</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在上面的代码中，我们使用了如下所示的 CUDA API:

  <div class="table" title="Manually-managed Memory Vector Addition 所使用的 CUDA API">
    <table border="1" align="center" bgcolor="#FFFFFF">
      <tr>
        <th align="center">API</th>
        <th align="center">功能</th>
      </tr>
      <tr>
        <td><code><def>cudaMalloc</def>(&d_vector_a, vector_size)</code></td>
        <td>在 GPU 设备上分配内存空间</td>
      </tr>
      <tr>
        <td>
          <code><def>cudaMemcpy</def>(d_vector_a, vector_a.data(), vector_size, cudaMemcpyHostToDevice)</code><br>
          <code><def>cudaMemcpy</def>(vector_c.data(), d_vector_c, vector_size, cudaMemcpyDeviceToHost)</code>
        </td>
        <td>在主机和设备之间拷贝数据</td>
      </tr>
      <tr>
        <td><code><def>cudaFree</def>(d_vector_a)</code></td>
        <td>释放在设备上分配的内存空间</td>
      </tr>
    </table>
  </div>
</div>

<h2 class="title">Unified Memory</h2>
<div class="div_learning_post">
  <h3 class="title">Motivation</h3>

  <div class="img" title="Single-pointer" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf">
    <img src="./pic/single_pointer.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;CUDA 提供了一种自动管理变量存储位置的方法，而无需程序员手动进行管理，这种机制称为 <def>Unified Memory</def>，以下简称 UM。Unified Memory 的初衷是为了实现 <def>Single Pointer</def> 的程序设计: 即使某个变量名所代表的背后的数据可能被多个处理器使用，UM 使得我们在程序上可以基于一个指针来对其进行读写访问，将这部分数据的存储位置交给 CUDA Runtime 处理，而无需手动维护管理多个指针变量。

  <div class="img" title="Deep-copy Nightmare" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="deep_copy_nightmare">
    <img src="./pic/deep_copy_nightmare.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;另外，UM 解决的另一个问题是深度拷贝的问题。如 <imgref>deep_copy_nightmare</imgref> 所示，在不采用 UM 的情况下，如果想要在多个处理器之间实现一个二维数组的管理，那么在进行拷贝的时候将会出现红色部分般繁琐的程序; 而在使用 UM 的情况下，由于 CUDA 底层 Runtime 可以为我们处理数据存储，我们只需要进行初始化之后，就可以在不同位置，基于首指针对这个二维数组发起访问。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;值得注意的是，上述提到的多个处理器，可以是由 PCIe 总线连接的 Host CPU 和 GPU，也可以是由 NVLink 等连接的多个 GPU。

  <h3 class="title">示例代码</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;针对向量加法的计算场景，Unified Memory 的示例代码如下所示:

  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cassert&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">cout</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CUDA kernel for vector addition</span></span><br><span class="line"><span class="comment">// No change when using CUDA unified memory</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">vectorAdd</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> *b, <span class="keyword">int</span> *c, <span class="keyword">int</span> N)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Calculate global thread thread ID</span></span><br><span class="line">  <span class="keyword">int</span> tid = (blockDim.x * blockIdx.x) + threadIdx.x;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Boundary check</span></span><br><span class="line">  <span class="keyword">if</span> (tid &lt; N) &#123;</span><br><span class="line">    c[tid] = a[tid] + b[tid];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Array size of 2^16 (65536 elements)</span></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">1</span> &lt;&lt; <span class="number">16</span>;</span><br><span class="line">  <span class="keyword">size_t</span> bytes = N * <span class="keyword">sizeof</span>(<span class="keyword">int</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Declare unified memory pointers</span></span><br><span class="line">  <span class="keyword">int</span> *a, *b, *c;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Allocation memory for these pointers</span></span><br><span class="line">  cudaMallocManaged(&amp;a, bytes);</span><br><span class="line">  cudaMallocManaged(&amp;b, bytes);</span><br><span class="line">  cudaMallocManaged(&amp;c, bytes);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Initialize vectors</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">    a[i] = rand() % <span class="number">100</span>;</span><br><span class="line">    b[i] = rand() % <span class="number">100</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Threads per CTA (1024 threads per CTA)</span></span><br><span class="line">  <span class="keyword">int</span> BLOCK_SIZE = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// CTAs per Grid</span></span><br><span class="line">  <span class="keyword">int</span> GRID_SIZE = (N + BLOCK_SIZE - <span class="number">1</span>) / BLOCK_SIZE;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Call CUDA kernel</span></span><br><span class="line">  vectorAdd&lt;&lt;&lt;GRID_SIZE, BLOCK_SIZE&gt;&gt;&gt;(a, b, c, N);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Wait for all previous operations before using values</span></span><br><span class="line">  <span class="comment">// We need this because we don&#x27;t get the implicit synchronization of</span></span><br><span class="line">  <span class="comment">// cudaMemcpy like in the original example</span></span><br><span class="line">  cudaDeviceSynchronize();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Verify the result on the CPU</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">    assert(c[i] == a[i] + b[i]);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Free unified memory (same as memory allocated with cudaMalloc)</span></span><br><span class="line">  cudaFree(a);</span><br><span class="line">  cudaFree(b);</span><br><span class="line">  cudaFree(c);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;COMPLETED SUCCESSFULLY!\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在这段程序中:

  <ul>
    <li>Line 25 我们首先声明了三个指针，用以指向我们后面基于 Unified Memory 进行声明的变量;</li>
    <li>Line 28~30 调用了 <code>cudaMallocManaged</code> API 基于 Unified Memory 方式分配了内存空间，而不是我们上面使用的 <code>cudaMalloc</code>;</li>
    <li>Line 53 加入了 Synchronization Barrier API <code>cudaDeviceSynchronize</code>，以等待异步的 Kernel 结束运行之后，再进行后续处理。</li>
  </ul>

  <h3 class="title">基本原理</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Unified Memory 的基本原理，可以参考 NVIDIA 博文 <cite>max_unified_memory_performance</cite>，以及来自 GTC'17 的一篇详细的 Talk <cite>unified_memory_gtc_17</cite>。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先需要明确的是，CUDA Runtime 在管理 GPU Device 上的内存空间的时候，其基于的是类似于 CPU 侧的基于页表的内存映射机制 (i.e. x86 架构保护模式内存访问以及页表机制，可见我另外的文章介绍 <a href="/sec_learning/Tech_OS_And_Linux_Kernel/Assembly_8_Protect_Mode/index.html">Intel 32 位处理器保护模式</a> 和 <a href="/sec_learning/Tech_OS_And_Linux_Kernel/Assembly_12_Page/index.html">分页机制和动态页面分配</a>)，以实现物理设备内存在多个 CUDA Kernels 之间的共享。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;切入正题，总的来说， UM 的基本原理可以分为三个部分进行说明: <def>On-demand Migration</def>, <def>Memory Oversubscription</def> 和 <def>System-Wide Atomics</def>，下面我们分别就这三个方面展开进行分析。

  <h4 class="title">On-demand Migration</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;下图展示了基于 UM 的数据访问过程:

  <div class="img" title="处理器 A 访问 Page 2 和 Page 3，但发生 Page Fault" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="page_fault_1">
    <img src="./pic/on_demand_1.png" width="80%" />
  </div>

  <div class="img" title="CUDA Runtime 异常处理程序将缺失的 Page 拷贝到处理器 A 的内存中" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="page_fault_2">
    <img src="./pic/on_demand_2.png" width="80%" />
  </div>

  <div class="img" title="处理器 A 以本地内存访问的形式访问 Page 2 和 Page 3 的数据" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="page_fault_3">
    <img src="./pic/on_demand_3.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;具体来说，当 GPU 处理器遭遇如上所示的缺页时，会进行如下步骤 <cite>max_unified_memory_performance</cite>:

  <ol>
    <li>在 GPU 内存侧分配对应大小的 GPU Pages;</li>
    <li>在 CPU 内存侧解开 CPU Pages 和当前变量指针的映射关系 (unmap);</li>
    <li>将 CPU Pages 的内容拷贝到刚刚分配好的新的 GPU Pages 中;</li>
    <li>将当前变量指针映射到 GPU Pages 区域 (map);</li>
    <li>释放 CPU Pages</li>
  </ol>

  <h4 class="title">Memory Oversubscription</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;下面的图展示了当 GPU 物理内存不够时，所进行的 Pages 替换过程。

  <div class="img" title="处理器 A 访问内存发生 Page Fault，但是物理内存已满" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="oversub_1">
    <img src="./pic/over_sub_1.png" width="80%" />
  </div>

  <div class="img" title="处理器 A 内存基于一定的机制将某些 Pages 淘汰到处理器 B 的内存中" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="oversub_2">
    <img src="./pic/over_sub_2.png" width="80%" />
  </div>

  <div class="img" title="CUDA Runtime 异常处理程序将缺失的 Page 拷贝到处理器 A 的内存中" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="oversub_3">
    <img src="./pic/over_sub_3.png" width="80%" />
  </div>

  <div class="img" title="处理器 A 以本地内存访问的形式访问这些 Pages 的数据" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="oversub_4">
    <img src="./pic/over_sub_4.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;当我们有大量的数据需要交给 GPU 进行处理，但是物理的内存不够时，采用非 UM 的方式需要我们非常小心地去管理内存，以避免 <def>Out Of Memory (OOM)</def> 的错误。UM 使得程序员可以从繁琐的内存管理任务中解放出来。

  <h4 class="title">System-Wide Atomics</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于不同的处理器同时发起对同一片 Pages 访问的情况，UM 的底层提供了基于独占访问 (Exclusive Access) 的系统级别的操作原子性保证，如下图所示:

  <div class="img" title="处理器 A 和处理器 B 同时发起对 Page 2 的访问，此时 Page 2 正位于处理器 B 的内存中" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="atom_1">
    <img src="./pic/system_atom_1.png" width="80%" />
  </div>

  <div class="img" title="处理器 B 在完成对 Page 2 的操作后，Page 2 被迁移到处理器 A 的内存中" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="atom_2">
    <img src="./pic/system_atom_2.png" width="80%" />
  </div>

  <div class="img" title="处理器 A 完成对 Page 2 的访问" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="atom_3">
    <img src="./pic/system_atom_3.png" width="80%" />
  </div>

  <h3 class="title">关键 API</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在上面的代码中，我们使用了如下所示的 CUDA API:

  <!--table start-->
  <div class="table" title="Unified Memory Vector Addition 所使用的 CUDA API">
    <table border="1" align="center" bgcolor="#FFFFFF">
      <tr>
        <th align="center">API</th>
        <th align="center">功能</th>
      </tr>
      <tr>
        <td><code><def>cudaMallocManaged</def>(&a, bytes)</code></td>
        <td>以 Unified Memory 的方式分配内存，并且使用指定的指针来对其进行访问</td>
      </tr>
      <tr>
        <td><code><def>cudaDeviceSynchronize</def>()</code></td>
        <td>Synchronization Memory Barrier</td>
      </tr>
    </table>
  </div>
  <!--table end-->
</div>

<h2 class="title">Unified Memory with Prefetching</h2>
<div class="div_learning_post">
  <h3 class="title">Motivation</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于上面我们介绍的关于 UM 的 On-demand Migration 的底层原理，我们可以知道，只有当处理器在访问某段内存空间的时候发生 Page 
  Fault，才会触发 CUDA Runtime 将对应的 Pages 拷贝到对应处理器的内存中。以 GPU 执行 CUDA Kernel 为例，Page Fault 造成的内存拷贝将导致 CUDA Kernel 不得不被迫等待数据准备完成，这将导致较大的处理时延，我们在后面的性能分析中将看到相关的结果。更加严重的是，Page Fault 将导致 SM 发生停顿 (stall)，这导致其它的程序将也遭受阻塞，大大降低了 GPU 的运行效率。

  <h4 class="title">Prefetching 异步预存取</h4>

  <div class="img" title="数据提前预取 (Prefetching)" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="prefetching">
    <img src="./pic/prefetching.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;为了避免 Page Fault 造成的开销，CUDA 向程序员提供了用于进行数据的 <def>提前预取 (Prefetching)</def> 的 API，<code>cudaMemPrefetchAsync</code>。如 <imgref>prefetching</imgref> 所示，在调用 CUDA Kernel，或者 CPU 函数前后，我们在程序中可以调用该 API 来实现 <note>异步的</note> 数据提前的存取，以避免在函数开始执行后触发 Page Fault，导致昂贵的数据拷贝和进程阻塞开销。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;除了预存取之外，CUDA 还提供了向 CUDA Runtime 提供 <def>内存提示 (Memory Hint)</def> 的 API。值得注意的是，这类内存提示的 API 并不会显式地对数据进行迁移和拷贝的操作，但是会指导底层的 CUDA Runtime 在后续的内存操作中，按照指定的规则进行操作。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;内存提示的 API 是 <code>cudaMemAdvise</code>，它的函数原型如下所示:

  <div align="center" style="margin-bottom:30px">
    <code><def>cudaMemAdvise</def>(ptr, size, advice, processor)</code>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在传入的参数中，<code>ptr</code> 指示的是我们针对的数据对应的指针，<code>size</code> 对应的是我们针对的数据的大小，<code>advice</code> 针对的是我们要指定的 Memory Hint 的类型，具体有如下三个宏:

  <ul>
    <li><code>cudaMemAdviseSetReadMostly</code>: 指示当前的数据是多处理器共享的只读数据;</li>
    <li><code>cudaMemAdviseSetPreferredLocation</code>: 指示当前的数据更佳的存储位置;</li>
    <li><code>cudaMemAdviseSetAccessedBy</code>: ?;</li>
  </ul>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;最后，参数 <code>device</code> 指示的是 Memory Hint 的参数数据，这与具体的 <code>advice</code> 类型有关。我们下面对这三个 <code>advice</code> 进行介绍。

  <h4 class="title"><note>Memory Hint</note>: Read-mostly</h4>

  <div class="img" title="Memory Hint: Read-mostly 数据" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="hint_1">
    <img src="./pic/hint_1.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;如 <imgref>hint_1</imgref> 所示，如果我们知道我们的数据是只读的数据，并且需要在多个处理器之间共享的话，我们可以在 <code>cudaMemPrefetchAsync</code> 启动异步预存取之前，使用 <code>cudaMemAdvise</code>，搭配 <code>cudaMemAdviseSetReadMostly</code> 来提供对应的内存提示。在这种情况下，在执行 <code>cudaMemPrefetchAsync</code> 的时候，CUDA Runtime 将会对数据进行拷贝，而不是 Migration。值得注意的是，基于 <code>cudaMemAdviseSetReadMostly</code> 类型的 <code>cudaMemAdvise</code> Memory Hint 的 <code>device</code> 参数会被 CUDA Runtime 忽视 <cite>unified_memory_20</cite>。

  <h4 class="title"><note>Memory Hint</note>: Preferred-location</h4>

  <div class="img" title="Memory Hint: 数据的 Preferred-location" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="hint_2">
    <img src="./pic/hint_2.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;如果我们能提前判断数据存储的更佳位置，我们可以使用 <code>cudaMemAdvise</code>，搭配 <code>cudaMemAdviseSetPreferredLocation</code> 来提供对应的内存提示。如 <imgref>hint_2</imgref> 所示，程序提示了数据 <code>data</code> 的更佳存储位置位于 CPU 侧，则: <cite>unified_memory_20</cite>

  <ol>
    <li>当 CPU 访问这部分数据，但发生 Page Fault 时，CUDA Runtime 将会将这部分数据 <note>Migrate</note> 到 CPU 内存中，也即 On-demand 的 Migration;</li>
    <li>
    当 GPU 访问这部分数据，但发生 Page Fault 时:
    <ul>
      <li>如果设备支持的话，将会把 CPU 侧的这部分 Pages 直接映射 (mapping) 给 GPU 使用;</li>
      <li>如果设备不支持远程 P2P 映射，则会触发数据的 <note>Migration</note> 操作</li>
    </ul>
    </li>
  </ol>

  <h4 class="title"><note>Memory Hint</note>: Accessed-by</h4>

  <div class="img" title="Memory Hint: Accessed-by 映射关系指定" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="hint_3">
    <img src="./pic/hint_3.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;如 <imgref>hint_3</imgref> 所示，<code>cudaMemAdviseSetAccessedBy</code> 的内存提示可以直接在 GPU 和 CPU 的内存 Pages 之间建立映射关系。从而在 CUDA Kernel 运行起来以后不会发生 Page Fault，而是直接访问 CPU 内存以获得相应的 Pages。

  <div class="img" title="Memory Hint: Volta+ 后的 Accessed-by" source="https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf" label="hint_4">
    <img src="./pic/hint_4.png" width="80%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;另外， 如 <imgref>hint_4</imgref> 所示，在 Volta+ 的架构以后 CUDA 引入了计数器机制，当基于 Mapping 的机制跨处理器访问内存次数超过一定界限后，CUDA 将会触发数据的 <note>Migration</note> 操作。

  <h3 class="title">示例程序</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;下面是基于 Prefetch UM 的向量加法实例代码。

  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief [CUDA Kernel] Conduct Vector Adding (a+b=c)</span></span><br><span class="line"><span class="comment"> * \param vector_a  source vector</span></span><br><span class="line"><span class="comment"> * \param vector_b  source vector</span></span><br><span class="line"><span class="comment"> * \param vector_c  destination vector</span></span><br><span class="line"><span class="comment"> * \param d         dimension of vectors</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">vectorAdd</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_a, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_b, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_c, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> d)</span></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (tid &lt; d)&#123;</span><br><span class="line">    vector_c[tid] = vector_a[tid] + vector_b[tid];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief verify the vector addition result from GPU</span></span><br><span class="line"><span class="comment"> * \param vector_a  pointer to source vector array</span></span><br><span class="line"><span class="comment"> * \param vector_b  pointer to source vector array</span></span><br><span class="line"><span class="comment"> * \param vector_c  pointer to destination vector array</span></span><br><span class="line"><span class="comment"> * \param d         dimension of vectors</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">verifyVectorAdditionResult</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_a, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_b, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_c,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> d</span></span></span><br><span class="line"><span class="function"><span class="params">  )</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;d; i++)&#123;</span><br><span class="line">      PROFILE(</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;[i]=&quot;</span> &lt;&lt; i </span><br><span class="line">          &lt;&lt; <span class="string">&quot;, vector_a= &quot;</span> &lt;&lt; vector_a[i]</span><br><span class="line">          &lt;&lt; <span class="string">&quot;, vector_b= &quot;</span> &lt;&lt; vector_b[i]</span><br><span class="line">          &lt;&lt; <span class="string">&quot;, vector_c= &quot;</span> &lt;&lt; vector_c[i]</span><br><span class="line">          &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">      )</span><br><span class="line">      assert(vector_c[i] == vector_a[i]+vector_b[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="comment">// initialize constants</span></span><br><span class="line">  <span class="keyword">constexpr</span> <span class="keyword">int</span> N = <span class="number">1</span> &lt;&lt; <span class="number">16</span>;</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="keyword">int</span> unit_size = <span class="keyword">sizeof</span>(<span class="keyword">int</span>);</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="keyword">int</span> vector_size = N*unit_size;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// get cuda device index</span></span><br><span class="line">  <span class="keyword">int</span> gpu_id = cudaGetDevice(&amp;gpu_id);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// data pointers</span></span><br><span class="line">  <span class="keyword">int</span> *vector_a, *vector_b, *vector_c;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// allocate unified memory for vectors</span></span><br><span class="line">  cudaMallocManaged(&amp;vector_a, vector_size);</span><br><span class="line">  cudaMallocManaged(&amp;vector_b, vector_size);</span><br><span class="line">  cudaMallocManaged(&amp;vector_c, vector_size);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// put vector_c at gpu side</span></span><br><span class="line">  cudaMemPrefetchAsync(vector_c, vector_size, gpu_id);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// initialize source vectors</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++)&#123;</span><br><span class="line">    vector_a[i] = rand() % <span class="number">100</span>;</span><br><span class="line">    vector_b[i] = rand() % <span class="number">100</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// migrate vector_a and vector_b to gpu</span></span><br><span class="line">  cudaMemPrefetchAsync(vector_a, vector_size, gpu_id);</span><br><span class="line">  cudaMemPrefetchAsync(vector_b, vector_size, gpu_id);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// initialize kernel configuration</span></span><br><span class="line">  <span class="comment">// number of kernels per block</span></span><br><span class="line">  <span class="keyword">int</span> NUM_THREADS_PER_BLOCK = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// number of blocks per grid</span></span><br><span class="line">  <span class="keyword">int</span> NUM_BLOCKS_PER_GRID = N % NUM_THREADS_PER_BLOCK == <span class="number">0</span> ?</span><br><span class="line">                            N / NUM_THREADS_PER_BLOCK :</span><br><span class="line">                            N / NUM_THREADS_PER_BLOCK + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// launch kernel</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Launch Kernel: &quot;</span> </span><br><span class="line">    &lt;&lt; NUM_THREADS_PER_BLOCK &lt;&lt; <span class="string">&quot; threads per block, &quot;</span> </span><br><span class="line">    &lt;&lt; NUM_BLOCKS_PER_GRID &lt;&lt; <span class="string">&quot; blocks per grid&quot;</span> </span><br><span class="line">    &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">  vectorAdd&lt;&lt;&lt;NUM_BLOCKS_PER_GRID, NUM_THREADS_PER_BLOCK&gt;&gt;&gt;(vector_a, vector_b, vector_c, N);</span><br><span class="line">  cudaDeviceSynchronize();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// migrate vector_a, vector_b and vector_c to cpu side</span></span><br><span class="line">  cudaMemPrefetchAsync(vector_a, vector_size, cudaCpuDeviceId);</span><br><span class="line">  cudaMemPrefetchAsync(vector_b, vector_size, cudaCpuDeviceId);</span><br><span class="line">  cudaMemPrefetchAsync(vector_c, vector_size, cudaCpuDeviceId);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// verify result</span></span><br><span class="line">  verifyVectorAdditionResult(vector_a, vector_b, vector_c, N);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// free device memory</span></span><br><span class="line">  cudaFree(vector_a);</span><br><span class="line">  cudaFree(vector_b);</span><br><span class="line">  cudaFree(vector_c);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Get correct vector addition result!&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;如上所示，我们调用 <code>cudaMemPrefetchAsync</code> API，在调用 CUDA Kernel 的前后异步地开始数据的预存取。我们在后面测试性能的试验中会看到，Prefetching 的操作使得 CUDA Kernel 在运行的过程中不会发生 Page Fault，加速了其运行过程。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;下面的示例代码在 Prefetching 的基础上，引入了 Memory Hint 加以优化:

  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief [CUDA Kernel] Conduct Vector Adding (a+b=c)</span></span><br><span class="line"><span class="comment"> * \param vector_a  source vector</span></span><br><span class="line"><span class="comment"> * \param vector_b  source vector</span></span><br><span class="line"><span class="comment"> * \param vector_c  destination vector</span></span><br><span class="line"><span class="comment"> * \param d         dimension of vectors</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">vectorAdd</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_a, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_b, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_c, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> d)</span></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (tid &lt; d)&#123;</span><br><span class="line">    vector_c[tid] = vector_a[tid] + vector_b[tid];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief verify the vector addition result from GPU</span></span><br><span class="line"><span class="comment"> * \param vector_a  pointer to source vector array</span></span><br><span class="line"><span class="comment"> * \param vector_b  pointer to source vector array</span></span><br><span class="line"><span class="comment"> * \param vector_c  pointer to destination vector array</span></span><br><span class="line"><span class="comment"> * \param d         dimension of vectors</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">verifyVectorAdditionResult</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_a, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_b, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *vector_c,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> d</span></span></span><br><span class="line"><span class="function"><span class="params">  )</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;d; i++)&#123;</span><br><span class="line">      PROFILE(</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;[i]=&quot;</span> &lt;&lt; i </span><br><span class="line">          &lt;&lt; <span class="string">&quot;, vector_a= &quot;</span> &lt;&lt; vector_a[i]</span><br><span class="line">          &lt;&lt; <span class="string">&quot;, vector_b= &quot;</span> &lt;&lt; vector_b[i]</span><br><span class="line">          &lt;&lt; <span class="string">&quot;, vector_c= &quot;</span> &lt;&lt; vector_c[i]</span><br><span class="line">          &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">      )</span><br><span class="line">      assert(vector_c[i] == vector_a[i]+vector_b[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="comment">// initialize constants</span></span><br><span class="line">  <span class="keyword">constexpr</span> <span class="keyword">int</span> N = <span class="number">1</span> &lt;&lt; <span class="number">16</span>;</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="keyword">int</span> unit_size = <span class="keyword">sizeof</span>(<span class="keyword">int</span>);</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="keyword">int</span> vector_size = N*unit_size;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// get cuda device index</span></span><br><span class="line">  <span class="keyword">int</span> gpu_id = cudaGetDevice(&amp;gpu_id);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// data pointers</span></span><br><span class="line">  <span class="keyword">int</span> *vector_a, *vector_b, *vector_c;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// allocate unified memory for vectors</span></span><br><span class="line">  cudaMallocManaged(&amp;vector_a, vector_size);</span><br><span class="line">  cudaMallocManaged(&amp;vector_b, vector_size);</span><br><span class="line">  cudaMallocManaged(&amp;vector_c, vector_size);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// put vector_a and vector_b at cpu side for initialization</span></span><br><span class="line">  <span class="comment">// put vector_c at gpu side</span></span><br><span class="line">  cudaMemAdvise(vector_a, vector_size, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);</span><br><span class="line">  cudaMemAdvise(vector_b, vector_size, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);</span><br><span class="line">  cudaMemPrefetchAsync(vector_c, vector_size, gpu_id);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// initialize source vectors</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++)&#123;</span><br><span class="line">    vector_a[i] = rand() % <span class="number">100</span>;</span><br><span class="line">    vector_b[i] = rand() % <span class="number">100</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// set vector_a and vector_b as read mostly</span></span><br><span class="line">  <span class="comment">// COPY vector_a and vector_b to gpu</span></span><br><span class="line">  cudaMemAdvise(vector_a, vector_size, cudaMemAdviseSetReadMostly, gpu_id);</span><br><span class="line">  cudaMemAdvise(vector_b, vector_size, cudaMemAdviseSetReadMostly, gpu_id);</span><br><span class="line">  cudaMemPrefetchAsync(vector_a, vector_size, gpu_id);</span><br><span class="line">  cudaMemPrefetchAsync(vector_b, vector_size, gpu_id);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// initialize kernel configuration</span></span><br><span class="line">  <span class="comment">// number of kernels per block</span></span><br><span class="line">  <span class="keyword">int</span> NUM_THREADS_PER_BLOCK = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// number of blocks per grid</span></span><br><span class="line">  <span class="keyword">int</span> NUM_BLOCKS_PER_GRID = N % NUM_THREADS_PER_BLOCK == <span class="number">0</span> ?</span><br><span class="line">                            N / NUM_THREADS_PER_BLOCK :</span><br><span class="line">                            N / NUM_THREADS_PER_BLOCK + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// launch kernel</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Launch Kernel: &quot;</span> </span><br><span class="line">    &lt;&lt; NUM_THREADS_PER_BLOCK &lt;&lt; <span class="string">&quot; threads per block, &quot;</span> </span><br><span class="line">    &lt;&lt; NUM_BLOCKS_PER_GRID &lt;&lt; <span class="string">&quot; blocks per grid&quot;</span> </span><br><span class="line">    &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">  vectorAdd&lt;&lt;&lt;NUM_BLOCKS_PER_GRID, NUM_THREADS_PER_BLOCK&gt;&gt;&gt;(vector_a, vector_b, vector_c, N);</span><br><span class="line">  </span><br><span class="line">  cudaDeviceSynchronize();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// migrate vector_a, vector_b and vector_c to cpu side</span></span><br><span class="line">  <span class="comment">// cudaMemPrefetchAsync(vector_a, vector_size, cudaCpuDeviceId);</span></span><br><span class="line">  <span class="comment">// cudaMemPrefetchAsync(vector_b, vector_size, cudaCpuDeviceId);</span></span><br><span class="line">  cudaMemPrefetchAsync(vector_c, vector_size, cudaCpuDeviceId);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// verify result</span></span><br><span class="line">  verifyVectorAdditionResult(vector_a, vector_b, vector_c, N);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// free device memory</span></span><br><span class="line">  cudaFree(vector_a);</span><br><span class="line">  cudaFree(vector_b);</span><br><span class="line">  cudaFree(vector_c);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Get correct vector addition result!&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;如上所示，在 Line 66~68 中我们声明了 <code>vector_a</code> 和 <code>vector_b</code> 更佳的存储位置应该在 CPU 侧，而 <code>vector_c</code> 则被预存取到了 GPU 内存中；在 Line 71～73 完成 <code>vector_a</code> 和 <code>vector_b</code> 的初始化后，我们又在 Line 78~79 内存提示 <code>vector_a</code> 和 <code>vector_b</code> 是 Read-mostly 的，然后在 Line 80~81 调用 <code>cudaMemPrefetchAsync</code> 将 <code>vector_a</code> 和 <code>vector_b</code> 预存取到 GPU 内存中。由于我们之前在 Line 66~68 中我们声明了 <code>vector_a</code> 和 <code>vector_b</code> 更佳的存储位置应该在 CPU 侧，因此此处的预存取实际上是一次拷贝操作。在完成 CUDA Kernel 后，我们在 Line 104 调用 <code>cudaMemPrefetchAsync</code> API 将 <code>vector_c</code> 预存取到 CPU 侧，而 <code>vector_a</code> 和 <code>vector_b</code> 本身就有一份副本在 CPU 侧，因此不需要进行预存取操作。
</div>



<div class="div_ref" id="ref_container"></div>

</body>


<!--图片、引用-->
<!-- 
<div class="img" title="img title" label="img_label" source="url">
  <img src="" height="" />
</div>

<imaging>img_label</imaging>
-->

<!--等式、引用-->
<!-- 
<div class="equation" label="equation_label">
</div>

<equation>equation_label</equation>
-->

<!--定理、引用、证明-->
<!-- 
<div class="theorm" label="theorm_label">
</div>

<theorm>theorm_label</theorm>

<div class="theorm_prove">
</div>
-->

<!--引用其它章节-->
<!-- 
<ref></ref> 
-->

<!--引用文献-->
<!-- 
<cite></cite> 
-->

<!--关键词-->
<!-- 
<def></def> 
-->

<!--醒目注意-->
<!-- 
<note></note> 
-->

<!--表格-->
<!--
<table border="1" align="center" bgcolor="#FFFFFF">
  <caption>表格</caption>
  <tr>
    <th>A</th>
    <th>B</th>
    <th>C</th>
  </tr>
  <tr>
    <td>xxx</td>
    <td>xxx</td>
    <td>xxx</td>
  </tr>
</table>
-->

<!--矩阵公式-->
<!--
<div class="cmath" align="center">
  `((1, 0),(1, 0))`
</div><br>
-->

<!--伪代码-->
<!--
<pre id="quicksort" style="display:hidden;">
  % This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
  \begin{algorithm}
  \caption{Quicksort}
  \begin{algorithmic}
  \PROCEDURE{Quicksort}{$A, p, r$}
      % Add Here

      % 空行
      % \STATE \texttt{\\}
  \ENDPROCEDURE
  \end{algorithmic}
  \end{algorithm}
</pre>
<script>
    pseudocode.renderElement(document.getElementById("quicksort"));
</script>
-->
<!--
Latex 伪代码格式见: https://github.com/SaswatPadhi/pseudocode.js
-->

<!--图片-->
<!--
<div align="center">
  <img src="./pic/xxx.png" width=80%>
</div>
-->

<!--正文-->
<!--
<p>
&nbsp;&nbsp;&nbsp;&nbsp;公式：<span>`\overline{A}\overline{B}`</span>
</p>
-->
      </div>
      
      
      
    </div>
    
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Tech_OS_And_Linux_Kernel/">TECH_OS_AND_LINUX_KERNEL</a></li>
          <li>CUDA_1_VECTOR_ADDITION</li>
        
  </ul>

    
    
    


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhuobin Huang"
      src="/images/avatar_2.png">
  <p class="site-author-name" itemprop="name">Zhuobin Huang</p>
  <div class="site-description" itemprop="description">System Engineer</div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zobinHuang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zobinHuang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zobin1999@gmail.com" title="E-Mail → mailto:zobin1999@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhuobin Huang</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
